{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from ppo.history import *\n",
    "from ppo.models import *\n",
    "from ppo.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General parameters\n",
    "max_steps = 5e10 ##5e5 # Set maximum number of steps to run environment.\n",
    "run_path = \"ppo\" # The sub-directory name for model and summary statistics\n",
    "load_model = False # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model.\n",
    "summary_freq = 10000 # Frequency at which to save training statistics.\n",
    "save_freq = 50000 # Frequency at which to save model.\n",
    "env_name = \"ObstacleCurriculum\" # Name of the training environment file.\n",
    "curriculum_file = None\n",
    "\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.99 #0.99 # Reward discount rate.\n",
    "lambd = 0.95 ##0.95 # Lambda parameter for GAE.\n",
    "time_horizon = 2048 ##2048 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 1e-3 ##1e-3 # Strength of entropy regularization\n",
    "num_epoch = 5 ##5 # Number of gradient descent steps per batch of experiences.\n",
    "num_layers = 2 ##2 # Number of hidden layers between state/observation encoding and value/policy layers.\n",
    "epsilon = 0.2 ##0.2 # Acceptable threshold around ratio of old and new policy probabilities.\n",
    "buffer_size = 4096 ##2048 # How large the experience buffer should be before gradient descent.\n",
    "learning_rate = 3e-4 # Model learning rate.\n",
    "hidden_units = 128 ##64 # Number of units in hidden layer.\n",
    "batch_size = 32 ##64 # How many experiences per gradient descent update step.\n",
    "normalize = False\n",
    "\n",
    "### Logging dictionary for hyperparameters\n",
    "hyperparameter_dict = {'max_steps':max_steps, 'run_path':run_path, 'env_name':env_name,\n",
    "    'curriculum_file':curriculum_file, 'gamma':gamma, 'lambd':lambd, 'time_horizon':time_horizon,\n",
    "    'beta':beta, 'num_epoch':num_epoch, 'epsilon':epsilon, 'buffe_size':buffer_size,\n",
    "    'leaning_rate':learning_rate, 'hidden_units':hidden_units, 'batch_size':batch_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'ObstacleCurriculumAcademy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: ObstacleCurriculumAcademy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: ObstacleCurriculumBrain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 9\n",
      "        Action space type: discrete\n",
      "        Action space size (per agent): 8\n",
      "        Memory space size (per agent): 0\n",
      "        Action descriptions: , , , , , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name, curriculum=curriculum_file)\n",
    "print(str(env))\n",
    "brain_name = env.external_brain_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10000. Mean Reward: -1.4472185430463576. Std of Reward: 0.3865069569045623.\n",
      "Step: 20000. Mean Reward: -1.4423856209150328. Std of Reward: 0.3932403034581145.\n",
      "Step: 30000. Mean Reward: -1.648333333333333. Std of Reward: 0.521688591909104.\n",
      "Step: 40000. Mean Reward: -1.908888888888887. Std of Reward: 0.8133887352108438.\n",
      "Step: 50000. Mean Reward: -2.3485964912280672. Std of Reward: 0.9786034420692511.\n",
      "Saved Model\n",
      "Step: 60000. Mean Reward: -2.4949056603773547. Std of Reward: 1.0470085376406815.\n",
      "Step: 70000. Mean Reward: -2.453725490196072. Std of Reward: 1.1260197309320443.\n",
      "Step: 80000. Mean Reward: -2.4192307692307624. Std of Reward: 1.0911604552279368.\n",
      "Step: 90000. Mean Reward: -2.3154629629629575. Std of Reward: 1.0781024433039161.\n",
      "Step: 100000. Mean Reward: -2.1031147540983555. Std of Reward: 1.0953860626170775.\n",
      "Saved Model\n",
      "Step: 110000. Mean Reward: -2.787999999999987. Std of Reward: 1.2320084212374414.\n",
      "Step: 120000. Mean Reward: -2.931911764705863. Std of Reward: 1.2956352148122376.\n",
      "Step: 130000. Mean Reward: -2.49622222222221. Std of Reward: 1.1466337958251391.\n",
      "Step: 140000. Mean Reward: -2.164210526315786. Std of Reward: 0.9067175179917113.\n",
      "Step: 150000. Mean Reward: -2.366938775510196. Std of Reward: 1.0916205965736387.\n",
      "Saved Model\n",
      "Step: 160000. Mean Reward: -2.3471428571428534. Std of Reward: 0.8267220740702987.\n",
      "Step: 170000. Mean Reward: -2.2124999999999972. Std of Reward: 0.8032081218694398.\n",
      "Step: 180000. Mean Reward: -2.1107272727272695. Std of Reward: 0.7712294895942643.\n",
      "Step: 190000. Mean Reward: -2.3256521739130385. Std of Reward: 0.7693875378278707.\n",
      "Step: 200000. Mean Reward: -1.9303906249999958. Std of Reward: 0.836334218800774.\n",
      "Saved Model\n",
      "Step: 210000. Mean Reward: -2.059622641509429. Std of Reward: 0.6915615697610549.\n",
      "Step: 220000. Mean Reward: -1.9183333333333301. Std of Reward: 0.6440952803419114.\n",
      "Step: 230000. Mean Reward: -1.8256428571428553. Std of Reward: 0.5764385244316839.\n",
      "Step: 240000. Mean Reward: -1.877076923076922. Std of Reward: 0.45262303240937096.\n",
      "Step: 250000. Mean Reward: -1.83358108108108. Std of Reward: 0.574488915207358.\n",
      "Saved Model\n",
      "Step: 260000. Mean Reward: -1.9566935483870946. Std of Reward: 0.5864387762976377.\n",
      "Step: 270000. Mean Reward: -1.9242187499999983. Std of Reward: 0.5632316205598149.\n",
      "Step: 280000. Mean Reward: -2.043818181818181. Std of Reward: 0.40620635557039714.\n",
      "Step: 290000. Mean Reward: -1.8374637681159405. Std of Reward: 0.5206821217169403.\n",
      "Step: 300000. Mean Reward: -1.8211594202898538. Std of Reward: 0.5101151794686178.\n",
      "Saved Model\n",
      "Step: 310000. Mean Reward: -1.7839333333333327. Std of Reward: 0.436901433074122.\n",
      "Step: 320000. Mean Reward: -1.8681060606060598. Std of Reward: 0.44378687653134785.\n",
      "Step: 330000. Mean Reward: -1.947258064516128. Std of Reward: 0.44796464201929903.\n",
      "Step: 340000. Mean Reward: -2.0668518518518497. Std of Reward: 0.4739191370080923.\n",
      "Step: 350000. Mean Reward: -2.0774545454545446. Std of Reward: 0.5400192527942211.\n",
      "Saved Model\n",
      "Step: 360000. Mean Reward: -2.0591071428571412. Std of Reward: 0.5462102446655039.\n",
      "Step: 370000. Mean Reward: -2.2357291666666637. Std of Reward: 0.49926820362336616.\n",
      "Step: 380000. Mean Reward: -2.1536999999999966. Std of Reward: 0.6041124150354775.\n",
      "Step: 390000. Mean Reward: -2.1262499999999966. Std of Reward: 0.7882440268300986.\n",
      "Step: 400000. Mean Reward: -1.9205999999999963. Std of Reward: 1.0023839783236734.\n",
      "Saved Model\n",
      "Step: 410000. Mean Reward: -1.7292452830188652. Std of Reward: 1.157994931159438.\n",
      "Step: 420000. Mean Reward: -1.3138636363636345. Std of Reward: 1.2143073207733457.\n",
      "Step: 430000. Mean Reward: -1.146124999999999. Std of Reward: 1.167164559680851.\n",
      "Step: 440000. Mean Reward: -0.7888124999999994. Std of Reward: 1.2598281737378898.\n",
      "Step: 450000. Mean Reward: -0.8966249999999988. Std of Reward: 1.2510127834578644.\n",
      "Saved Model\n",
      "Step: 460000. Mean Reward: -1.0670833333333318. Std of Reward: 1.2808037163133998.\n",
      "Step: 470000. Mean Reward: -0.7397560975609743. Std of Reward: 1.4185095885141963.\n",
      "Step: 480000. Mean Reward: -0.5551630434782594. Std of Reward: 1.3796233439757755.\n",
      "Step: 490000. Mean Reward: -0.4363917525773185. Std of Reward: 1.3055779899213316.\n",
      "Step: 500000. Mean Reward: -0.5116666666666647. Std of Reward: 1.493921720599737.\n",
      "Saved Model\n",
      "Step: 510000. Mean Reward: -0.3121428571428555. Std of Reward: 1.427126164528537.\n",
      "Step: 520000. Mean Reward: -0.13368131868131689. Std of Reward: 1.4152471991514193.\n",
      "Step: 530000. Mean Reward: -0.268798076923075. Std of Reward: 1.426650443547125.\n",
      "Step: 540000. Mean Reward: 0.008388429752066603. Std of Reward: 1.0591112395387765.\n",
      "Step: 550000. Mean Reward: -0.10658536585365788. Std of Reward: 1.0306804361255013.\n",
      "Saved Model\n",
      "Step: 560000. Mean Reward: 0.17676470588235293. Std of Reward: 0.7126820018866397.\n",
      "Step: 570000. Mean Reward: 0.04753676470588249. Std of Reward: 0.8529878275464851.\n",
      "Step: 580000. Mean Reward: 0.24293893129771. Std of Reward: 0.6922473828182406.\n",
      "Step: 590000. Mean Reward: 0.17135416666666675. Std of Reward: 0.7558005364875442.\n",
      "Step: 600000. Mean Reward: 0.3090298507462686. Std of Reward: 0.5803136946372707.\n",
      "Saved Model\n",
      "Step: 610000. Mean Reward: 0.337111111111111. Std of Reward: 0.6104981168007673.\n",
      "Step: 620000. Mean Reward: 0.16630952380952407. Std of Reward: 0.8028844770949854.\n",
      "Step: 630000. Mean Reward: 0.2205970149253731. Std of Reward: 0.7811131181043306.\n",
      "Step: 640000. Mean Reward: 0.18612403100775224. Std of Reward: 0.7921272606213009.\n",
      "Step: 650000. Mean Reward: 0.28633093525179854. Std of Reward: 0.7194901211043486.\n",
      "Saved Model\n",
      "Step: 660000. Mean Reward: 0.28669172932330855. Std of Reward: 0.7289329123338256.\n",
      "Step: 670000. Mean Reward: 0.3723684210526316. Std of Reward: 0.6651695976134457.\n",
      "Step: 680000. Mean Reward: 0.24152000000000023. Std of Reward: 0.7290783837146713.\n",
      "Step: 690000. Mean Reward: 0.30253623188405787. Std of Reward: 0.6255792233037148.\n",
      "Step: 700000. Mean Reward: 0.2555970149253735. Std of Reward: 0.8380661474214132.\n",
      "Saved Model\n",
      "Step: 710000. Mean Reward: 0.2691558441558441. Std of Reward: 0.6743033454511588.\n",
      "Step: 720000. Mean Reward: 0.30220779220779215. Std of Reward: 0.7043496555986081.\n",
      "Step: 730000. Mean Reward: 0.37126666666666674. Std of Reward: 0.6813954276988413.\n",
      "Step: 740000. Mean Reward: 0.3521768707482992. Std of Reward: 0.5924557068972375.\n",
      "Step: 750000. Mean Reward: 0.26718045112782024. Std of Reward: 0.8843023998354166.\n",
      "Saved Model\n",
      "Step: 760000. Mean Reward: 0.3356451612903228. Std of Reward: 0.7591140229872833.\n",
      "Step: 770000. Mean Reward: 0.4483221476510068. Std of Reward: 0.6436980540684365.\n",
      "Step: 780000. Mean Reward: 0.4555333333333334. Std of Reward: 0.6109771263221611.\n",
      "Step: 790000. Mean Reward: 0.3169852941176481. Std of Reward: 1.0271935422855494.\n",
      "Step: 800000. Mean Reward: 0.2823448275862077. Std of Reward: 0.9814612707190797.\n",
      "Saved Model\n",
      "Step: 810000. Mean Reward: 0.07504347826087258. Std of Reward: 1.512686135963337.\n",
      "Step: 820000. Mean Reward: 0.36573529411764794. Std of Reward: 0.9728527504450145.\n",
      "Step: 830000. Mean Reward: 0.2012931034482777. Std of Reward: 1.2453709322599162.\n",
      "Step: 840000. Mean Reward: -0.32417647058822846. Std of Reward: 2.087534691090215.\n",
      "Step: 850000. Mean Reward: 0.033990384615387614. Std of Reward: 1.5276080126433376.\n",
      "Saved Model\n",
      "Step: 860000. Mean Reward: 0.23875000000000124. Std of Reward: 1.1200377442796723.\n",
      "Step: 870000. Mean Reward: 0.30863970588235373. Std of Reward: 1.001269684192155.\n",
      "Step: 880000. Mean Reward: 0.3312000000000005. Std of Reward: 0.8822512265033472.\n",
      "Step: 890000. Mean Reward: 0.4391071428571433. Std of Reward: 0.8107056599604012.\n",
      "Step: 900000. Mean Reward: 0.48718354430379746. Std of Reward: 0.5775755289379406.\n",
      "Saved Model\n",
      "Step: 910000. Mean Reward: 0.5061728395061726. Std of Reward: 0.4508803265745104.\n",
      "Step: 920000. Mean Reward: 0.4690909090909088. Std of Reward: 0.4380051645849277.\n",
      "Step: 930000. Mean Reward: 0.48986754966887436. Std of Reward: 0.6184428070854356.\n",
      "Step: 940000. Mean Reward: 0.43359060402684585. Std of Reward: 0.6843105285680078.\n",
      "Step: 950000. Mean Reward: 0.49017857142857124. Std of Reward: 0.4655206675559837.\n",
      "Saved Model\n",
      "Step: 960000. Mean Reward: 0.5254294478527606. Std of Reward: 0.40913547892405894.\n",
      "Step: 970000. Mean Reward: 0.403006756756757. Std of Reward: 0.7005327943075091.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 980000. Mean Reward: 0.5020312499999998. Std of Reward: 0.46549748148989745.\n",
      "Step: 990000. Mean Reward: 0.482208588957055. Std of Reward: 0.47926326965877664.\n",
      "Step: 1000000. Mean Reward: 0.4364242424242422. Std of Reward: 0.5483865607565209.\n",
      "Saved Model\n",
      "Step: 1010000. Mean Reward: 0.5337048192771082. Std of Reward: 0.4072664275248118.\n",
      "Step: 1020000. Mean Reward: 0.5371893491124258. Std of Reward: 0.3951861964028943.\n",
      "Step: 1030000. Mean Reward: 0.4903987730061349. Std of Reward: 0.4803213333919053.\n",
      "Step: 1040000. Mean Reward: 0.5028612716763002. Std of Reward: 0.4778650054287421.\n",
      "Step: 1050000. Mean Reward: 0.4640462427745663. Std of Reward: 0.5201362517696285.\n",
      "Saved Model\n",
      "Step: 1060000. Mean Reward: 0.493441176470588. Std of Reward: 0.46674411626629436.\n",
      "Step: 1070000. Mean Reward: 0.45728571428571413. Std of Reward: 0.544092879226047.\n",
      "Step: 1080000. Mean Reward: 0.523801169590643. Std of Reward: 0.43114582301164234.\n",
      "Step: 1090000. Mean Reward: 0.4883035714285712. Std of Reward: 0.469352971031202.\n",
      "Step: 1100000. Mean Reward: 0.49260606060606044. Std of Reward: 0.43217266045889297.\n",
      "Saved Model\n",
      "Step: 1110000. Mean Reward: 0.4921597633136093. Std of Reward: 0.475745983427998.\n",
      "Step: 1120000. Mean Reward: 0.4852439024390245. Std of Reward: 0.5737711696711019.\n",
      "Step: 1130000. Mean Reward: 0.45325153374233107. Std of Reward: 0.4915156020285875.\n",
      "Step: 1140000. Mean Reward: 0.47009090909090884. Std of Reward: 0.46916188093601124.\n",
      "Step: 1150000. Mean Reward: 0.4561949685534592. Std of Reward: 0.5507697469800966.\n",
      "Saved Model\n",
      "Step: 1160000. Mean Reward: 0.5146666666666665. Std of Reward: 0.4125112058471805.\n",
      "Step: 1170000. Mean Reward: 0.4894339622641509. Std of Reward: 0.44826604477350124.\n",
      "Step: 1180000. Mean Reward: 0.509223602484472. Std of Reward: 0.4593073231319894.\n",
      "Step: 1190000. Mean Reward: 0.5273795180722889. Std of Reward: 0.3507417706817546.\n",
      "Step: 1200000. Mean Reward: 0.535267857142857. Std of Reward: 0.4764146412479447.\n",
      "Saved Model\n",
      "Step: 1210000. Mean Reward: 0.5217455621301774. Std of Reward: 0.38908297634348554.\n",
      "Step: 1220000. Mean Reward: 0.5512797619047616. Std of Reward: 0.3214721517524096.\n",
      "Step: 1230000. Mean Reward: 0.4892307692307691. Std of Reward: 0.4661789152944272.\n",
      "Step: 1240000. Mean Reward: 0.4770679012345679. Std of Reward: 0.5233326389066296.\n",
      "Step: 1250000. Mean Reward: 0.5738757396449703. Std of Reward: 0.30421531994744305.\n",
      "Saved Model\n",
      "Step: 1260000. Mean Reward: 0.5444827586206895. Std of Reward: 0.3895071557948307.\n",
      "Step: 1270000. Mean Reward: 0.4697802197802196. Std of Reward: 0.5135953382546173.\n",
      "Step: 1280000. Mean Reward: 0.5004670329670328. Std of Reward: 0.4812968133486324.\n",
      "Step: 1290000. Mean Reward: 0.5005586592178769. Std of Reward: 0.47300710504117305.\n",
      "Step: 1300000. Mean Reward: 0.43280423280423264. Std of Reward: 0.5899435618087967.\n",
      "Saved Model\n",
      "Step: 1310000. Mean Reward: 0.4657062146892656. Std of Reward: 0.5915974100780813.\n",
      "Step: 1320000. Mean Reward: 0.5031920903954801. Std of Reward: 0.4639161566376143.\n",
      "Step: 1330000. Mean Reward: 0.5310755813953487. Std of Reward: 0.404185054450237.\n",
      "Step: 1340000. Mean Reward: 0.5022950819672128. Std of Reward: 0.4799341716334606.\n",
      "Step: 1350000. Mean Reward: 0.5191803278688523. Std of Reward: 0.4661153674658996.\n",
      "Saved Model\n",
      "Step: 1360000. Mean Reward: 0.4895161290322579. Std of Reward: 0.49447841778366103.\n",
      "Step: 1370000. Mean Reward: 0.5251404494382023. Std of Reward: 0.5399683796857653.\n",
      "Step: 1380000. Mean Reward: 0.5326984126984126. Std of Reward: 0.46926613769469716.\n",
      "Step: 1390000. Mean Reward: 0.5441621621621621. Std of Reward: 0.4294008719583232.\n",
      "Step: 1400000. Mean Reward: 0.48137640449438185. Std of Reward: 0.46672320419161994.\n",
      "Saved Model\n",
      "Step: 1410000. Mean Reward: 0.5508152173913041. Std of Reward: 0.41618171014039945.\n",
      "Step: 1420000. Mean Reward: 0.5681147540983604. Std of Reward: 0.37834861616779714.\n",
      "Step: 1430000. Mean Reward: 0.5134782608695653. Std of Reward: 0.6013491349757673.\n",
      "Step: 1440000. Mean Reward: 0.5084473684210525. Std of Reward: 0.4804754606455448.\n",
      "Step: 1450000. Mean Reward: 0.49899999999999983. Std of Reward: 0.49342153460384236.\n",
      "Saved Model\n",
      "Step: 1460000. Mean Reward: 0.5005214723926382. Std of Reward: 0.6785824301951875.\n",
      "Step: 1470000. Mean Reward: 0.5284285714285715. Std of Reward: 0.5583191246034452.\n",
      "Step: 1480000. Mean Reward: 0.4618292682926832. Std of Reward: 0.6820423115563423.\n",
      "Step: 1490000. Mean Reward: 0.4801142857142858. Std of Reward: 0.6419743996421622.\n",
      "Step: 1500000. Mean Reward: 0.45887096774193603. Std of Reward: 0.8314818773628961.\n",
      "Saved Model\n",
      "Step: 1510000. Mean Reward: 0.4840123456790127. Std of Reward: 0.7496126241381826.\n",
      "Step: 1520000. Mean Reward: 0.5251754385964913. Std of Reward: 0.5755670722004631.\n",
      "Step: 1530000. Mean Reward: 0.617942857142857. Std of Reward: 0.22262344156600822.\n",
      "Step: 1540000. Mean Reward: 0.5935260115606935. Std of Reward: 0.3121267633116741.\n",
      "Step: 1550000. Mean Reward: 0.5563005780346822. Std of Reward: 0.5294540342586188.\n",
      "Saved Model\n",
      "Step: 1560000. Mean Reward: 0.5361607142857143. Std of Reward: 0.5112967795615023.\n",
      "Step: 1570000. Mean Reward: 0.6220505617977526. Std of Reward: 0.21042328321186782.\n",
      "Step: 1580000. Mean Reward: 0.5786388888888887. Std of Reward: 0.36164855198703755.\n",
      "Step: 1590000. Mean Reward: 0.5114124293785313. Std of Reward: 0.617387410045585.\n",
      "Step: 1600000. Mean Reward: 0.5416129032258064. Std of Reward: 0.42727579248567243.\n",
      "Saved Model\n",
      "Step: 1610000. Mean Reward: 0.5073513513513511. Std of Reward: 0.47702882321076734.\n",
      "Step: 1620000. Mean Reward: 0.5872965116279069. Std of Reward: 0.26122996059681336.\n",
      "Step: 1630000. Mean Reward: 0.5831714285714283. Std of Reward: 0.25547731078616476.\n",
      "Step: 1640000. Mean Reward: 0.493922155688623. Std of Reward: 0.7012969421906009.\n",
      "Step: 1650000. Mean Reward: 0.45849112426035527. Std of Reward: 0.6891214724323786.\n",
      "Saved Model\n",
      "Step: 1660000. Mean Reward: 0.5186127167630058. Std of Reward: 0.5262717437575274.\n",
      "Step: 1670000. Mean Reward: 0.5286388888888888. Std of Reward: 0.43578945940912656.\n",
      "Step: 1680000. Mean Reward: 0.47909883720930235. Std of Reward: 0.5829814375349129.\n",
      "Step: 1690000. Mean Reward: 0.4207062146892657. Std of Reward: 0.6954671101215419.\n",
      "Step: 1700000. Mean Reward: 0.40480113636363657. Std of Reward: 0.7480908201722419.\n",
      "Saved Model\n",
      "Step: 1710000. Mean Reward: 0.5101734104046245. Std of Reward: 0.6589354472830045.\n",
      "Step: 1720000. Mean Reward: 0.5165168539325841. Std of Reward: 0.45792914697421166.\n",
      "Step: 1730000. Mean Reward: 0.43477528089887657. Std of Reward: 0.7047449176400811.\n",
      "Step: 1740000. Mean Reward: 0.5137765957446807. Std of Reward: 0.4766632058305595.\n",
      "Step: 1750000. Mean Reward: 0.5514245810055866. Std of Reward: 0.5004975833149976.\n",
      "Saved Model\n",
      "Step: 1760000. Mean Reward: 0.5133620689655174. Std of Reward: 0.633149435746823.\n",
      "Step: 1770000. Mean Reward: 0.5203693181818182. Std of Reward: 0.5478631060057435.\n",
      "Step: 1780000. Mean Reward: 0.49340782122905025. Std of Reward: 0.5677991974755784.\n",
      "Step: 1790000. Mean Reward: 0.5575806451612902. Std of Reward: 0.4123777843734372.\n",
      "Step: 1800000. Mean Reward: 0.5647790055248617. Std of Reward: 0.3741520147840538.\n",
      "Saved Model\n",
      "Step: 1810000. Mean Reward: 0.5871428571428569. Std of Reward: 0.33297909618012905.\n",
      "Step: 1820000. Mean Reward: 0.5674702380952381. Std of Reward: 0.4951701237909843.\n",
      "Step: 1830000. Mean Reward: 0.5635000000000001. Std of Reward: 0.4640936296253393.\n",
      "Step: 1840000. Mean Reward: 0.6008426966292133. Std of Reward: 0.25672881240877127.\n",
      "Step: 1850000. Mean Reward: 0.5693604651162791. Std of Reward: 0.45578822379326506.\n",
      "Saved Model\n",
      "Step: 1860000. Mean Reward: 0.5644207317073173. Std of Reward: 0.5247273971096896.\n",
      "Step: 1870000. Mean Reward: 0.5936944444444443. Std of Reward: 0.31620772176287715.\n",
      "Step: 1880000. Mean Reward: 0.6202247191011234. Std of Reward: 0.21672149853738992.\n",
      "Step: 1890000. Mean Reward: 0.5810919540229884. Std of Reward: 0.32270313506927406.\n",
      "Step: 1900000. Mean Reward: 0.5931656804733728. Std of Reward: 0.4370106315372723.\n",
      "Saved Model\n",
      "Step: 1910000. Mean Reward: 0.5750892857142857. Std of Reward: 0.4287202277648282.\n",
      "Step: 1920000. Mean Reward: 0.6064367816091953. Std of Reward: 0.17896478823702286.\n",
      "Step: 1930000. Mean Reward: 0.5770180722891567. Std of Reward: 0.3786099253545861.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1940000. Mean Reward: 0.5913235294117646. Std of Reward: 0.38153580670666704.\n",
      "Step: 1950000. Mean Reward: 0.5932035928143713. Std of Reward: 0.3539728420673288.\n",
      "Saved Model\n",
      "Step: 1960000. Mean Reward: 0.5675297619047619. Std of Reward: 0.4066599568961752.\n",
      "Step: 1970000. Mean Reward: 0.5554705882352942. Std of Reward: 0.46690981843040175.\n",
      "Step: 1980000. Mean Reward: 0.5307975460122701. Std of Reward: 0.584570243901726.\n",
      "Step: 1990000. Mean Reward: 0.503814102564103. Std of Reward: 0.6472922179681582.\n",
      "Step: 2000000. Mean Reward: 0.5168639053254439. Std of Reward: 0.5429420069384528.\n",
      "Saved Model\n",
      "Step: 2010000. Mean Reward: 0.5435542168674701. Std of Reward: 0.5673108357945625.\n",
      "Step: 2020000. Mean Reward: 0.5450887573964497. Std of Reward: 0.4321309087529359.\n",
      "Step: 2030000. Mean Reward: 0.5528012048192772. Std of Reward: 0.45178743028909163.\n",
      "Step: 2040000. Mean Reward: 0.5688596491228068. Std of Reward: 0.29340362626247557.\n",
      "Step: 2050000. Mean Reward: 0.6177556818181816. Std of Reward: 0.2154479806219603.\n",
      "Saved Model\n",
      "Step: 2060000. Mean Reward: 0.5304519774011297. Std of Reward: 0.3824823310553524.\n",
      "Step: 2070000. Mean Reward: 0.6093023255813953. Std of Reward: 0.2182510592833689.\n",
      "Step: 2080000. Mean Reward: 0.5683235294117644. Std of Reward: 0.2917185106337388.\n",
      "Step: 2090000. Mean Reward: 0.570764705882353. Std of Reward: 0.4292568858486038.\n",
      "Step: 2100000. Mean Reward: 0.6114857142857141. Std of Reward: 0.20914197111443858.\n",
      "Saved Model\n",
      "Step: 2110000. Mean Reward: 0.5802259887005647. Std of Reward: 0.31604495282466283.\n",
      "Step: 2120000. Mean Reward: 0.5632880434782608. Std of Reward: 0.3966763118447845.\n",
      "Step: 2130000. Mean Reward: 0.5864080459770116. Std of Reward: 0.4587484850971589.\n",
      "Step: 2140000. Mean Reward: 0.5861079545454544. Std of Reward: 0.2878289516210464.\n",
      "Step: 2150000. Mean Reward: 0.5940055248618784. Std of Reward: 0.277610870733814.\n",
      "Saved Model\n",
      "Step: 2160000. Mean Reward: 0.5771703296703296. Std of Reward: 0.3263990825625794.\n",
      "Step: 2170000. Mean Reward: 0.5815934065934064. Std of Reward: 0.3357486133777747.\n",
      "Step: 2180000. Mean Reward: 0.599325842696629. Std of Reward: 0.28595021077107147.\n",
      "Step: 2190000. Mean Reward: 0.5678571428571427. Std of Reward: 0.368232071814387.\n",
      "Step: 2200000. Mean Reward: 0.531794117647059. Std of Reward: 0.5065252319022522.\n",
      "Saved Model\n",
      "Step: 2210000. Mean Reward: 0.5620111731843573. Std of Reward: 0.36007623322000654.\n",
      "Step: 2220000. Mean Reward: 0.5819836956521738. Std of Reward: 0.3592080733385033.\n",
      "Step: 2230000. Mean Reward: 0.5955337078651685. Std of Reward: 0.2885971911703582.\n",
      "Step: 2240000. Mean Reward: 0.57010989010989. Std of Reward: 0.3524704632796957.\n",
      "Step: 2250000. Mean Reward: 0.5740384615384614. Std of Reward: 0.3522299973712951.\n",
      "Saved Model\n",
      "Step: 2260000. Mean Reward: 0.5329838709677417. Std of Reward: 0.4241295257094983.\n",
      "Step: 2270000. Mean Reward: 0.5309705882352942. Std of Reward: 0.5686915573228063.\n",
      "Step: 2280000. Mean Reward: 0.5973011363636362. Std of Reward: 0.2558230149902637.\n",
      "Step: 2290000. Mean Reward: 0.596222222222222. Std of Reward: 0.2768631943669323.\n",
      "Step: 2300000. Mean Reward: 0.5790983606557375. Std of Reward: 0.32596578845519075.\n",
      "Saved Model\n",
      "Step: 2310000. Mean Reward: 0.5565846994535517. Std of Reward: 0.3510237987756341.\n",
      "Step: 2320000. Mean Reward: 0.5710588235294116. Std of Reward: 0.43676894828598983.\n",
      "Step: 2330000. Mean Reward: 0.5600301204819278. Std of Reward: 0.5226667880991775.\n",
      "Step: 2340000. Mean Reward: 0.5099411764705883. Std of Reward: 0.5847885460562758.\n",
      "Step: 2350000. Mean Reward: 0.5708791208791207. Std of Reward: 0.3297702275382902.\n",
      "Saved Model\n",
      "Step: 2360000. Mean Reward: 0.45163522012578655. Std of Reward: 0.6847219737974298.\n",
      "Step: 2370000. Mean Reward: 0.5983050847457625. Std of Reward: 0.24636271002203156.\n",
      "Step: 2380000. Mean Reward: 0.5604213483146068. Std of Reward: 0.4907346212903792.\n",
      "Step: 2390000. Mean Reward: 0.5652298850574711. Std of Reward: 0.31218815064914607.\n",
      "Step: 2400000. Mean Reward: 0.6168156424581005. Std of Reward: 0.20888818366500914.\n",
      "Saved Model\n",
      "Step: 2410000. Mean Reward: 0.6019653179190749. Std of Reward: 0.21953882024562563.\n",
      "Step: 2420000. Mean Reward: 0.5927906976744186. Std of Reward: 0.40255297546161484.\n",
      "Step: 2430000. Mean Reward: 0.5824316939890709. Std of Reward: 0.32930248270504986.\n",
      "Step: 2440000. Mean Reward: 0.5436813186813184. Std of Reward: 0.3880777048937799.\n",
      "Step: 2450000. Mean Reward: 0.6067159763313609. Std of Reward: 0.33858959436668207.\n",
      "Saved Model\n",
      "Step: 2460000. Mean Reward: 0.5776666666666664. Std of Reward: 0.33294327177797456.\n",
      "Step: 2470000. Mean Reward: 0.5433888888888889. Std of Reward: 0.5437152685090556.\n",
      "Step: 2480000. Mean Reward: 0.5727873563218391. Std of Reward: 0.4583943938355983.\n",
      "Step: 2490000. Mean Reward: 0.5494021739130432. Std of Reward: 0.3663694102649439.\n",
      "Step: 2500000. Mean Reward: 0.5816486486486485. Std of Reward: 0.32442274857887377.\n",
      "Saved Model\n",
      "Step: 2510000. Mean Reward: 0.6209562841530053. Std of Reward: 0.2482755569670228.\n",
      "Step: 2520000. Mean Reward: 0.5804644808743168. Std of Reward: 0.32856523268046767.\n",
      "Step: 2530000. Mean Reward: 0.5810597826086955. Std of Reward: 0.3315216756503859.\n",
      "Step: 2540000. Mean Reward: 0.5606010928961748. Std of Reward: 0.34562830946671097.\n",
      "Step: 2550000. Mean Reward: 0.5911621621621621. Std of Reward: 0.30109642107556556.\n",
      "Saved Model\n",
      "Step: 2560000. Mean Reward: 0.47698224852071064. Std of Reward: 0.7717873321553089.\n",
      "Step: 2570000. Mean Reward: 0.5750268817204298. Std of Reward: 0.3553105174259582.\n",
      "Step: 2580000. Mean Reward: 0.581730769230769. Std of Reward: 0.3336258499195965.\n",
      "Step: 2590000. Mean Reward: 0.5527005347593581. Std of Reward: 0.3883164073866868.\n",
      "Step: 2600000. Mean Reward: 0.5871229050279331. Std of Reward: 0.510033098367806.\n",
      "Saved Model\n",
      "Step: 2610000. Mean Reward: 0.589162162162162. Std of Reward: 0.32786199416362166.\n",
      "Step: 2620000. Mean Reward: 0.6038797814207648. Std of Reward: 0.2797187830113508.\n",
      "Step: 2630000. Mean Reward: 0.5884916201117317. Std of Reward: 0.2812029500054874.\n",
      "Step: 2640000. Mean Reward: 0.6234254143646407. Std of Reward: 0.20805699711916123.\n",
      "Step: 2650000. Mean Reward: 0.6106906077348064. Std of Reward: 0.2463831032319266.\n",
      "Saved Model\n",
      "Step: 2660000. Mean Reward: 0.6201098901098899. Std of Reward: 0.24990984970406588.\n",
      "Step: 2670000. Mean Reward: 0.63603825136612. Std of Reward: 0.21121108924460646.\n",
      "Step: 2680000. Mean Reward: 0.6291988950276243. Std of Reward: 0.17212209529687578.\n",
      "Step: 2690000. Mean Reward: 0.5865573770491802. Std of Reward: 0.301981445052443.\n",
      "Step: 2700000. Mean Reward: 0.627861111111111. Std of Reward: 0.2098657180169392.\n",
      "Saved Model\n",
      "Step: 2710000. Mean Reward: 0.5980662983425413. Std of Reward: 0.28102440078501006.\n",
      "Step: 2720000. Mean Reward: 0.6071621621621619. Std of Reward: 0.27316844753825464.\n",
      "Step: 2730000. Mean Reward: 0.5954891304347825. Std of Reward: 0.30642181053053175.\n",
      "Step: 2740000. Mean Reward: 0.5869505494505493. Std of Reward: 0.31072082585590755.\n",
      "Step: 2750000. Mean Reward: 0.5807567567567566. Std of Reward: 0.3516016945970759.\n",
      "Saved Model\n",
      "Step: 2760000. Mean Reward: 0.5745721925133689. Std of Reward: 0.3511331220309065.\n",
      "Step: 2770000. Mean Reward: 0.5904594594594593. Std of Reward: 0.32802633336897147.\n",
      "Step: 2780000. Mean Reward: 0.5834308510638296. Std of Reward: 0.3230416620672798.\n",
      "Step: 2790000. Mean Reward: 0.5770348837209304. Std of Reward: 0.4519781314856542.\n",
      "Step: 2800000. Mean Reward: 0.604005681818182. Std of Reward: 0.5010908611260807.\n",
      "Saved Model\n",
      "Step: 2810000. Mean Reward: 0.5641081081081079. Std of Reward: 0.3742446651506445.\n",
      "Step: 2820000. Mean Reward: 0.5977567567567565. Std of Reward: 0.30616630398786454.\n",
      "Step: 2830000. Mean Reward: 0.528096590909091. Std of Reward: 0.5787323890799554.\n",
      "Step: 2840000. Mean Reward: 0.5423546511627908. Std of Reward: 0.5313840603286736.\n",
      "Step: 2850000. Mean Reward: 0.6219230769230768. Std of Reward: 0.2796748202320889.\n",
      "Saved Model\n",
      "Step: 2860000. Mean Reward: 0.5752890173410407. Std of Reward: 0.525132163651537.\n",
      "Step: 2870000. Mean Reward: 0.5022812500000003. Std of Reward: 0.7306623379841286.\n",
      "Step: 2880000. Mean Reward: 0.501897590361446. Std of Reward: 0.6295893060809887.\n",
      "Step: 2890000. Mean Reward: 0.5200000000000001. Std of Reward: 0.6585252370295881.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2900000. Mean Reward: 0.5935838150289017. Std of Reward: 0.3830145715898863.\n",
      "Saved Model\n",
      "Step: 2910000. Mean Reward: 0.5348802395209582. Std of Reward: 0.6268462436693186.\n",
      "Step: 2920000. Mean Reward: 0.5664912280701756. Std of Reward: 0.5271655129019978.\n",
      "Step: 2930000. Mean Reward: 0.5677167630057803. Std of Reward: 0.4445977796534272.\n",
      "Step: 2940000. Mean Reward: 0.6417403314917125. Std of Reward: 0.16811888709058007.\n",
      "Step: 2950000. Mean Reward: 0.6252259887005648. Std of Reward: 0.11168562012482428.\n",
      "Saved Model\n",
      "Step: 2960000. Mean Reward: 0.6080898876404492. Std of Reward: 0.2181748932738219.\n",
      "Step: 2970000. Mean Reward: 0.6108707865168538. Std of Reward: 0.21345591377167444.\n",
      "Step: 2980000. Mean Reward: 0.6013999999999998. Std of Reward: 0.21751692216600402.\n",
      "Step: 2990000. Mean Reward: 0.5996306818181817. Std of Reward: 0.22454880258074017.\n",
      "Step: 3000000. Mean Reward: 0.5927049180327867. Std of Reward: 0.30509951852685896.\n",
      "Saved Model\n",
      "Step: 3010000. Mean Reward: 0.6112290502793294. Std of Reward: 0.2448689035753707.\n",
      "Step: 3020000. Mean Reward: 0.606270718232044. Std of Reward: 0.24798032508399914.\n",
      "Step: 3030000. Mean Reward: 0.5930056179775279. Std of Reward: 0.25082915303724596.\n",
      "Step: 3040000. Mean Reward: 0.533353293413174. Std of Reward: 0.5835762251095411.\n",
      "Step: 3050000. Mean Reward: 0.5897159090909089. Std of Reward: 0.2802184192271692.\n",
      "Saved Model\n",
      "Step: 3060000. Mean Reward: 0.5690178571428571. Std of Reward: 0.4782860206385536.\n",
      "Step: 3070000. Mean Reward: 0.4121875000000005. Std of Reward: 0.813643750264048.\n",
      "Step: 3080000. Mean Reward: 0.5274074074074075. Std of Reward: 0.5539299054862821.\n",
      "Step: 3090000. Mean Reward: 0.4224679487179493. Std of Reward: 0.7868737837396871.\n",
      "Step: 3100000. Mean Reward: 0.4212658227848106. Std of Reward: 0.797340900504121.\n",
      "Saved Model\n",
      "Step: 3110000. Mean Reward: 0.5730113636363634. Std of Reward: 0.30174307984526194.\n",
      "Step: 3120000. Mean Reward: 0.5535714285714284. Std of Reward: 0.3718928311556868.\n",
      "Step: 3130000. Mean Reward: 0.5666857142857142. Std of Reward: 0.4713459615931834.\n",
      "Step: 3140000. Mean Reward: 0.5883426966292133. Std of Reward: 0.2843703481704288.\n",
      "Step: 3150000. Mean Reward: 0.5330994152046785. Std of Reward: 0.5112741068534934.\n",
      "Saved Model\n",
      "Step: 3160000. Mean Reward: 0.5095161290322584. Std of Reward: 0.640926188240197.\n",
      "Step: 3170000. Mean Reward: 0.4331168831168836. Std of Reward: 0.7646780321362963.\n",
      "Step: 3180000. Mean Reward: 0.5015806451612904. Std of Reward: 0.6054219415971996.\n",
      "Step: 3190000. Mean Reward: 0.5105654761904762. Std of Reward: 0.6045005872607407.\n",
      "Step: 3200000. Mean Reward: 0.5382267441860464. Std of Reward: 0.4648126288973031.\n",
      "Saved Model\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "unpack requires a bytes object of length 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-fea09393b070>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_progress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;31m# Decide and take an action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mnew_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_experiences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_horizon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UNITY\\PPOL\\python\\ppo\\trainer.py\u001b[0m in \u001b[0;36mtake_action\u001b[1;34m(self, info, env, brain_name, steps, normalize)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entropy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearn_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[0mnew_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_experiences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UNITY\\PPOL\\python\\unityagents\\environment.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action, memory, value)\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb\"STEP\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 436\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    437\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_loaded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mUnityEnvironmentException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No Unity environment is loaded.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UNITY\\PPOL\\python\\unityagents\\environment.py\u001b[0m in \u001b[0;36m_get_state\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_brains\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m             \u001b[0mstate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"brain_name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[0mn_agent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"agents\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UNITY\\PPOL\\python\\unityagents\\environment.py\u001b[0m in \u001b[0;36m_get_state_dict\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \"\"\"\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb\"RECEIVED\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[0mstate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UNITY\\PPOL\\python\\unityagents\\environment.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m             \u001b[0mmessage_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"I\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mmessage_length\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: unpack requires a bytes object of length 4"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if curriculum_file == \"None\":\n",
    "    curriculum_file = None\n",
    "\n",
    "\n",
    "def get_progress():\n",
    "    if curriculum_file is not None:\n",
    "        if env._curriculum.measure_type == \"progress\":\n",
    "            return steps / max_steps\n",
    "        elif env._curriculum.measure_type == \"reward\":\n",
    "            return last_reward\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "ppo_model = create_agent_model(env, lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps, \n",
    "                               normalize=normalize, num_layers=num_layers)\n",
    "\n",
    "is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "\n",
    "model_path = './models/{}'.format(run_path)\n",
    "summary_path = './summaries/{}'.format(run_path)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps, last_reward = sess.run([ppo_model.global_step, ppo_model.last_reward])    \n",
    "    summary_writer = tf.summary.FileWriter(summary_path)\n",
    "    info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "    trainer = Trainer(ppo_model, sess, info, is_continuous, use_observations, use_states, train_model)\n",
    "    if train_model:\n",
    "        trainer.write_text(summary_writer, 'Hyperparameters', hyperparameter_dict, steps)\n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "        # Decide and take an action\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps, normalize)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        if len(trainer.training_buffer['actions']) > buffer_size and train_model:\n",
    "            # Perform gradient descent with experience buffer\n",
    "            trainer.update_model(batch_size, num_epoch)\n",
    "        if steps % summary_freq == 0 and steps != 0 and train_model:\n",
    "            # Write training statistics to tensorboard.\n",
    "            trainer.write_summary(summary_writer, steps, env._curriculum.lesson_number)\n",
    "        if steps % save_freq == 0 and steps != 0 and train_model:\n",
    "            # Save Tensorflow model\n",
    "            save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)\n",
    "        if len(trainer.stats['cumulative_reward']) > 0:\n",
    "            mean_reward = np.mean(trainer.stats['cumulative_reward'])\n",
    "            sess.run(ppo_model.update_reward, feed_dict={ppo_model.new_reward: mean_reward})\n",
    "            last_reward = sess.run(ppo_model.last_reward)\n",
    "    # Final save Tensorflow model\n",
    "    if steps != 0 and train_model:\n",
    "        save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "env.close()\n",
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/ppo\\model-3200000.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/ppo\\model-3200000.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 4 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 4 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 4 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
