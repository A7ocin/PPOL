{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from ppo.history import *\n",
    "from ppo.models import *\n",
    "from ppo.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### General parameters\n",
    "max_steps = 5e10 ##5e5 # Set maximum number of steps to run environment.\n",
    "run_path = \"ppo\" # The sub-directory name for model and summary statistics\n",
    "load_model = True # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model.\n",
    "summary_freq = 10000 # Frequency at which to save training statistics.\n",
    "save_freq = 50000 # Frequency at which to save model.\n",
    "env_name = \"Pedestrians\" # Name of the training environment file.\n",
    "curriculum_file = None\n",
    "\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.99 #0.99 # Reward discount rate.\n",
    "lambd = 0.95 ##0.95 # Lambda parameter for GAE.\n",
    "time_horizon = 2048 ##2048 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 1e-3 ##1e-3 # Strength of entropy regularization\n",
    "num_epoch = 5 ##5 # Number of gradient descent steps per batch of experiences.\n",
    "num_layers = 2 ##2 # Number of hidden layers between state/observation encoding and value/policy layers.\n",
    "epsilon = 0.2 ##0.2 # Acceptable threshold around ratio of old and new policy probabilities.\n",
    "buffer_size = 4096 ##2048 # How large the experience buffer should be before gradient descent.\n",
    "learning_rate = 3e-4 # Model learning rate.\n",
    "hidden_units = 128 ##64 # Number of units in hidden layer.\n",
    "batch_size = 32 ##64 # How many experiences per gradient descent update step.\n",
    "normalize = False\n",
    "\n",
    "### Logging dictionary for hyperparameters\n",
    "hyperparameter_dict = {'max_steps':max_steps, 'run_path':run_path, 'env_name':env_name,\n",
    "    'curriculum_file':curriculum_file, 'gamma':gamma, 'lambd':lambd, 'time_horizon':time_horizon,\n",
    "    'beta':beta, 'num_epoch':num_epoch, 'epsilon':epsilon, 'buffe_size':buffer_size,\n",
    "    'leaning_rate':learning_rate, 'hidden_units':hidden_units, 'batch_size':batch_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'PedestriansAcademy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: PedestriansAcademy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: PedestriansBrain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 9\n",
      "        Action space type: discrete\n",
      "        Action space size (per agent): 4\n",
      "        Memory space size (per agent): 0\n",
      "        Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name, curriculum=curriculum_file)\n",
    "print(str(env))\n",
    "brain_name = env.external_brain_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "INFO:tensorflow:Restoring parameters from ./models/ppo\\model-3600000.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/ppo\\model-3600000.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "Step: 3610000. Mean Reward: 0.7126975918195877. Std of Reward: 0.21236483353984448.\n",
      "Step: 3620000. Mean Reward: 0.6780392131960785. Std of Reward: 0.3288156612324075.\n",
      "Step: 3630000. Mean Reward: 0.6850343614072167. Std of Reward: 0.407820245536916.\n",
      "Step: 3640000. Mean Reward: 0.67519607597549. Std of Reward: 0.331236525498816.\n",
      "Step: 3650000. Mean Reward: 0.6742746089430052. Std of Reward: 0.33102011324137104.\n",
      "Saved Model\n",
      "Step: 3660000. Mean Reward: 0.6411881162574257. Std of Reward: 0.40931641167797533.\n",
      "Step: 3670000. Mean Reward: 0.6745622871717172. Std of Reward: 0.32825693298074704.\n",
      "Step: 3680000. Mean Reward: 0.6786217924567308. Std of Reward: 0.349322905839817.\n",
      "Step: 3690000. Mean Reward: 0.6772032495707316. Std of Reward: 0.32839138899376336.\n",
      "Step: 3700000. Mean Reward: 0.679224997595. Std of Reward: 0.32072980463762474.\n",
      "Saved Model\n",
      "Step: 3710000. Mean Reward: 0.6926178837560975. Std of Reward: 0.30371552387468037.\n",
      "Step: 3720000. Mean Reward: 0.6272307668564103. Std of Reward: 0.44684391250246275.\n",
      "Step: 3730000. Mean Reward: 0.643398371507317. Std of Reward: 0.416815480312037.\n",
      "Step: 3740000. Mean Reward: 0.6596617135990098. Std of Reward: 0.38517231195403.\n",
      "Step: 3750000. Mean Reward: 0.6860217733015075. Std of Reward: 0.29632070994401793.\n",
      "Saved Model\n",
      "Step: 3760000. Mean Reward: 0.6311030570676329. Std of Reward: 0.44113800756686106.\n",
      "Step: 3770000. Mean Reward: 0.631666664296875. Std of Reward: 0.4090260560930681.\n",
      "Step: 3780000. Mean Reward: 0.6727495745837563. Std of Reward: 0.32252648519332394.\n",
      "Step: 3790000. Mean Reward: 0.6291443826042787. Std of Reward: 0.7810555373055657.\n",
      "Step: 3800000. Mean Reward: 0.6627310205792079. Std of Reward: 0.3965889107504053.\n",
      "Saved Model\n",
      "Step: 3810000. Mean Reward: 0.696058330875. Std of Reward: 0.29200818922240496.\n",
      "Step: 3820000. Mean Reward: 0.6355344177173918. Std of Reward: 0.6034245325000935.\n",
      "Step: 3830000. Mean Reward: 0.6354003243970588. Std of Reward: 0.426005813766318.\n",
      "Step: 3840000. Mean Reward: 0.6782244531809044. Std of Reward: 0.3262324083843609.\n",
      "Step: 3850000. Mean Reward: 0.6469211797290639. Std of Reward: 0.39883423877381685.\n",
      "Saved Model\n",
      "Step: 3860000. Mean Reward: 0.6939603936336632. Std of Reward: 0.28008469073410563.\n",
      "Step: 3870000. Mean Reward: 0.7098109939639177. Std of Reward: 0.3038782233360125.\n",
      "Step: 3880000. Mean Reward: 0.6918974335794873. Std of Reward: 0.3046990306868637.\n",
      "Step: 3890000. Mean Reward: 0.6935299119846154. Std of Reward: 0.2725033855119352.\n",
      "Step: 3900000. Mean Reward: 0.7128721656796114. Std of Reward: 0.24558686351750805.\n",
      "Saved Model\n",
      "Step: 3910000. Mean Reward: 0.6182601601463414. Std of Reward: 0.4724498882412961.\n",
      "Step: 3920000. Mean Reward: 0.6527240118494627. Std of Reward: 0.506399151676728.\n",
      "Step: 3930000. Mean Reward: 0.6668599009323672. Std of Reward: 0.3679165712661522.\n",
      "Step: 3940000. Mean Reward: 0.6962249973749999. Std of Reward: 0.28251605037593913.\n",
      "Step: 3950000. Mean Reward: 0.6652677002849741. Std of Reward: 0.45003777165138803.\n",
      "Saved Model\n",
      "Step: 3960000. Mean Reward: 0.6920915007303919. Std of Reward: 0.2793838229360858.\n",
      "Step: 3970000. Mean Reward: 0.6974019580882352. Std of Reward: 0.29382182950704017.\n",
      "Step: 3980000. Mean Reward: 0.7124140866443298. Std of Reward: 0.16030603148629685.\n",
      "Step: 3990000. Mean Reward: 0.7005213243791469. Std of Reward: 0.29702344107692963.\n",
      "Step: 4000000. Mean Reward: 0.6967525747938146. Std of Reward: 0.27692762591810466.\n",
      "Saved Model\n",
      "Step: 4010000. Mean Reward: 0.6835738803711344. Std of Reward: 0.5456928081924154.\n",
      "Step: 4020000. Mean Reward: 0.6359587789354838. Std of Reward: 0.40274664516624586.\n",
      "Step: 4030000. Mean Reward: 0.6993609645854921. Std of Reward: 0.2564606093804478.\n",
      "Step: 4040000. Mean Reward: 0.6925687257422685. Std of Reward: 0.5964962388826011.\n",
      "Step: 4050000. Mean Reward: 0.7027668284039409. Std of Reward: 0.2562638261795989.\n",
      "Saved Model\n",
      "Step: 4060000. Mean Reward: 0.6544865294696971. Std of Reward: 0.3931127434005641.\n",
      "Step: 4070000. Mean Reward: 0.6648663493160377. Std of Reward: 0.39306287387787875.\n",
      "Step: 4080000. Mean Reward: 0.6616666641095237. Std of Reward: 0.396684773438798.\n",
      "Step: 4090000. Mean Reward: 0.6447512412288559. Std of Reward: 0.49386682747290184.\n",
      "Step: 4100000. Mean Reward: 0.6382728817616583. Std of Reward: 0.6084861962557903.\n",
      "Saved Model\n",
      "Step: 4110000. Mean Reward: 0.5029369894451232. Std of Reward: 1.0332878768053568.\n",
      "Step: 4120000. Mean Reward: 0.6417569761921184. Std of Reward: 0.420737135476256.\n",
      "Step: 4130000. Mean Reward: 0.5935341337590359. Std of Reward: 1.0588269219547446.\n",
      "Step: 4140000. Mean Reward: 0.648988388716418. Std of Reward: 0.40854996343541.\n",
      "Step: 4150000. Mean Reward: 0.6667182101701036. Std of Reward: 0.6979299061655063.\n",
      "Saved Model\n",
      "Step: 4160000. Mean Reward: 0.6041212094303043. Std of Reward: 0.8570549873422902.\n",
      "Step: 4170000. Mean Reward: 0.6458949070591132. Std of Reward: 0.4141689085509997.\n",
      "Step: 4180000. Mean Reward: 0.6565339941691543. Std of Reward: 0.3813522373195442.\n",
      "Step: 4190000. Mean Reward: 0.6449329957939699. Std of Reward: 0.4692065909057276.\n",
      "Step: 4200000. Mean Reward: 0.6403732613281254. Std of Reward: 0.5591134339956125.\n",
      "Saved Model\n",
      "Step: 4210000. Mean Reward: 0.6762192957421055. Std of Reward: 0.4463670000042465.\n",
      "Step: 4220000. Mean Reward: 0.6252724332740384. Std of Reward: 0.45065983828029743.\n",
      "Step: 4230000. Mean Reward: 0.5954787207553195. Std of Reward: 0.5852555270139256.\n",
      "Step: 4240000. Mean Reward: 0.626044773641791. Std of Reward: 0.4555454180978312.\n",
      "Step: 4250000. Mean Reward: 0.5850470782485884. Std of Reward: 0.8447310461540299.\n",
      "Saved Model\n",
      "Step: 4260000. Mean Reward: 0.5072097352022483. Std of Reward: 1.0108058416862553.\n",
      "Step: 4270000. Mean Reward: 0.5483544280000013. Std of Reward: 0.9814567656696863.\n",
      "Step: 4280000. Mean Reward: 0.6492147409951923. Std of Reward: 0.4834452486889867.\n",
      "Step: 4290000. Mean Reward: 0.6628904966135265. Std of Reward: 0.46212467003982544.\n",
      "Step: 4300000. Mean Reward: 0.5241447344868421. Std of Reward: 1.2244974050299224.\n",
      "Saved Model\n",
      "Step: 4310000. Mean Reward: 0.6353873846378383. Std of Reward: 0.6009243003951306.\n",
      "Step: 4320000. Mean Reward: 0.5898417109050286. Std of Reward: 0.7166359760966244.\n",
      "Step: 4330000. Mean Reward: 0.6528597424207655. Std of Reward: 0.6624834511208512.\n",
      "Step: 4340000. Mean Reward: 0.6377938318959544. Std of Reward: 0.7261267444163361.\n",
      "Step: 4350000. Mean Reward: 0.5360149547179508. Std of Reward: 1.1850256881826127.\n",
      "Saved Model\n",
      "Step: 4360000. Mean Reward: 0.5258932438300672. Std of Reward: 1.0581115768004359.\n",
      "Step: 4370000. Mean Reward: 0.6050457852032971. Std of Reward: 0.717252452868216.\n",
      "Step: 4380000. Mean Reward: 0.7242546036834172. Std of Reward: 0.22010165903649614.\n",
      "Step: 4390000. Mean Reward: 0.4914341059651173. Std of Reward: 0.8730661964932251.\n",
      "Step: 4400000. Mean Reward: 0.5442489687962976. Std of Reward: 0.969288926483375.\n",
      "Saved Model\n",
      "Step: 4410000. Mean Reward: 0.6029012318209885. Std of Reward: 0.7402299754906811.\n",
      "Step: 4420000. Mean Reward: 0.48060090462585203. Std of Reward: 1.0460567342594507.\n",
      "Step: 4430000. Mean Reward: 0.55003225575484. Std of Reward: 0.9841152431893697.\n",
      "Step: 4440000. Mean Reward: 0.5540445834203843. Std of Reward: 1.551398761333336.\n",
      "Step: 4450000. Mean Reward: 0.6908710192763818. Std of Reward: 0.374387115881624.\n",
      "Saved Model\n",
      "Step: 4460000. Mean Reward: 0.4978494602516145. Std of Reward: 1.000326505013228.\n",
      "Step: 4470000. Mean Reward: 0.5660390921358036. Std of Reward: 0.9388515407516255.\n",
      "Step: 4480000. Mean Reward: 0.5351091244464292. Std of Reward: 0.8093755170569672.\n",
      "Step: 4490000. Mean Reward: 0.6186111084611119. Std of Reward: 0.7387433124853592.\n",
      "Step: 4500000. Mean Reward: 0.6424043691202189. Std of Reward: 0.5696499356614497.\n",
      "Saved Model\n",
      "Step: 4510000. Mean Reward: 0.6147988476034488. Std of Reward: 0.677074462788107.\n",
      "Step: 4520000. Mean Reward: 0.6176285687371432. Std of Reward: 0.6603604675682094.\n",
      "Step: 4530000. Mean Reward: 0.6431239067958117. Std of Reward: 0.5196034724720691.\n",
      "Step: 4540000. Mean Reward: 0.5716472841860469. Std of Reward: 0.7314694197323311.\n",
      "Step: 4550000. Mean Reward: 0.583598036617648. Std of Reward: 0.8116688072775116.\n",
      "Saved Model\n",
      "Step: 4560000. Mean Reward: 0.6087570597005653. Std of Reward: 0.5929714867390531.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4570000. Mean Reward: 0.4445486084444462. Std of Reward: 1.1781051560265845.\n",
      "Step: 4580000. Mean Reward: 0.6382774647150842. Std of Reward: 0.5877711186946408.\n",
      "Step: 4590000. Mean Reward: 0.7128630337029702. Std of Reward: 0.2292594893626741.\n",
      "Step: 4600000. Mean Reward: 0.6703015049798996. Std of Reward: 0.49469248175234437.\n",
      "Saved Model\n",
      "Step: 4610000. Mean Reward: 0.6308333307287236. Std of Reward: 0.5424982566915652.\n",
      "Step: 4620000. Mean Reward: 0.6061742396590915. Std of Reward: 0.7241307550232929.\n",
      "Step: 4630000. Mean Reward: 0.6248998152404376. Std of Reward: 0.5814542243866272.\n",
      "Step: 4640000. Mean Reward: 0.6058039192235306. Std of Reward: 0.8517116792444583.\n",
      "Step: 4650000. Mean Reward: 0.5617682900609766. Std of Reward: 0.8839591217195956.\n",
      "Saved Model\n",
      "Step: 4660000. Mean Reward: 0.6798384326530614. Std of Reward: 0.49481072942765914.\n",
      "Step: 4670000. Mean Reward: 0.5992241353103455. Std of Reward: 0.7593413292266876.\n",
      "Step: 4680000. Mean Reward: 0.6325284063238642. Std of Reward: 0.6793198833397983.\n",
      "Step: 4690000. Mean Reward: 0.5874611375544045. Std of Reward: 0.6995923242777022.\n",
      "Step: 4700000. Mean Reward: 0.5297468328987357. Std of Reward: 0.995935907503764.\n",
      "Saved Model\n",
      "Step: 4710000. Mean Reward: 0.6474625441741578. Std of Reward: 0.6210266004777115.\n",
      "Step: 4720000. Mean Reward: 0.6053333307222226. Std of Reward: 0.61078830830653.\n",
      "Step: 4730000. Mean Reward: 0.6214124267062154. Std of Reward: 0.7071847988332413.\n",
      "Step: 4740000. Mean Reward: 0.6777339876403942. Std of Reward: 0.41147316235656856.\n",
      "Step: 4750000. Mean Reward: 0.663956137557895. Std of Reward: 0.5132701666921164.\n",
      "Saved Model\n",
      "Step: 4760000. Mean Reward: 0.6323056968860106. Std of Reward: 0.5373267431680605.\n",
      "Step: 4770000. Mean Reward: 0.5644341537160502. Std of Reward: 0.8340207687301409.\n",
      "Step: 4780000. Mean Reward: 0.5769980479824572. Std of Reward: 0.9201893079214162.\n",
      "Step: 4790000. Mean Reward: 0.6593316384873099. Std of Reward: 0.5047268294788232.\n",
      "Step: 4800000. Mean Reward: 0.6141325508596505. Std of Reward: 1.0533418203343516.\n",
      "Saved Model\n",
      "Step: 4810000. Mean Reward: 0.7289966804129352. Std of Reward: 0.15067262842830997.\n",
      "Step: 4820000. Mean Reward: 0.6925041025123153. Std of Reward: 0.34922372144674163.\n",
      "Step: 4830000. Mean Reward: 0.7045730681724136. Std of Reward: 0.2594492470845176.\n",
      "Step: 4840000. Mean Reward: 0.5894857114171436. Std of Reward: 0.7865622942394889.\n",
      "Step: 4850000. Mean Reward: 0.6394935517790059. Std of Reward: 0.6332859129865772.\n",
      "Saved Model\n",
      "Step: 4860000. Mean Reward: 0.6040952354457145. Std of Reward: 0.6817378519153754.\n",
      "Step: 4870000. Mean Reward: 0.5776722506201126. Std of Reward: 0.9308286657363822.\n",
      "Step: 4880000. Mean Reward: 0.6311764676631019. Std of Reward: 0.5549569311484085.\n",
      "Step: 4890000. Mean Reward: 0.5978994056390541. Std of Reward: 0.7241768833658776.\n",
      "Step: 4900000. Mean Reward: 0.7047594472164949. Std of Reward: 0.3211896635462811.\n",
      "Saved Model\n",
      "Step: 4910000. Mean Reward: 0.6623508744789475. Std of Reward: 0.47745760001524445.\n",
      "Step: 4920000. Mean Reward: 0.6643836779218751. Std of Reward: 0.46950767046146574.\n",
      "Step: 4930000. Mean Reward: 0.6655169312834227. Std of Reward: 0.5259688452472003.\n",
      "Step: 4940000. Mean Reward: 0.6658418341479593. Std of Reward: 0.4801220286860368.\n",
      "Step: 4950000. Mean Reward: 0.6868866301827413. Std of Reward: 0.38307021344925457.\n",
      "Saved Model\n",
      "Step: 4960000. Mean Reward: 0.6088352245965912. Std of Reward: 0.6466011443299.\n",
      "Step: 4970000. Mean Reward: 0.6257297270270274. Std of Reward: 0.6530152920848071.\n",
      "Step: 4980000. Mean Reward: 0.6512992804301079. Std of Reward: 0.5621925144431975.\n",
      "Step: 4990000. Mean Reward: 0.6673920526113991. Std of Reward: 0.4306877093864917.\n",
      "Step: 5000000. Mean Reward: 0.6784700826974361. Std of Reward: 0.4970270113522691.\n",
      "Saved Model\n",
      "Step: 5010000. Mean Reward: 0.5369062475562512. Std of Reward: 1.0002311855554238.\n",
      "Step: 5020000. Mean Reward: 0.6088611084611119. Std of Reward: 0.7829593563200906.\n",
      "Step: 5030000. Mean Reward: 0.6363176237225132. Std of Reward: 0.5385239830252649.\n",
      "Step: 5040000. Mean Reward: 0.6147053846565657. Std of Reward: 0.5236102711961946.\n",
      "Step: 5050000. Mean Reward: 0.5726023362690065. Std of Reward: 0.7995130306669035.\n",
      "Saved Model\n",
      "Step: 5060000. Mean Reward: 0.6287179459340664. Std of Reward: 0.6337106824009773.\n",
      "Step: 5070000. Mean Reward: 0.650652554761905. Std of Reward: 0.5473378602326658.\n",
      "Step: 5080000. Mean Reward: 0.6780406475414635. Std of Reward: 0.41238176722376185.\n",
      "Step: 5090000. Mean Reward: 0.670802789518325. Std of Reward: 0.5266327510889658.\n",
      "Step: 5100000. Mean Reward: 0.6629541419947091. Std of Reward: 0.5366532363036806.\n",
      "Saved Model\n",
      "Step: 5110000. Mean Reward: 0.7208866966305421. Std of Reward: 0.35078761431867184.\n",
      "Step: 5120000. Mean Reward: 0.6452536206956525. Std of Reward: 0.5631269950489253.\n",
      "Step: 5130000. Mean Reward: 0.6856060578342249. Std of Reward: 0.5503942201199779.\n",
      "Step: 5140000. Mean Reward: 0.6436936908054061. Std of Reward: 0.6987718670112365.\n",
      "Step: 5150000. Mean Reward: 0.6440221059489799. Std of Reward: 0.576069146775863.\n",
      "Saved Model\n",
      "Step: 5160000. Mean Reward: 0.6654037774845364. Std of Reward: 0.5441666283520357.\n",
      "Step: 5170000. Mean Reward: 0.6738888861927084. Std of Reward: 0.40796175428932707.\n",
      "Step: 5180000. Mean Reward: 0.677653058596939. Std of Reward: 0.43625674219751476.\n",
      "Step: 5190000. Mean Reward: 0.6851530583163268. Std of Reward: 0.452695008896697.\n",
      "Step: 5200000. Mean Reward: 0.6739843721614586. Std of Reward: 0.48532425949570696.\n",
      "Saved Model\n",
      "Step: 5210000. Mean Reward: 0.6382051254769234. Std of Reward: 0.56705265664535.\n",
      "Step: 5220000. Mean Reward: 0.6253130729723761. Std of Reward: 0.6534144815396001.\n",
      "Step: 5230000. Mean Reward: 0.5809860356439794. Std of Reward: 0.6755025288167452.\n",
      "Step: 5240000. Mean Reward: 0.5766074922899418. Std of Reward: 0.8591696462285283.\n",
      "Step: 5250000. Mean Reward: 0.605909907221622. Std of Reward: 0.6298466356062427.\n",
      "Saved Model\n",
      "Step: 5260000. Mean Reward: 0.6567875621709847. Std of Reward: 0.5133988665460787.\n",
      "Step: 5270000. Mean Reward: 0.5845252498242434. Std of Reward: 0.8749720108955084.\n",
      "Step: 5280000. Mean Reward: 0.6586904735989014. Std of Reward: 0.5210532300942383.\n",
      "Step: 5290000. Mean Reward: 0.6604013934607333. Std of Reward: 0.5685392130584475.\n",
      "Step: 5300000. Mean Reward: 0.6540449411292142. Std of Reward: 0.6882197574104811.\n",
      "Saved Model\n",
      "Step: 5310000. Mean Reward: 0.6001149400057479. Std of Reward: 0.7831478049091841.\n",
      "Step: 5320000. Mean Reward: 0.4686069624552255. Std of Reward: 1.054951449437798.\n",
      "Step: 5330000. Mean Reward: 0.5843575391005597. Std of Reward: 0.827575343461107.\n",
      "Step: 5340000. Mean Reward: 0.5827875216315798. Std of Reward: 0.8017771200125065.\n",
      "Step: 5350000. Mean Reward: 0.6004597673390815. Std of Reward: 0.7621193931107043.\n",
      "Saved Model\n",
      "Step: 5360000. Mean Reward: 0.6770267978944727. Std of Reward: 0.4817042912018767.\n",
      "Step: 5370000. Mean Reward: 0.6572852206804125. Std of Reward: 0.4955283325469619.\n",
      "Step: 5380000. Mean Reward: 0.6373717921758245. Std of Reward: 0.6256074995243781.\n",
      "Step: 5390000. Mean Reward: 0.6806547592500002. Std of Reward: 0.5145696243938911.\n",
      "Step: 5400000. Mean Reward: 0.5713047591200004. Std of Reward: 0.7299796443122052.\n",
      "Saved Model\n",
      "Step: 5410000. Mean Reward: 0.5628981455277783. Std of Reward: 0.7443924182304746.\n",
      "Step: 5420000. Mean Reward: 0.5278725465000008. Std of Reward: 0.8531096326524105.\n",
      "Step: 5430000. Mean Reward: 0.6824712617586208. Std of Reward: 0.43901209372893324.\n",
      "Step: 5440000. Mean Reward: 0.5178815232710852. Std of Reward: 0.9284938066455825.\n",
      "Step: 5450000. Mean Reward: 0.628604865926967. Std of Reward: 0.7630247139438235.\n",
      "Saved Model\n",
      "Step: 5460000. Mean Reward: 0.5176147679101804. Std of Reward: 0.8257125082113954.\n",
      "Step: 5470000. Mean Reward: 0.6325505023939395. Std of Reward: 0.5322939062769021.\n",
      "Step: 5480000. Mean Reward: 0.6041233113526018. Std of Reward: 0.6740570269967825.\n",
      "Step: 5490000. Mean Reward: 0.584108524226745. Std of Reward: 0.8102116029678026.\n",
      "Step: 5500000. Mean Reward: 0.5923809494890115. Std of Reward: 0.7029708513288309.\n",
      "Saved Model\n",
      "Step: 5510000. Mean Reward: 0.4595116744394917. Std of Reward: 0.9935676035208547.\n",
      "Step: 5520000. Mean Reward: 0.4422351393255821. Std of Reward: 1.363453594546877.\n",
      "Step: 5530000. Mean Reward: 0.6162409391793484. Std of Reward: 0.69195455162867.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5540000. Mean Reward: 0.6708828800108112. Std of Reward: 0.524802952763389.\n",
      "Step: 5550000. Mean Reward: 0.5605784286352951. Std of Reward: 0.8614097211011416.\n",
      "Saved Model\n",
      "Step: 5560000. Mean Reward: 0.6360614497262576. Std of Reward: 0.6072417487842706.\n",
      "Step: 5570000. Mean Reward: 0.6026086927453425. Std of Reward: 0.7108199020760978.\n",
      "Step: 5580000. Mean Reward: 0.49561507652976333. Std of Reward: 1.0259255166208852.\n",
      "Step: 5590000. Mean Reward: 0.5735628716586835. Std of Reward: 0.8232136026306458.\n",
      "Step: 5600000. Mean Reward: 0.6249035063315791. Std of Reward: 0.5994814076000623.\n",
      "Saved Model\n",
      "Step: 5610000. Mean Reward: 0.5700198385476198. Std of Reward: 0.7733453056301619.\n",
      "Step: 5620000. Mean Reward: 0.6050086328652854. Std of Reward: 0.6819708916473548.\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "unpack requires a bytes object of length 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-fea09393b070>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_progress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;31m# Decide and take an action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mnew_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_experiences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_horizon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UNITY\\PPOL\\python\\ppo\\trainer.py\u001b[0m in \u001b[0;36mtake_action\u001b[1;34m(self, info, env, brain_name, steps, normalize)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entropy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearn_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[0mnew_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_experiences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UNITY\\PPOL\\python\\unityagents\\environment.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action, memory, value)\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb\"STEP\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 436\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    437\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_loaded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mUnityEnvironmentException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No Unity environment is loaded.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UNITY\\PPOL\\python\\unityagents\\environment.py\u001b[0m in \u001b[0;36m_get_state\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_brains\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m             \u001b[0mstate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"brain_name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[0mn_agent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"agents\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UNITY\\PPOL\\python\\unityagents\\environment.py\u001b[0m in \u001b[0;36m_get_state_dict\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \"\"\"\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb\"RECEIVED\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[0mstate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UNITY\\PPOL\\python\\unityagents\\environment.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m             \u001b[0mmessage_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"I\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mmessage_length\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: unpack requires a bytes object of length 4"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if curriculum_file == \"None\":\n",
    "    curriculum_file = None\n",
    "\n",
    "\n",
    "def get_progress():\n",
    "    if curriculum_file is not None:\n",
    "        if env._curriculum.measure_type == \"progress\":\n",
    "            return steps / max_steps\n",
    "        elif env._curriculum.measure_type == \"reward\":\n",
    "            return last_reward\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "ppo_model = create_agent_model(env, lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps, \n",
    "                               normalize=normalize, num_layers=num_layers)\n",
    "\n",
    "is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "\n",
    "model_path = './models/{}'.format(run_path)\n",
    "summary_path = './summaries/{}'.format(run_path)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps, last_reward = sess.run([ppo_model.global_step, ppo_model.last_reward])    \n",
    "    summary_writer = tf.summary.FileWriter(summary_path)\n",
    "    info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "    trainer = Trainer(ppo_model, sess, info, is_continuous, use_observations, use_states, train_model)\n",
    "    if train_model:\n",
    "        trainer.write_text(summary_writer, 'Hyperparameters', hyperparameter_dict, steps)\n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "        # Decide and take an action\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps, normalize)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        if len(trainer.training_buffer['actions']) > buffer_size and train_model:\n",
    "            # Perform gradient descent with experience buffer\n",
    "            trainer.update_model(batch_size, num_epoch)\n",
    "        if steps % summary_freq == 0 and steps != 0 and train_model:\n",
    "            # Write training statistics to tensorboard.\n",
    "            trainer.write_summary(summary_writer, steps, env._curriculum.lesson_number)\n",
    "        if steps % save_freq == 0 and steps != 0 and train_model:\n",
    "            # Save Tensorflow model\n",
    "            save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)\n",
    "        if len(trainer.stats['cumulative_reward']) > 0:\n",
    "            mean_reward = np.mean(trainer.stats['cumulative_reward'])\n",
    "            sess.run(ppo_model.update_reward, feed_dict={ppo_model.new_reward: mean_reward})\n",
    "            last_reward = sess.run(ppo_model.last_reward)\n",
    "    # Final save Tensorflow model\n",
    "    if steps != 0 and train_model:\n",
    "        save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "env.close()\n",
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/ppo\\model-5600000.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/ppo\\model-5600000.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 4 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 4 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 4 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
