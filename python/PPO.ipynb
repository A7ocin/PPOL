{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from ppo.history import *\n",
    "from ppo.models import *\n",
    "from ppo.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### General parameters\n",
    "max_steps = 5e10 ##5e5 # Set maximum number of steps to run environment.\n",
    "run_path = \"ppo\" # The sub-directory name for model and summary statistics\n",
    "load_model = False # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model.\n",
    "summary_freq = 10000 # Frequency at which to save training statistics.\n",
    "save_freq = 50000 # Frequency at which to save model.\n",
    "env_name = \"ObstacleCurriculum\" # Name of the training environment file.\n",
    "curriculum_file = None\n",
    "\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.99 #0.99 # Reward discount rate.\n",
    "lambd = 0.95 ##0.95 # Lambda parameter for GAE.\n",
    "time_horizon = 2048 ##2048 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 1e-3 ##1e-3 # Strength of entropy regularization\n",
    "num_epoch = 5 ##5 # Number of gradient descent steps per batch of experiences.\n",
    "num_layers = 2 ##2 # Number of hidden layers between state/observation encoding and value/policy layers.\n",
    "epsilon = 0.2 ##0.2 # Acceptable threshold around ratio of old and new policy probabilities.\n",
    "buffer_size = 4096 ##2048 # How large the experience buffer should be before gradient descent.\n",
    "learning_rate = 3e-4 # Model learning rate.\n",
    "hidden_units = 128 ##64 # Number of units in hidden layer.\n",
    "batch_size = 32 ##64 # How many experiences per gradient descent update step.\n",
    "normalize = False\n",
    "\n",
    "### Logging dictionary for hyperparameters\n",
    "hyperparameter_dict = {'max_steps':max_steps, 'run_path':run_path, 'env_name':env_name,\n",
    "    'curriculum_file':curriculum_file, 'gamma':gamma, 'lambd':lambd, 'time_horizon':time_horizon,\n",
    "    'beta':beta, 'num_epoch':num_epoch, 'epsilon':epsilon, 'buffe_size':buffer_size,\n",
    "    'leaning_rate':learning_rate, 'hidden_units':hidden_units, 'batch_size':batch_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'ObstacleCurriculumAcademy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: ObstacleCurriculumAcademy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: ObstacleCurriculumBrain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 9\n",
      "        Action space type: discrete\n",
      "        Action space size (per agent): 8\n",
      "        Memory space size (per agent): 0\n",
      "        Action descriptions: , , , , , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name, curriculum=curriculum_file)\n",
    "print(str(env))\n",
    "brain_name = env.external_brain_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10000. Mean Reward: -2.957192985105239. Std of Reward: 2.7339894146717194.\n",
      "Step: 20000. Mean Reward: -3.9787500028749605. Std of Reward: 2.7575518049844994.\n",
      "Step: 30000. Mean Reward: -6.531562504687429. Std of Reward: 3.4786931550326825.\n",
      "Step: 40000. Mean Reward: -3.93483333769996. Std of Reward: 2.567898239007162.\n",
      "Step: 50000. Mean Reward: -8.593939400363682. Std of Reward: 7.53625303170814.\n",
      "Saved Model\n",
      "Step: 60000. Mean Reward: -6.249313729176419. Std of Reward: 5.031749362987308.\n",
      "Step: 70000. Mean Reward: -4.995104170687437. Std of Reward: 3.0578013547018372.\n",
      "Step: 80000. Mean Reward: -4.8710000035999474. Std of Reward: 3.744505768114265.\n",
      "Step: 90000. Mean Reward: -4.5079166724999435. Std of Reward: 3.521069278291441.\n",
      "Step: 100000. Mean Reward: -8.67944444677764. Std of Reward: 5.394263469609611.\n",
      "Saved Model\n",
      "Step: 110000. Mean Reward: -5.67862745458817. Std of Reward: 4.433387153357006.\n",
      "Step: 120000. Mean Reward: -3.3440384656538122. Std of Reward: 2.4129653435596197.\n",
      "Step: 130000. Mean Reward: -4.790490199882291. Std of Reward: 3.254752874170663.\n",
      "Step: 140000. Mean Reward: -4.028583337049952. Std of Reward: 2.952446221197858.\n",
      "Step: 150000. Mean Reward: -4.594537040333271. Std of Reward: 3.7017394891663593.\n",
      "Saved Model\n",
      "Step: 160000. Mean Reward: -2.1116145866562226. Std of Reward: 3.1236310462378407.\n",
      "Step: 170000. Mean Reward: -1.1994791699999892. Std of Reward: 1.7667942544911692.\n",
      "Step: 180000. Mean Reward: -1.2778231326326428. Std of Reward: 1.8848630587875523.\n",
      "Step: 190000. Mean Reward: -1.2954717022641422. Std of Reward: 1.9091144168070584.\n",
      "Step: 200000. Mean Reward: -1.5706201582092871. Std of Reward: 2.4683862116463935.\n",
      "Saved Model\n",
      "Step: 210000. Mean Reward: -0.8332812530312442. Std of Reward: 1.784969701108042.\n",
      "Step: 220000. Mean Reward: -1.2587239613906147. Std of Reward: 2.587379286546695.\n",
      "Step: 230000. Mean Reward: -0.8722535242394311. Std of Reward: 1.8042617800217586.\n",
      "Step: 240000. Mean Reward: -0.8227333364666618. Std of Reward: 1.7304920007474431.\n",
      "Step: 250000. Mean Reward: -0.8694675948888825. Std of Reward: 1.9805278513920186.\n",
      "Saved Model\n",
      "Step: 260000. Mean Reward: -0.39581196839316163. Std of Reward: 1.054179458327787.\n",
      "Step: 270000. Mean Reward: -0.2934413606666663. Std of Reward: 1.1392240304653471.\n",
      "Step: 280000. Mean Reward: -0.07549857823076922. Std of Reward: 0.9352522308702863.\n",
      "Step: 290000. Mean Reward: -0.1345565774128431. Std of Reward: 1.093940860603976.\n",
      "Step: 300000. Mean Reward: -0.2680149834044918. Std of Reward: 1.4340324675681737.\n",
      "Saved Model\n",
      "Step: 310000. Mean Reward: -0.5705490216705822. Std of Reward: 2.0241785611035863.\n",
      "Step: 320000. Mean Reward: -0.5211224509183624. Std of Reward: 1.8840514667392363.\n",
      "Step: 330000. Mean Reward: -0.48184524020237507. Std of Reward: 2.0392968103023623.\n",
      "Step: 340000. Mean Reward: -0.8217333350933329. Std of Reward: 3.4859984212678263.\n",
      "Step: 350000. Mean Reward: -0.1659139807338689. Std of Reward: 1.3602884427707196.\n",
      "Saved Model\n",
      "Step: 360000. Mean Reward: -0.2768382371102924. Std of Reward: 1.3105782734539204.\n",
      "Step: 370000. Mean Reward: 0.2261921273611109. Std of Reward: 0.695477887895354.\n",
      "Step: 380000. Mean Reward: -0.17280142021985737. Std of Reward: 1.135699836026269.\n",
      "Step: 390000. Mean Reward: -0.010434784695651539. Std of Reward: 1.0735121764783806.\n",
      "Step: 400000. Mean Reward: 0.1614252854689655. Std of Reward: 0.8231616453960446.\n",
      "Saved Model\n",
      "Step: 410000. Mean Reward: 0.06854650963953485. Std of Reward: 0.856844621744007.\n",
      "Step: 420000. Mean Reward: 0.13240475980714347. Std of Reward: 1.081406276317178.\n",
      "Step: 430000. Mean Reward: 0.18340277539880942. Std of Reward: 0.8212587799375858.\n",
      "Step: 440000. Mean Reward: 0.15664596081366455. Std of Reward: 0.8894665389871009.\n",
      "Step: 450000. Mean Reward: 0.1816232616874999. Std of Reward: 0.7970910003468681.\n",
      "Saved Model\n",
      "Step: 460000. Mean Reward: 0.22615384426627247. Std of Reward: 0.9621333541584212.\n",
      "Step: 470000. Mean Reward: 0.1611451923312883. Std of Reward: 0.9240614263885015.\n",
      "Step: 480000. Mean Reward: 0.2314220997065218. Std of Reward: 0.8739668418844261.\n",
      "Step: 490000. Mean Reward: 0.17652694425149734. Std of Reward: 0.9792555831473592.\n",
      "Step: 500000. Mean Reward: 0.30312770369480535. Std of Reward: 0.8623831940534993.\n",
      "Saved Model\n",
      "Step: 510000. Mean Reward: 0.1848190458857143. Std of Reward: 0.9293593709103785.\n",
      "Step: 520000. Mean Reward: -0.0983100250979011. Std of Reward: 1.264615578946611.\n",
      "Step: 530000. Mean Reward: 0.15272548833529426. Std of Reward: 0.9814161384642354.\n",
      "Step: 540000. Mean Reward: 0.21271638865745846. Std of Reward: 0.8069761268783832.\n",
      "Step: 550000. Mean Reward: 0.34466480264245797. Std of Reward: 0.6801765974825892.\n",
      "Saved Model\n",
      "Step: 560000. Mean Reward: 0.24938271420370414. Std of Reward: 0.9519713655755861.\n",
      "Step: 570000. Mean Reward: 0.17397085457923522. Std of Reward: 0.9171205135435583.\n",
      "Step: 580000. Mean Reward: 0.3531481464938272. Std of Reward: 0.7730680150595172.\n",
      "Step: 590000. Mean Reward: 0.3498749986062505. Std of Reward: 0.8870131987111444.\n",
      "Step: 600000. Mean Reward: 0.19374999842105392. Std of Reward: 1.167459539967627.\n",
      "Saved Model\n",
      "Step: 610000. Mean Reward: 0.3814102550128208. Std of Reward: 0.7579535103788231.\n",
      "Step: 620000. Mean Reward: 0.15558113894079043. Std of Reward: 1.072012223184771.\n",
      "Step: 630000. Mean Reward: 0.1971614568281274. Std of Reward: 1.3816837609861143.\n",
      "Step: 640000. Mean Reward: 0.2614679896357626. Std of Reward: 1.0894915461285977.\n",
      "Step: 650000. Mean Reward: 0.0735365839430924. Std of Reward: 1.4999757384599193.\n",
      "Saved Model\n",
      "Step: 660000. Mean Reward: 0.3193267093708614. Std of Reward: 0.9064222087097797.\n",
      "Step: 670000. Mean Reward: -0.14356521874783013. Std of Reward: 2.5759261445354857.\n",
      "Step: 680000. Mean Reward: 0.4768200396564415. Std of Reward: 0.49723981872214706.\n",
      "Step: 690000. Mean Reward: 0.342260535051724. Std of Reward: 0.6439911360919617.\n",
      "Step: 700000. Mean Reward: 0.10428571303809653. Std of Reward: 2.5405518173362416.\n",
      "Saved Model\n",
      "Step: 710000. Mean Reward: 0.45669607692352926. Std of Reward: 0.5218529538975692.\n",
      "Step: 720000. Mean Reward: 0.4359627317950309. Std of Reward: 0.5168041504718899.\n",
      "Step: 730000. Mean Reward: 0.3078051631549309. Std of Reward: 1.223923841041314.\n",
      "Step: 740000. Mean Reward: 0.47498989774545447. Std of Reward: 0.4917785865734704.\n",
      "Step: 750000. Mean Reward: 0.3591422989590642. Std of Reward: 0.6057231206469234.\n",
      "Saved Model\n",
      "Step: 760000. Mean Reward: 0.47209551563742674. Std of Reward: 0.4862164577277068.\n",
      "Step: 770000. Mean Reward: 0.2954797965606088. Std of Reward: 1.5764373688565954.\n",
      "Step: 780000. Mean Reward: 0.5194642844107141. Std of Reward: 0.4588326040067059.\n",
      "Step: 790000. Mean Reward: 0.38218820731292663. Std of Reward: 1.296782718634821.\n",
      "Step: 800000. Mean Reward: 0.5234259249012344. Std of Reward: 0.43171017015951874.\n",
      "Saved Model\n",
      "Step: 810000. Mean Reward: 0.5495445122546582. Std of Reward: 0.37184021818507873.\n",
      "Step: 820000. Mean Reward: 0.5621987938674696. Std of Reward: 0.3622762446956781.\n",
      "Step: 830000. Mean Reward: 0.5908080793212119. Std of Reward: 0.2547740531583488.\n",
      "Step: 840000. Mean Reward: 0.5144871782485204. Std of Reward: 0.42329477665518384.\n",
      "Step: 850000. Mean Reward: 0.5229629616257309. Std of Reward: 0.4244787259812812.\n",
      "Saved Model\n",
      "Step: 860000. Mean Reward: 0.5352422465290696. Std of Reward: 0.4067976849646054.\n",
      "Step: 870000. Mean Reward: 0.6129238080628571. Std of Reward: 0.2767027062661104.\n",
      "Step: 880000. Mean Reward: 0.5716971530243901. Std of Reward: 0.31752058950640344.\n",
      "Step: 890000. Mean Reward: 0.5083962251132079. Std of Reward: 0.6980269406924012.\n",
      "Step: 900000. Mean Reward: 0.5517916652312504. Std of Reward: 0.7083885407369791.\n",
      "Saved Model\n",
      "Step: 910000. Mean Reward: 0.608139533610465. Std of Reward: 0.2809671392361797.\n",
      "Step: 920000. Mean Reward: 0.6165345515609754. Std of Reward: 0.18118343824299404.\n",
      "Step: 930000. Mean Reward: 0.5385378309447856. Std of Reward: 0.5949976592774726.\n",
      "Step: 940000. Mean Reward: 0.6203238078971427. Std of Reward: 0.2685145315253665.\n",
      "Step: 950000. Mean Reward: 0.5742603531834324. Std of Reward: 0.6699879018180497.\n",
      "Saved Model\n",
      "Step: 960000. Mean Reward: 0.6157333316799998. Std of Reward: 0.3117063918760824.\n",
      "Step: 970000. Mean Reward: 0.5907285409580842. Std of Reward: 0.7029481577274295.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 980000. Mean Reward: 0.6078288267459458. Std of Reward: 0.3716438008268471.\n",
      "Step: 990000. Mean Reward: 0.6170599233764044. Std of Reward: 0.32659858669557157.\n",
      "Step: 1000000. Mean Reward: 0.6224063653314605. Std of Reward: 0.3093349943137739.\n",
      "Saved Model\n",
      "Step: 1010000. Mean Reward: 0.6091569750290703. Std of Reward: 0.5635198618306414.\n",
      "Step: 1020000. Mean Reward: 0.5071478855352127. Std of Reward: 1.0916589895481799.\n",
      "Step: 1030000. Mean Reward: 0.6263394664969327. Std of Reward: 0.38153252886593536.\n",
      "Step: 1040000. Mean Reward: 0.6023108364846634. Std of Reward: 0.8329111673780921.\n",
      "Step: 1050000. Mean Reward: 0.6702162140594594. Std of Reward: 0.2604313492314153.\n",
      "Saved Model\n",
      "Step: 1060000. Mean Reward: 0.6421228051894735. Std of Reward: 0.339257612262162.\n",
      "Step: 1070000. Mean Reward: 0.6014065487283243. Std of Reward: 0.8111065370373916.\n",
      "Step: 1080000. Mean Reward: 0.590612242520408. Std of Reward: 0.45737180450766457.\n",
      "Step: 1090000. Mean Reward: 0.6209867308059701. Std of Reward: 0.4292234387489338.\n",
      "Step: 1100000. Mean Reward: 0.6597938120721648. Std of Reward: 0.3300955513206292.\n",
      "Saved Model\n",
      "Step: 1110000. Mean Reward: 0.6105389203473062. Std of Reward: 0.8523872249727453.\n",
      "Step: 1120000. Mean Reward: 0.4411580064285739. Std of Reward: 1.462826354116001.\n",
      "Step: 1130000. Mean Reward: 0.48299362819745406. Std of Reward: 1.3625919752111986.\n",
      "Step: 1140000. Mean Reward: 0.5332679715163422. Std of Reward: 1.3705308284468212.\n",
      "Step: 1150000. Mean Reward: 0.3828030278090952. Std of Reward: 2.0270929937277105.\n",
      "Saved Model\n",
      "Step: 1160000. Mean Reward: 0.5706429753553313. Std of Reward: 1.2010380605896533.\n",
      "Step: 1170000. Mean Reward: 0.5725322256906086. Std of Reward: 0.9409942526999148.\n",
      "Step: 1180000. Mean Reward: 0.47197488302055013. Std of Reward: 1.3790577042929177.\n",
      "Step: 1190000. Mean Reward: 0.5371057320322591. Std of Reward: 1.0657240323897272.\n",
      "Step: 1200000. Mean Reward: 0.45308942802439195. Std of Reward: 1.2529795143187128.\n",
      "Saved Model\n",
      "Step: 1210000. Mean Reward: 0.39954545197727637. Std of Reward: 1.7128997611879728.\n",
      "Step: 1220000. Mean Reward: 0.5471868337070082. Std of Reward: 1.2097192626931539.\n",
      "Step: 1230000. Mean Reward: 0.6276460453350515. Std of Reward: 0.39876313993630685.\n",
      "Step: 1240000. Mean Reward: 0.6596791415454546. Std of Reward: 0.3151086793755485.\n",
      "Step: 1250000. Mean Reward: 0.6094362717696078. Std of Reward: 0.4574635484904277.\n",
      "Saved Model\n",
      "Step: 1260000. Mean Reward: 0.6791751672704082. Std of Reward: 0.2737812352209597.\n",
      "Step: 1270000. Mean Reward: 0.6514479140124999. Std of Reward: 0.3635424721055718.\n",
      "Step: 1280000. Mean Reward: 0.5284402823475919. Std of Reward: 1.6976750868881099.\n",
      "Step: 1290000. Mean Reward: 0.6472705287487922. Std of Reward: 0.39970191209060313.\n",
      "Step: 1300000. Mean Reward: 0.4703793076758514. Std of Reward: 2.4637481469314673.\n",
      "Saved Model\n",
      "Step: 1310000. Mean Reward: 0.6407881746108374. Std of Reward: 0.4024823185180991.\n",
      "Step: 1320000. Mean Reward: 0.707478846467005. Std of Reward: 0.20109034477679202.\n",
      "Step: 1330000. Mean Reward: 0.6429233484652408. Std of Reward: 0.42939743233050637.\n",
      "Step: 1340000. Mean Reward: 0.6290452234874373. Std of Reward: 0.4596108536490162.\n",
      "Step: 1350000. Mean Reward: 0.4502877671294998. Std of Reward: 2.1343745203388753.\n",
      "Saved Model\n",
      "Step: 1360000. Mean Reward: 0.711779385516908. Std of Reward: 0.22864566159331143.\n",
      "Step: 1370000. Mean Reward: 0.6446907190103092. Std of Reward: 0.3801530880737378.\n",
      "Step: 1380000. Mean Reward: 0.5142987779329208. Std of Reward: 2.0449348403401086.\n",
      "Step: 1390000. Mean Reward: 0.5452314788944452. Std of Reward: 0.971450483288344.\n",
      "Step: 1400000. Mean Reward: 0.5033232905120458. Std of Reward: 1.7500834602796025.\n",
      "Saved Model\n",
      "Step: 1410000. Mean Reward: 0.6463333306048781. Std of Reward: 0.3895115555316215.\n",
      "Step: 1420000. Mean Reward: 0.6505664997438423. Std of Reward: 0.3688524107848565.\n",
      "Step: 1430000. Mean Reward: 0.7064393909595958. Std of Reward: 0.20194103388369772.\n",
      "Step: 1440000. Mean Reward: 0.6747512410995025. Std of Reward: 0.2969190774624066.\n",
      "Step: 1450000. Mean Reward: 0.6586923049487181. Std of Reward: 0.4025668875857988.\n",
      "Saved Model\n",
      "Step: 1460000. Mean Reward: 0.5655486517142818. Std of Reward: 1.8654176147880113.\n",
      "Step: 1470000. Mean Reward: 0.5362121186424231. Std of Reward: 1.6459949184326559.\n",
      "Step: 1480000. Mean Reward: 0.4318259775661594. Std of Reward: 2.8909400810683232.\n",
      "Step: 1490000. Mean Reward: 0.6765032653823528. Std of Reward: 0.32569767645368547.\n",
      "Step: 1500000. Mean Reward: 0.7060406474536585. Std of Reward: 0.23439803763407183.\n",
      "Saved Model\n",
      "Step: 1510000. Mean Reward: 0.6176388860208344. Std of Reward: 0.6905832654336657.\n",
      "Step: 1520000. Mean Reward: -3.6551111132333687. Std of Reward: 16.26991115090307.\n",
      "Step: 1530000. Mean Reward: 0.53756261768208. Std of Reward: 1.4535672407804843.\n",
      "Step: 1540000. Mean Reward: 0.6425612024124294. Std of Reward: 0.40972465605610586.\n",
      "Step: 1550000. Mean Reward: 0.4922592565499992. Std of Reward: 1.5653628894515608.\n",
      "Saved Model\n",
      "Step: 1560000. Mean Reward: 0.5213479025737717. Std of Reward: 1.0851648263300715.\n",
      "Step: 1570000. Mean Reward: 0.5488813787612612. Std of Reward: 0.5661623900242719.\n",
      "Step: 1580000. Mean Reward: 0.4310689625792971. Std of Reward: 2.6573615647173323.\n",
      "Step: 1590000. Mean Reward: 0.6738003190676329. Std of Reward: 0.34700002063655405.\n",
      "Step: 1600000. Mean Reward: 0.2253516791834908. Std of Reward: 4.518054505790266.\n",
      "Saved Model\n",
      "Step: 1610000. Mean Reward: 0.34163288023647176. Std of Reward: 2.8203569131782773.\n",
      "Step: 1620000. Mean Reward: 0.5343548359731192. Std of Reward: 0.8748030167596006.\n",
      "Step: 1630000. Mean Reward: 0.2044182362641434. Std of Reward: 3.9380216702536486.\n",
      "Step: 1640000. Mean Reward: 0.6349909393152177. Std of Reward: 0.5043308061894131.\n",
      "Step: 1650000. Mean Reward: 0.5123121360924787. Std of Reward: 2.0797076311385987.\n",
      "Saved Model\n",
      "Step: 1660000. Mean Reward: 0.20543543259460403. Std of Reward: 4.741733693826746.\n",
      "Step: 1670000. Mean Reward: 0.6499688446121495. Std of Reward: 0.41304862063449543.\n",
      "Step: 1680000. Mean Reward: 0.6731105963225805. Std of Reward: 0.3775971866836996.\n",
      "Step: 1690000. Mean Reward: 0.4542695445678948. Std of Reward: 2.1079815981703103.\n",
      "Step: 1700000. Mean Reward: 0.3856060581258623. Std of Reward: 2.6358054734161116.\n",
      "Saved Model\n",
      "Step: 1710000. Mean Reward: 0.41656176886013646. Std of Reward: 1.9529750002266024.\n",
      "Step: 1720000. Mean Reward: -0.16805221107231394. Std of Reward: 4.566019058136161.\n",
      "Step: 1730000. Mean Reward: 0.453422616589281. Std of Reward: 1.9492782312121457.\n",
      "Step: 1740000. Mean Reward: 0.06461058944859503. Std of Reward: 3.3863926251256955.\n",
      "Step: 1750000. Mean Reward: 0.4540686248117614. Std of Reward: 1.8304532095442938.\n",
      "Saved Model\n",
      "Step: 1760000. Mean Reward: 0.1470987628796212. Std of Reward: 3.1832405322179.\n",
      "Step: 1770000. Mean Reward: 0.39695363960925906. Std of Reward: 2.600020353704669.\n",
      "Step: 1780000. Mean Reward: 0.1480508449491573. Std of Reward: 4.391865400668948.\n",
      "Step: 1790000. Mean Reward: 0.6152342316594595. Std of Reward: 0.4801191408095285.\n",
      "Step: 1800000. Mean Reward: -1.130929489846198. Std of Reward: 7.612954871267786.\n",
      "Saved Model\n",
      "Step: 1810000. Mean Reward: 0.07551020151022471. Std of Reward: 5.531267314452833.\n",
      "Step: 1820000. Mean Reward: -0.8442156893334002. Std of Reward: 6.340134285023785.\n",
      "Step: 1830000. Mean Reward: 0.6679841240666665. Std of Reward: 0.3602863494336434.\n",
      "Step: 1840000. Mean Reward: 0.6249078312258063. Std of Reward: 0.4659142128853696.\n",
      "Step: 1850000. Mean Reward: 0.4385906832941196. Std of Reward: 2.270167026157593.\n",
      "Saved Model\n",
      "Step: 1860000. Mean Reward: 0.4326690791376811. Std of Reward: 2.269816372447267.\n",
      "Step: 1870000. Mean Reward: 0.647553514059633. Std of Reward: 0.43147280925980686.\n",
      "Step: 1880000. Mean Reward: 0.5480606031272703. Std of Reward: 1.7437866251363894.\n",
      "Step: 1890000. Mean Reward: 0.6202130870547945. Std of Reward: 0.4694446614541414.\n",
      "Step: 1900000. Mean Reward: 0.19296130660715316. Std of Reward: 4.774796091034663.\n",
      "Saved Model\n",
      "Step: 1910000. Mean Reward: -0.31598958629681273. Std of Reward: 8.00256164851509.\n",
      "Step: 1920000. Mean Reward: 0.3327116371428511. Std of Reward: 3.734287816653231.\n",
      "Step: 1930000. Mean Reward: 0.623843697215311. Std of Reward: 0.45404453295728564.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1940000. Mean Reward: 0.36632849947824697. Std of Reward: 2.8421080884528647.\n",
      "Step: 1950000. Mean Reward: 0.6301472838093023. Std of Reward: 0.44971959834179953.\n",
      "Saved Model\n",
      "Step: 1960000. Mean Reward: 0.576996924861751. Std of Reward: 0.5353023901403439.\n",
      "Step: 1970000. Mean Reward: 0.45178727782893885. Std of Reward: 2.28524138385734.\n",
      "Step: 1980000. Mean Reward: 0.5609860354031422. Std of Reward: 0.9592277875829726.\n",
      "Step: 1990000. Mean Reward: 0.4413647313840452. Std of Reward: 3.061297673536694.\n",
      "Step: 2000000. Mean Reward: 0.4959959321341428. Std of Reward: 1.7978646029237688.\n",
      "Saved Model\n",
      "Step: 2010000. Mean Reward: 0.2867787086890593. Std of Reward: 3.0087517360991645.\n",
      "Step: 2020000. Mean Reward: 0.5014320955481433. Std of Reward: 2.0575311642464684.\n",
      "Step: 2030000. Mean Reward: -0.831994050875015. Std of Reward: 7.961970076410234.\n",
      "Step: 2040000. Mean Reward: 0.22680685059811892. Std of Reward: 3.705026279742367.\n",
      "Step: 2050000. Mean Reward: -0.007067904185221856. Std of Reward: 4.694774072314068.\n",
      "Saved Model\n",
      "Step: 2060000. Mean Reward: -2.980866669399988. Std of Reward: 12.493612964055119.\n",
      "Step: 2070000. Mean Reward: 0.48446601633494235. Std of Reward: 2.4863256790825514.\n",
      "Step: 2080000. Mean Reward: -0.07921568935290588. Std of Reward: 6.464266365664204.\n",
      "Step: 2090000. Mean Reward: -0.013596494273710177. Std of Reward: 4.387834512778276.\n",
      "Step: 2100000. Mean Reward: 0.30335917051161476. Std of Reward: 2.84419507907298.\n",
      "Saved Model\n",
      "Step: 2110000. Mean Reward: 0.22215962156335406. Std of Reward: 4.166939459496474.\n",
      "Step: 2120000. Mean Reward: -0.08078347851282017. Std of Reward: 5.369949777787259.\n",
      "Step: 2130000. Mean Reward: 0.5935115833475949. Std of Reward: 1.2135612080333138.\n",
      "Step: 2140000. Mean Reward: 0.3774161044093917. Std of Reward: 2.1930249267332083.\n",
      "Step: 2150000. Mean Reward: 0.49846153531360665. Std of Reward: 1.7809068731309263.\n",
      "Saved Model\n",
      "Step: 2160000. Mean Reward: 0.5759020589226813. Std of Reward: 1.011687727491219.\n",
      "Step: 2170000. Mean Reward: 0.6396141943888888. Std of Reward: 0.430343083785016.\n",
      "Step: 2180000. Mean Reward: 0.6562926798341464. Std of Reward: 0.3990932754990204.\n",
      "Step: 2190000. Mean Reward: 0.5270020935723249. Std of Reward: 1.987733766732226.\n",
      "Step: 2200000. Mean Reward: 0.45242825311920043. Std of Reward: 2.157324712574111.\n",
      "Saved Model\n",
      "Step: 2210000. Mean Reward: -0.9029666698600319. Std of Reward: 8.133342310133107.\n",
      "Step: 2220000. Mean Reward: 0.6754407017836537. Std of Reward: 0.3388954304754662.\n",
      "Step: 2230000. Mean Reward: 0.586685820425287. Std of Reward: 1.5264817142245635.\n",
      "Step: 2240000. Mean Reward: -0.38529570201616786. Std of Reward: 6.069796216986039.\n",
      "Step: 2250000. Mean Reward: 0.65506666368. Std of Reward: 0.3758559059335822.\n",
      "Saved Model\n",
      "Step: 2260000. Mean Reward: -0.1819080488483755. Std of Reward: 8.624698266380904.\n",
      "Step: 2270000. Mean Reward: 0.6157130980000005. Std of Reward: 0.7216627428277125.\n",
      "Step: 2280000. Mean Reward: 0.5578488339302341. Std of Reward: 1.357354259369425.\n",
      "Step: 2290000. Mean Reward: 0.45053652647945536. Std of Reward: 1.7930257552712405.\n",
      "Step: 2300000. Mean Reward: 0.03644927244565034. Std of Reward: 4.728344241136074.\n",
      "Saved Model\n",
      "Step: 2310000. Mean Reward: 0.5309829026730717. Std of Reward: 2.001122191168272.\n",
      "Step: 2320000. Mean Reward: 0.3877260949689818. Std of Reward: 2.81320175022436.\n",
      "Step: 2330000. Mean Reward: -0.06570881532185267. Std of Reward: 4.679064508453471.\n",
      "Step: 2340000. Mean Reward: 0.3825595205071396. Std of Reward: 2.30646841792394.\n",
      "Step: 2350000. Mean Reward: 0.40912698121428936. Std of Reward: 1.7901997032830697.\n",
      "Saved Model\n",
      "Step: 2360000. Mean Reward: 0.33478464090449495. Std of Reward: 3.4410025295071733.\n",
      "Step: 2370000. Mean Reward: 0.6928873209248827. Std of Reward: 0.31235312126283316.\n",
      "Step: 2380000. Mean Reward: 0.46612268227778014. Std of Reward: 1.4122615991667995.\n",
      "Step: 2390000. Mean Reward: 0.1308814076057672. Std of Reward: 3.2571953296685052.\n",
      "Step: 2400000. Mean Reward: 0.2662681130942007. Std of Reward: 2.4013288350716118.\n",
      "Saved Model\n",
      "Step: 2410000. Mean Reward: 0.5246078403101617. Std of Reward: 1.1977025791585112.\n",
      "Step: 2420000. Mean Reward: 0.6519211792364532. Std of Reward: 0.4086461261299563.\n",
      "Step: 2430000. Mean Reward: 0.417058284907106. Std of Reward: 1.4712430715242377.\n",
      "Step: 2440000. Mean Reward: 0.04285713989793075. Std of Reward: 4.0194936844029145.\n",
      "Step: 2450000. Mean Reward: 0.19126404197189717. Std of Reward: 3.633160496047015.\n",
      "Saved Model\n",
      "Step: 2460000. Mean Reward: 0.5215016472326738. Std of Reward: 0.8049277721514086.\n",
      "Step: 2470000. Mean Reward: 0.3379818562040834. Std of Reward: 1.729225915978054.\n",
      "Step: 2480000. Mean Reward: 0.6023379601574074. Std of Reward: 0.49300072642873166.\n",
      "Step: 2490000. Mean Reward: 0.4221616132060563. Std of Reward: 1.9396523624482342.\n",
      "Step: 2500000. Mean Reward: 0.39190839401527067. Std of Reward: 1.6459487891960602.\n",
      "Saved Model\n",
      "Step: 2510000. Mean Reward: 0.023415838742557597. Std of Reward: 3.6816147994435924.\n",
      "Step: 2520000. Mean Reward: 0.29828124698436675. Std of Reward: 3.5634060853501563.\n",
      "Step: 2530000. Mean Reward: 0.3033900195578238. Std of Reward: 2.0498838839865643.\n",
      "Step: 2540000. Mean Reward: 0.596475532293578. Std of Reward: 0.5079979857219361.\n",
      "Step: 2550000. Mean Reward: 0.25956467378357645. Std of Reward: 2.693837186386696.\n",
      "Saved Model\n",
      "Step: 2560000. Mean Reward: 0.4118709649935455. Std of Reward: 1.7706519295865557.\n",
      "Step: 2570000. Mean Reward: 0.19776352984615064. Std of Reward: 3.603836802845274.\n",
      "Step: 2580000. Mean Reward: 0.56557970726087. Std of Reward: 0.6774993282970118.\n",
      "Step: 2590000. Mean Reward: 0.30923708614084766. Std of Reward: 1.8386356061750326.\n",
      "Step: 2600000. Mean Reward: 0.4488888858639455. Std of Reward: 1.6885071844839294.\n",
      "Saved Model\n",
      "Step: 2610000. Mean Reward: 0.42591666367499936. Std of Reward: 1.8402616509405547.\n",
      "Step: 2620000. Mean Reward: 0.6748214251020409. Std of Reward: 0.330170168937617.\n",
      "Step: 2630000. Mean Reward: 0.4378505711931041. Std of Reward: 1.8139837079606314.\n",
      "Step: 2640000. Mean Reward: 0.4925182451678829. Std of Reward: 1.448098175471215.\n",
      "Step: 2650000. Mean Reward: 0.3683119626474364. Std of Reward: 2.2614152419674936.\n",
      "Saved Model\n",
      "Step: 2660000. Mean Reward: 0.5346771347277495. Std of Reward: 0.987693190755204.\n",
      "Step: 2670000. Mean Reward: 0.5588351221935495. Std of Reward: 1.0983113269365854.\n",
      "Step: 2680000. Mean Reward: 0.3168518484273457. Std of Reward: 2.557834424154596.\n",
      "Step: 2690000. Mean Reward: 0.4604362382751663. Std of Reward: 2.04677427958889.\n",
      "Step: 2700000. Mean Reward: 0.4066428537785726. Std of Reward: 1.9319777574103458.\n",
      "Saved Model\n",
      "Step: 2710000. Mean Reward: 0.6842113788341463. Std of Reward: 0.29508707775246407.\n",
      "Step: 2720000. Mean Reward: 0.6467692273846154. Std of Reward: 0.3650909664636027.\n",
      "Step: 2730000. Mean Reward: 0.5144025125220135. Std of Reward: 1.4314533680609758.\n",
      "Step: 2740000. Mean Reward: 0.621648645497298. Std of Reward: 0.8194504855701729.\n",
      "Step: 2750000. Mean Reward: 0.6696286198043478. Std of Reward: 0.32621479238826934.\n",
      "Saved Model\n",
      "Step: 2760000. Mean Reward: 0.5671594946021518. Std of Reward: 0.9926423525796447.\n",
      "Step: 2770000. Mean Reward: 0.5751373593516491. Std of Reward: 0.8682621596120795.\n",
      "Step: 2780000. Mean Reward: 0.5999372725752694. Std of Reward: 0.8091550597857393.\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "unpack requires a bytes object of length 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-fea09393b070>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_progress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;31m# Decide and take an action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mnew_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_experiences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_horizon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UNITY\\PPOL\\python\\ppo\\trainer.py\u001b[0m in \u001b[0;36mtake_action\u001b[1;34m(self, info, env, brain_name, steps, normalize)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entropy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearn_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[0mnew_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_experiences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UNITY\\PPOL\\python\\unityagents\\environment.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action, memory, value)\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb\"STEP\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 436\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    437\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_loaded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mUnityEnvironmentException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No Unity environment is loaded.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UNITY\\PPOL\\python\\unityagents\\environment.py\u001b[0m in \u001b[0;36m_get_state\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_brains\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m             \u001b[0mstate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"brain_name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[0mn_agent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"agents\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UNITY\\PPOL\\python\\unityagents\\environment.py\u001b[0m in \u001b[0;36m_get_state_dict\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \"\"\"\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb\"RECEIVED\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[0mstate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UNITY\\PPOL\\python\\unityagents\\environment.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m             \u001b[0mmessage_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"I\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mmessage_length\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: unpack requires a bytes object of length 4"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if curriculum_file == \"None\":\n",
    "    curriculum_file = None\n",
    "\n",
    "\n",
    "def get_progress():\n",
    "    if curriculum_file is not None:\n",
    "        if env._curriculum.measure_type == \"progress\":\n",
    "            return steps / max_steps\n",
    "        elif env._curriculum.measure_type == \"reward\":\n",
    "            return last_reward\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "ppo_model = create_agent_model(env, lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps, \n",
    "                               normalize=normalize, num_layers=num_layers)\n",
    "\n",
    "is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "\n",
    "model_path = './models/{}'.format(run_path)\n",
    "summary_path = './summaries/{}'.format(run_path)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps, last_reward = sess.run([ppo_model.global_step, ppo_model.last_reward])    \n",
    "    summary_writer = tf.summary.FileWriter(summary_path)\n",
    "    info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "    trainer = Trainer(ppo_model, sess, info, is_continuous, use_observations, use_states, train_model)\n",
    "    if train_model:\n",
    "        trainer.write_text(summary_writer, 'Hyperparameters', hyperparameter_dict, steps)\n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "        # Decide and take an action\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps, normalize)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        if len(trainer.training_buffer['actions']) > buffer_size and train_model:\n",
    "            # Perform gradient descent with experience buffer\n",
    "            trainer.update_model(batch_size, num_epoch)\n",
    "        if steps % summary_freq == 0 and steps != 0 and train_model:\n",
    "            # Write training statistics to tensorboard.\n",
    "            trainer.write_summary(summary_writer, steps, env._curriculum.lesson_number)\n",
    "        if steps % save_freq == 0 and steps != 0 and train_model:\n",
    "            # Save Tensorflow model\n",
    "            save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)\n",
    "        if len(trainer.stats['cumulative_reward']) > 0:\n",
    "            mean_reward = np.mean(trainer.stats['cumulative_reward'])\n",
    "            sess.run(ppo_model.update_reward, feed_dict={ppo_model.new_reward: mean_reward})\n",
    "            last_reward = sess.run(ppo_model.last_reward)\n",
    "    # Final save Tensorflow model\n",
    "    if steps != 0 and train_model:\n",
    "        save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "env.close()\n",
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/ppo\\model-2750000.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/ppo\\model-2750000.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 4 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 4 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 4 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
