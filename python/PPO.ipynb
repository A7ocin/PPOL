{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from ppo.history import *\n",
    "from ppo.models import *\n",
    "from ppo.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General parameters\n",
    "max_steps = 5e10 ##5e5 # Set maximum number of steps to run environment.\n",
    "run_path = \"ppo\" # The sub-directory name for model and summary statistics\n",
    "load_model = False # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model.\n",
    "summary_freq = 10000 # Frequency at which to save training statistics.\n",
    "save_freq = 50000 # Frequency at which to save model.\n",
    "env_name = \"ObstacleCurriculum\" # Name of the training environment file.\n",
    "curriculum_file = None\n",
    "\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.99 #0.99 # Reward discount rate.\n",
    "lambd = 0.95 ##0.95 # Lambda parameter for GAE.\n",
    "time_horizon = 2048 ##2048 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 1e-3 ##1e-3 # Strength of entropy regularization\n",
    "num_epoch = 5 ##5 # Number of gradient descent steps per batch of experiences.\n",
    "num_layers = 2 ##2 # Number of hidden layers between state/observation encoding and value/policy layers.\n",
    "epsilon = 0.2 ##0.2 # Acceptable threshold around ratio of old and new policy probabilities.\n",
    "buffer_size = 4096 ##2048 # How large the experience buffer should be before gradient descent.\n",
    "learning_rate = 3e-4 # Model learning rate.\n",
    "hidden_units = 128 ##64 # Number of units in hidden layer.\n",
    "batch_size = 32 ##64 # How many experiences per gradient descent update step.\n",
    "normalize = False\n",
    "\n",
    "### Logging dictionary for hyperparameters\n",
    "hyperparameter_dict = {'max_steps':max_steps, 'run_path':run_path, 'env_name':env_name,\n",
    "    'curriculum_file':curriculum_file, 'gamma':gamma, 'lambd':lambd, 'time_horizon':time_horizon,\n",
    "    'beta':beta, 'num_epoch':num_epoch, 'epsilon':epsilon, 'buffe_size':buffer_size,\n",
    "    'leaning_rate':learning_rate, 'hidden_units':hidden_units, 'batch_size':batch_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'ObstacleCurriculumAcademy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: ObstacleCurriculumAcademy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: ObstacleCurriculumBrain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 9\n",
      "        Action space type: discrete\n",
      "        Action space size (per agent): 4\n",
      "        Memory space size (per agent): 0\n",
      "        Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name, curriculum=curriculum_file)\n",
    "print(str(env))\n",
    "brain_name = env.external_brain_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10000. Mean Reward: -2.101749999999994. Std of Reward: 1.9047602265989345.\n",
      "Step: 20000. Mean Reward: -4.577199999999976. Std of Reward: 4.638622657643109.\n",
      "Step: 30000. Mean Reward: -7.404285714285613. Std of Reward: 4.9910862892698775.\n",
      "Step: 40000. Mean Reward: -7.710999999999894. Std of Reward: 6.252166744417382.\n",
      "Step: 50000. Mean Reward: -14.580000000000334. Std of Reward: 12.129465930122834.\n",
      "Saved Model\n",
      "Step: 60000. Mean Reward: -9.207727272727618. Std of Reward: 12.518894876371341.\n",
      "Step: 70000. Mean Reward: -6.531999999999966. Std of Reward: 6.507186232671291.\n",
      "Step: 80000. Mean Reward: -10.766428571429. Std of Reward: 11.983110946628242.\n",
      "Step: 90000. Mean Reward: -5.696052631578976. Std of Reward: 6.7692650185946475.\n",
      "Step: 100000. Mean Reward: -9.516111111111467. Std of Reward: 10.502496998804059.\n",
      "Saved Model\n",
      "Step: 110000. Mean Reward: -3.768749999999973. Std of Reward: 3.347090854383813.\n",
      "Step: 120000. Mean Reward: -7.4208333333334515. Std of Reward: 8.431952085503461.\n",
      "Step: 130000. Mean Reward: -3.546749999999958. Std of Reward: 3.1199051568115967.\n",
      "Step: 140000. Mean Reward: -2.3617105263157776. Std of Reward: 0.9849414064296955.\n",
      "Step: 150000. Mean Reward: -2.4869999999999886. Std of Reward: 0.7897071790046072.\n",
      "Saved Model\n",
      "Step: 160000. Mean Reward: -2.929482758620667. Std of Reward: 2.1401882550242983.\n",
      "Step: 170000. Mean Reward: -3.249821428571402. Std of Reward: 2.3710461382142314.\n",
      "Step: 180000. Mean Reward: -3.501739130434749. Std of Reward: 2.4296003467628835.\n",
      "Step: 190000. Mean Reward: -3.7867391304347406. Std of Reward: 2.7289983067114116.\n",
      "Step: 200000. Mean Reward: -3.021041666666633. Std of Reward: 2.6383983379941585.\n",
      "Saved Model\n",
      "Step: 210000. Mean Reward: -1.9045555555555473. Std of Reward: 1.3800051619155422.\n",
      "Step: 220000. Mean Reward: -1.7680952380952288. Std of Reward: 0.8964811848216444.\n",
      "Step: 230000. Mean Reward: -2.189880952380941. Std of Reward: 1.6380056427947958.\n",
      "Step: 240000. Mean Reward: -2.3362820512820326. Std of Reward: 2.1921631156406094.\n",
      "Step: 250000. Mean Reward: -1.540769230769223. Std of Reward: 1.7503566627249154.\n",
      "Saved Model\n",
      "Step: 260000. Mean Reward: -1.2383823529411715. Std of Reward: 1.613040023840718.\n",
      "Step: 270000. Mean Reward: -1.4562999999999888. Std of Reward: 2.31272519119758.\n",
      "Step: 280000. Mean Reward: -1.5322549019607703. Std of Reward: 2.7286794395905885.\n",
      "Step: 290000. Mean Reward: -0.9534615384615305. Std of Reward: 2.263267026476541.\n",
      "Step: 300000. Mean Reward: -1.1487272727272602. Std of Reward: 2.8386592895208906.\n",
      "Saved Model\n",
      "Step: 310000. Mean Reward: -1.94743243243241. Std of Reward: 4.767998988416653.\n",
      "Step: 320000. Mean Reward: -1.6143636363636154. Std of Reward: 3.707371165486033.\n",
      "Step: 330000. Mean Reward: -1.361810344827568. Std of Reward: 3.7255084932121667.\n",
      "Step: 340000. Mean Reward: -0.9179220779220673. Std of Reward: 2.7951910641022413.\n",
      "Step: 350000. Mean Reward: -0.7733870967741865. Std of Reward: 2.870885757558384.\n",
      "Saved Model\n",
      "Step: 360000. Mean Reward: -0.7661309523809418. Std of Reward: 2.8333769973375795.\n",
      "Step: 370000. Mean Reward: -0.37628712871286596. Std of Reward: 2.0674008714087253.\n",
      "Step: 380000. Mean Reward: -0.5351249999999904. Std of Reward: 2.673920573684791.\n",
      "Step: 390000. Mean Reward: -0.2411499999999938. Std of Reward: 2.25825705523084.\n",
      "Step: 400000. Mean Reward: -0.2878676470588214. Std of Reward: 1.4657795848306086.\n",
      "Saved Model\n",
      "Step: 410000. Mean Reward: -0.4815060240963772. Std of Reward: 2.4207650556761684.\n",
      "Step: 420000. Mean Reward: -0.4933333333333249. Std of Reward: 2.519855241181081.\n",
      "Step: 430000. Mean Reward: -0.7037499999999881. Std of Reward: 2.899010781461356.\n",
      "Step: 440000. Mean Reward: -0.1356779661016921. Std of Reward: 1.5914149864211995.\n",
      "Step: 450000. Mean Reward: -0.031249999999995944. Std of Reward: 1.9187502714440388.\n",
      "Saved Model\n",
      "Step: 460000. Mean Reward: 0.1610322580645173. Std of Reward: 1.2932650224583593.\n",
      "Step: 470000. Mean Reward: -0.6702564102563985. Std of Reward: 3.1068068256977637.\n",
      "Step: 480000. Mean Reward: 0.2601298701298709. Std of Reward: 0.9756607731033853.\n",
      "Step: 490000. Mean Reward: 0.25065359477124244. Std of Reward: 0.910159379279081.\n",
      "Step: 500000. Mean Reward: 0.3080645161290321. Std of Reward: 0.7338547262766175.\n",
      "Saved Model\n",
      "Step: 510000. Mean Reward: 0.21581632653061344. Std of Reward: 1.1218916303322746.\n",
      "Step: 520000. Mean Reward: 0.27014285714285774. Std of Reward: 0.9338458772457949.\n",
      "Step: 530000. Mean Reward: 0.15973053892215622. Std of Reward: 0.9082206492642652.\n",
      "Step: 540000. Mean Reward: 0.20590116279069817. Std of Reward: 0.8603270920687555.\n",
      "Step: 550000. Mean Reward: 0.2867883211678851. Std of Reward: 1.0788397283988056.\n",
      "Saved Model\n",
      "Step: 560000. Mean Reward: -0.012999999999992238. Std of Reward: 2.085284708568432.\n",
      "Step: 570000. Mean Reward: 0.2133223684210543. Std of Reward: 1.1260681981425689.\n",
      "Step: 580000. Mean Reward: 0.21079136690647757. Std of Reward: 1.2565869983927689.\n",
      "Step: 590000. Mean Reward: 0.3603160919540237. Std of Reward: 0.8287220379462729.\n",
      "Step: 600000. Mean Reward: 0.2570866141732313. Std of Reward: 1.1417779409822673.\n",
      "Saved Model\n",
      "Step: 610000. Mean Reward: 0.03313829787234603. Std of Reward: 1.4926384935284909.\n",
      "Step: 620000. Mean Reward: 0.2019629629629656. Std of Reward: 1.116719086049286.\n",
      "Step: 630000. Mean Reward: 0.40537500000000054. Std of Reward: 0.7352494368409906.\n",
      "Step: 640000. Mean Reward: 0.5482320441988952. Std of Reward: 0.5447997972500213.\n",
      "Step: 650000. Mean Reward: 0.5865079365079364. Std of Reward: 0.44202273899525085.\n",
      "Saved Model\n",
      "Step: 660000. Mean Reward: 0.4930674846625772. Std of Reward: 0.6729140286279836.\n",
      "Step: 670000. Mean Reward: 0.44742937853107334. Std of Reward: 0.6160199175334502.\n",
      "Step: 680000. Mean Reward: 0.32523026315789605. Std of Reward: 0.8624687902087427.\n",
      "Step: 690000. Mean Reward: 0.4246944444444447. Std of Reward: 0.69232682629924.\n",
      "Step: 700000. Mean Reward: 0.3389820359281443. Std of Reward: 0.8004821785899803.\n",
      "Saved Model\n",
      "Step: 710000. Mean Reward: 0.5494186046511627. Std of Reward: 0.47356602312509455.\n",
      "Step: 720000. Mean Reward: 0.5613202247191014. Std of Reward: 0.54188194092954.\n",
      "Step: 730000. Mean Reward: 0.4832857142857144. Std of Reward: 0.6077992418285375.\n",
      "Step: 740000. Mean Reward: 0.6060109289617485. Std of Reward: 0.38492485944862204.\n",
      "Step: 750000. Mean Reward: 0.6049481865284972. Std of Reward: 0.41319837713835267.\n",
      "Saved Model\n",
      "Step: 760000. Mean Reward: 0.5822043010752687. Std of Reward: 0.4218051723513605.\n",
      "Step: 770000. Mean Reward: 0.513174603174603. Std of Reward: 0.5511709288361957.\n",
      "Step: 780000. Mean Reward: 0.5723589743589742. Std of Reward: 0.4670511138326098.\n",
      "Step: 790000. Mean Reward: 0.6106735751295335. Std of Reward: 0.3896345599892233.\n",
      "Step: 800000. Mean Reward: 0.6080303030303028. Std of Reward: 0.4103785011830438.\n",
      "Saved Model\n",
      "Step: 810000. Mean Reward: 0.6173262032085562. Std of Reward: 0.4159192711519026.\n",
      "Step: 820000. Mean Reward: 0.5689896373056994. Std of Reward: 0.49402480470583116.\n",
      "Step: 830000. Mean Reward: 0.5239942528735635. Std of Reward: 0.5676663713506117.\n",
      "Step: 840000. Mean Reward: 0.5923284313725489. Std of Reward: 0.44921455960844153.\n",
      "Step: 850000. Mean Reward: 0.6318965517241377. Std of Reward: 0.37718000059023843.\n",
      "Saved Model\n",
      "Step: 860000. Mean Reward: 0.6505940594059404. Std of Reward: 0.33534398412299216.\n",
      "Step: 870000. Mean Reward: 0.6119306930693067. Std of Reward: 0.4194046170708088.\n",
      "Step: 880000. Mean Reward: 0.6044581280788176. Std of Reward: 0.4453677309083214.\n",
      "Step: 890000. Mean Reward: 0.6320544554455445. Std of Reward: 0.37592738397821396.\n",
      "Step: 900000. Mean Reward: 0.6398309178743959. Std of Reward: 0.3677593670969691.\n",
      "Saved Model\n",
      "Step: 910000. Mean Reward: 0.6939356435643564. Std of Reward: 0.20492628671539936.\n",
      "Step: 920000. Mean Reward: 0.6361083743842363. Std of Reward: 0.3682161472724081.\n",
      "Step: 930000. Mean Reward: 0.7071531100478466. Std of Reward: 0.18206065423498607.\n",
      "Step: 940000. Mean Reward: 0.6619230769230767. Std of Reward: 0.3239133517894186.\n",
      "Step: 950000. Mean Reward: 0.623238095238095. Std of Reward: 0.40428094954845506.\n",
      "Saved Model\n",
      "Step: 960000. Mean Reward: 0.6534374999999998. Std of Reward: 0.3436282454830365.\n",
      "Step: 970000. Mean Reward: 0.5907524271844659. Std of Reward: 0.4612817641010394.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 980000. Mean Reward: 0.605642857142857. Std of Reward: 0.44011572187261816.\n",
      "Step: 990000. Mean Reward: 0.5335781990521326. Std of Reward: 0.5595655621037549.\n",
      "Step: 1000000. Mean Reward: 0.545942028985507. Std of Reward: 0.528256595279024.\n",
      "Saved Model\n",
      "Step: 1010000. Mean Reward: 0.5865121951219512. Std of Reward: 0.4601480738319774.\n",
      "Step: 1020000. Mean Reward: 0.581431924882629. Std of Reward: 0.4788037139992622.\n",
      "Step: 1030000. Mean Reward: 0.5397417840375586. Std of Reward: 0.5335348539381193.\n",
      "Step: 1040000. Mean Reward: 0.6049759615384613. Std of Reward: 0.4218453330298623.\n",
      "Step: 1050000. Mean Reward: 0.580929648241206. Std of Reward: 0.447862893004128.\n",
      "Saved Model\n",
      "Step: 1060000. Mean Reward: 0.5289583333333331. Std of Reward: 0.4995794672494279.\n",
      "Step: 1070000. Mean Reward: 0.5574999999999998. Std of Reward: 0.47944168655643704.\n",
      "Step: 1080000. Mean Reward: 0.5983589743589742. Std of Reward: 0.3728368370143239.\n",
      "Step: 1090000. Mean Reward: 0.5006067961165047. Std of Reward: 0.5638264259456003.\n",
      "Step: 1100000. Mean Reward: 0.5124154589371979. Std of Reward: 0.547913884196523.\n",
      "Saved Model\n",
      "Step: 1110000. Mean Reward: 0.5484438775510202. Std of Reward: 0.47772019382161834.\n",
      "Step: 1120000. Mean Reward: 0.5826842105263156. Std of Reward: 0.38892010445003916.\n",
      "Step: 1130000. Mean Reward: 0.5477093596059112. Std of Reward: 0.4942803887676301.\n",
      "Step: 1140000. Mean Reward: 0.5555927835051545. Std of Reward: 0.4690885922409568.\n",
      "Step: 1150000. Mean Reward: 0.6191406249999998. Std of Reward: 0.3329882317984767.\n",
      "Saved Model\n",
      "Step: 1160000. Mean Reward: 0.6531249999999998. Std of Reward: 0.25783377437475746.\n",
      "Step: 1170000. Mean Reward: 0.6663333333333332. Std of Reward: 0.23813219053907891.\n",
      "Step: 1180000. Mean Reward: 0.6393846153846153. Std of Reward: 0.3145964134459533.\n",
      "Step: 1190000. Mean Reward: 0.6177339901477832. Std of Reward: 0.37455989041109083.\n",
      "Step: 1200000. Mean Reward: 0.6292893401015227. Std of Reward: 0.33644340536451317.\n",
      "Saved Model\n",
      "Step: 1210000. Mean Reward: 0.6361082474226802. Std of Reward: 0.28411679686684654.\n",
      "Step: 1220000. Mean Reward: 0.6383333333333332. Std of Reward: 0.3209161466841679.\n",
      "Step: 1230000. Mean Reward: 0.6610784313725488. Std of Reward: 0.26887804970511386.\n",
      "Step: 1240000. Mean Reward: 0.6583088235294116. Std of Reward: 0.3040171527033164.\n",
      "Step: 1250000. Mean Reward: 0.6141749999999998. Std of Reward: 0.3764564309119981.\n",
      "Saved Model\n",
      "Step: 1260000. Mean Reward: 0.6492788461538461. Std of Reward: 0.320546722165391.\n",
      "Step: 1270000. Mean Reward: 0.6589805825242716. Std of Reward: 0.2711834538506712.\n",
      "Step: 1280000. Mean Reward: 0.6649504950495049. Std of Reward: 0.24072600077733602.\n",
      "Step: 1290000. Mean Reward: 0.6821463414634145. Std of Reward: 0.20066664031521433.\n",
      "Step: 1300000. Mean Reward: 0.6678186274509802. Std of Reward: 0.23611701869150215.\n",
      "Saved Model\n",
      "Step: 1310000. Mean Reward: 0.6686386138613861. Std of Reward: 0.23873221896165556.\n",
      "Step: 1320000. Mean Reward: 0.6427339901477831. Std of Reward: 0.30398468359351294.\n",
      "Step: 1330000. Mean Reward: 0.6237623762376237. Std of Reward: 0.3545705155052245.\n",
      "Step: 1340000. Mean Reward: 0.6577093596059113. Std of Reward: 0.2748435278285539.\n",
      "Step: 1350000. Mean Reward: 0.6322058823529411. Std of Reward: 0.34702310592211194.\n",
      "Saved Model\n",
      "Step: 1360000. Mean Reward: 0.6439069767441857. Std of Reward: 0.35526647803647515.\n",
      "Step: 1370000. Mean Reward: 0.6474759615384614. Std of Reward: 0.31836458508687326.\n",
      "Step: 1380000. Mean Reward: 0.5677230046948356. Std of Reward: 0.494576314420068.\n",
      "Step: 1390000. Mean Reward: 0.5754460093896712. Std of Reward: 0.48103801412016906.\n",
      "Step: 1400000. Mean Reward: 0.6419194312796207. Std of Reward: 0.3598716044754482.\n",
      "Saved Model\n",
      "Step: 1410000. Mean Reward: 0.5362093023255813. Std of Reward: 0.5280866541885634.\n",
      "Step: 1420000. Mean Reward: 0.6191549295774648. Std of Reward: 0.3947418356874483.\n",
      "Step: 1430000. Mean Reward: 0.6267840375586853. Std of Reward: 0.37612692684426774.\n",
      "Step: 1440000. Mean Reward: 0.513623853211009. Std of Reward: 0.5756531478129756.\n",
      "Step: 1450000. Mean Reward: 0.5249065420560747. Std of Reward: 0.5404022304773223.\n",
      "Saved Model\n",
      "Step: 1460000. Mean Reward: 0.4711210762331837. Std of Reward: 0.615086561887096.\n",
      "Step: 1470000. Mean Reward: 0.5958088235294116. Std of Reward: 0.4052552544936269.\n",
      "Step: 1480000. Mean Reward: 0.5463207547169809. Std of Reward: 0.5073487328643416.\n",
      "Step: 1490000. Mean Reward: 0.5554186046511627. Std of Reward: 0.5059392018147405.\n",
      "Step: 1500000. Mean Reward: 0.5498863636363636. Std of Reward: 0.5178239967713271.\n",
      "Saved Model\n",
      "Step: 1510000. Mean Reward: 0.5922429906542055. Std of Reward: 0.44562186604519094.\n",
      "Step: 1520000. Mean Reward: 0.533716216216216. Std of Reward: 0.5429849894297663.\n",
      "Step: 1530000. Mean Reward: 0.5869392523364485. Std of Reward: 0.43934887150095264.\n",
      "Step: 1540000. Mean Reward: 0.6108644859813083. Std of Reward: 0.41769150328481014.\n",
      "Step: 1550000. Mean Reward: 0.568341121495327. Std of Reward: 0.4746367824188413.\n",
      "Saved Model\n",
      "Step: 1560000. Mean Reward: 0.6283168316831682. Std of Reward: 0.3611575704686162.\n",
      "Step: 1580000. Mean Reward: -0.37979999999994546. Std of Reward: 12.940261284179641.\n",
      "Step: 1590000. Mean Reward: 0.4724663677130044. Std of Reward: 0.6105696211913789.\n",
      "Step: 1600000. Mean Reward: 0.5274772727272726. Std of Reward: 0.5480248621034187.\n",
      "Saved Model\n",
      "Step: 1610000. Mean Reward: 0.6199539170506911. Std of Reward: 0.40602972320411435.\n",
      "Step: 1620000. Mean Reward: 0.5883796296296295. Std of Reward: 0.4547833879501519.\n",
      "Step: 1630000. Mean Reward: 0.5636175115207372. Std of Reward: 0.4983828005805826.\n",
      "Step: 1640000. Mean Reward: 0.4998604651162791. Std of Reward: 0.5863586719388765.\n",
      "Step: 1650000. Mean Reward: 0.48962328767123275. Std of Reward: 0.6032123157014028.\n",
      "Saved Model\n",
      "Step: 1660000. Mean Reward: -2.2146428571433776. Std of Reward: 17.749552053132295.\n",
      "Step: 1670000. Mean Reward: 0.6555188679245282. Std of Reward: 0.30960217111800514.\n",
      "Step: 1680000. Mean Reward: 0.6053191489361701. Std of Reward: 0.415985633261687.\n",
      "Step: 1690000. Mean Reward: -0.03907142857149906. Std of Reward: 7.843861216303183.\n",
      "Step: 1700000. Mean Reward: 0.6176146788990825. Std of Reward: 0.40881659395253506.\n",
      "Saved Model\n",
      "Step: 1710000. Mean Reward: 0.6388311688311686. Std of Reward: 0.2911128402900681.\n",
      "Step: 1720000. Mean Reward: -2.6686666666671273. Std of Reward: 18.093660995561734.\n",
      "Step: 1730000. Mean Reward: -0.580450000000163. Std of Reward: 10.509591050442163.\n",
      "Step: 1740000. Mean Reward: 0.44166666666666654. Std of Reward: 0.6448406069482662.\n",
      "Step: 1750000. Mean Reward: 0.47792792792792776. Std of Reward: 0.6258115338396197.\n",
      "Saved Model\n",
      "Step: 1760000. Mean Reward: 0.5058083832335327. Std of Reward: 0.577069646105795.\n",
      "Step: 1770000. Mean Reward: -3.701136363636777. Std of Reward: 19.0439529851252.\n",
      "Step: 1780000. Mean Reward: -1.5543396226418171. Std of Reward: 14.355255498005667.\n",
      "Step: 1790000. Mean Reward: 0.7142592592592593. Std of Reward: 0.07215856643921373.\n",
      "Step: 1800000. Mean Reward: -3.209833333333957. Std of Reward: 19.780920962862716.\n",
      "Saved Model\n",
      "Step: 1810000. Mean Reward: -0.8140983606557697. Std of Reward: 9.848332367287343.\n",
      "Step: 1820000. Mean Reward: -0.5257758620691371. Std of Reward: 10.4192287755566.\n",
      "Step: 1830000. Mean Reward: 0.34904958677685943. Std of Reward: 0.7416285725954144.\n",
      "Step: 1840000. Mean Reward: -2.5852857142861603. Std of Reward: 17.34515877055037.\n",
      "Step: 1850000. Mean Reward: -2.0406410256412975. Std of Reward: 14.910645017756485.\n",
      "Saved Model\n",
      "Step: 1860000. Mean Reward: -4.5160000000004015. Std of Reward: 19.367433141747714.\n",
      "Step: 1870000. Mean Reward: -47.01500000000553. Std of Reward: 47.72500000000552.\n",
      "Step: 1880000. Mean Reward: -1.5018852459019194. Std of Reward: 13.583949791489589.\n",
      "Step: 1890000. Mean Reward: 0.344451219512195. Std of Reward: 0.7418007120520658.\n",
      "Step: 1900000. Mean Reward: 0.44824324324324305. Std of Reward: 0.6399991725250311.\n",
      "Saved Model\n",
      "Step: 1910000. Mean Reward: -0.5369863013699916. Std of Reward: 9.114382948619562.\n",
      "Step: 1920000. Mean Reward: -25.837500000004137. Std of Reward: 45.98739263374973.\n",
      "Step: 1940000. Mean Reward: -0.37191011235970256. Std of Reward: 9.470012917948722.\n",
      "Step: 1950000. Mean Reward: 0.5092553191489361. Std of Reward: 0.6001958711523284.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "Step: 1960000. Mean Reward: -1.6552884615388568. Std of Reward: 15.644104118385245.\n",
      "Step: 1970000. Mean Reward: -0.8265131578948484. Std of Reward: 10.28321591810012.\n",
      "Step: 1980000. Mean Reward: -1.4283333333334702. Std of Reward: 12.357860704044969.\n",
      "Step: 1990000. Mean Reward: 0.4249999999999999. Std of Reward: 0.6874182596035426.\n",
      "Step: 2000000. Mean Reward: 0.5064285714285712. Std of Reward: 0.5825860714197043.\n",
      "Saved Model\n",
      "Step: 2010000. Mean Reward: -0.137288135593259. Std of Reward: 7.58459144526325.\n",
      "Step: 2020000. Mean Reward: 0.425580357142857. Std of Reward: 0.6647628959474527.\n",
      "Step: 2030000. Mean Reward: -9.941500000001595. Std of Reward: 31.45749346738072.\n",
      "Step: 2040000. Mean Reward: -0.07142335766426514. Std of Reward: 7.003581407829986.\n",
      "Step: 2050000. Mean Reward: 0.5444976076555023. Std of Reward: 0.5137768130527499.\n",
      "Saved Model\n",
      "Step: 2060000. Mean Reward: 0.5418396226415092. Std of Reward: 0.5028169822861289.\n",
      "Step: 2070000. Mean Reward: 0.6360930232558137. Std of Reward: 0.3545263341124775.\n",
      "Step: 2080000. Mean Reward: 0.5205468749999999. Std of Reward: 0.5473832998634512.\n",
      "Step: 2090000. Mean Reward: -7.733333333334429. Std of Reward: 27.470585809596816.\n",
      "Step: 2100000. Mean Reward: -2.2670000000002863. Std of Reward: 15.52477141308764.\n",
      "Saved Model\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "unpack requires a bytes object of length 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-fea09393b070>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_progress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;31m# Decide and take an action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mnew_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_experiences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_horizon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UNITY\\PPOL\\python\\ppo\\trainer.py\u001b[0m in \u001b[0;36mtake_action\u001b[1;34m(self, info, env, brain_name, steps, normalize)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entropy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearn_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[0mnew_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_experiences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UNITY\\PPOL\\python\\unityagents\\environment.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action, memory, value)\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb\"STEP\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 436\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    437\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_loaded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mUnityEnvironmentException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No Unity environment is loaded.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UNITY\\PPOL\\python\\unityagents\\environment.py\u001b[0m in \u001b[0;36m_get_state\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_brains\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m             \u001b[0mstate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"brain_name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[0mn_agent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"agents\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UNITY\\PPOL\\python\\unityagents\\environment.py\u001b[0m in \u001b[0;36m_get_state_dict\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \"\"\"\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb\"RECEIVED\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[0mstate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UNITY\\PPOL\\python\\unityagents\\environment.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m             \u001b[0mmessage_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"I\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mmessage_length\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: unpack requires a bytes object of length 4"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if curriculum_file == \"None\":\n",
    "    curriculum_file = None\n",
    "\n",
    "\n",
    "def get_progress():\n",
    "    if curriculum_file is not None:\n",
    "        if env._curriculum.measure_type == \"progress\":\n",
    "            return steps / max_steps\n",
    "        elif env._curriculum.measure_type == \"reward\":\n",
    "            return last_reward\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "ppo_model = create_agent_model(env, lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps, \n",
    "                               normalize=normalize, num_layers=num_layers)\n",
    "\n",
    "is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "\n",
    "model_path = './models/{}'.format(run_path)\n",
    "summary_path = './summaries/{}'.format(run_path)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps, last_reward = sess.run([ppo_model.global_step, ppo_model.last_reward])    \n",
    "    summary_writer = tf.summary.FileWriter(summary_path)\n",
    "    info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "    trainer = Trainer(ppo_model, sess, info, is_continuous, use_observations, use_states, train_model)\n",
    "    if train_model:\n",
    "        trainer.write_text(summary_writer, 'Hyperparameters', hyperparameter_dict, steps)\n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "        # Decide and take an action\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps, normalize)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        if len(trainer.training_buffer['actions']) > buffer_size and train_model:\n",
    "            # Perform gradient descent with experience buffer\n",
    "            trainer.update_model(batch_size, num_epoch)\n",
    "        if steps % summary_freq == 0 and steps != 0 and train_model:\n",
    "            # Write training statistics to tensorboard.\n",
    "            trainer.write_summary(summary_writer, steps, env._curriculum.lesson_number)\n",
    "        if steps % save_freq == 0 and steps != 0 and train_model:\n",
    "            # Save Tensorflow model\n",
    "            save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)\n",
    "        if len(trainer.stats['cumulative_reward']) > 0:\n",
    "            mean_reward = np.mean(trainer.stats['cumulative_reward'])\n",
    "            sess.run(ppo_model.update_reward, feed_dict={ppo_model.new_reward: mean_reward})\n",
    "            last_reward = sess.run(ppo_model.last_reward)\n",
    "    # Final save Tensorflow model\n",
    "    if steps != 0 and train_model:\n",
    "        save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "env.close()\n",
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/ppo\\model-2100000.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/ppo\\model-2100000.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 4 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 4 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 4 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
