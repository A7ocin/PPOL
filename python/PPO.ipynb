{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from ppo.history import *\n",
    "from ppo.models import *\n",
    "from ppo.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### General parameters\n",
    "max_steps = 5e10 ##5e5 # Set maximum number of steps to run environment.\n",
    "run_path = \"ppo\" # The sub-directory name for model and summary statistics\n",
    "load_model = True # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model.\n",
    "summary_freq = 10000 # Frequency at which to save training statistics.\n",
    "save_freq = 50000 # Frequency at which to save model.\n",
    "env_name = \"ObstacleCurriculum\" # Name of the training environment file.\n",
    "curriculum_file = None\n",
    "\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.99 #0.99 # Reward discount rate.\n",
    "lambd = 0.95 ##0.95 # Lambda parameter for GAE.\n",
    "time_horizon = 2048 ##2048 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 1e-3 ##1e-3 # Strength of entropy regularization\n",
    "num_epoch = 5 ##5 # Number of gradient descent steps per batch of experiences.\n",
    "num_layers = 2 ##2 # Number of hidden layers between state/observation encoding and value/policy layers.\n",
    "epsilon = 0.2 ##0.2 # Acceptable threshold around ratio of old and new policy probabilities.\n",
    "buffer_size = 4096 ##2048 # How large the experience buffer should be before gradient descent.\n",
    "learning_rate = 3e-4 # Model learning rate.\n",
    "hidden_units = 128 ##64 # Number of units in hidden layer.\n",
    "batch_size = 32 ##64 # How many experiences per gradient descent update step.\n",
    "normalize = False\n",
    "\n",
    "### Logging dictionary for hyperparameters\n",
    "hyperparameter_dict = {'max_steps':max_steps, 'run_path':run_path, 'env_name':env_name,\n",
    "    'curriculum_file':curriculum_file, 'gamma':gamma, 'lambd':lambd, 'time_horizon':time_horizon,\n",
    "    'beta':beta, 'num_epoch':num_epoch, 'epsilon':epsilon, 'buffe_size':buffer_size,\n",
    "    'leaning_rate':learning_rate, 'hidden_units':hidden_units, 'batch_size':batch_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'ObstacleCurriculumAcademy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: ObstacleCurriculumAcademy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: ObstacleCurriculumBrain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 9\n",
      "        Action space type: discrete\n",
      "        Action space size (per agent): 8\n",
      "        Memory space size (per agent): 0\n",
      "        Action descriptions: , , , , , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name, curriculum=curriculum_file)\n",
    "print(str(env))\n",
    "brain_name = env.external_brain_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "INFO:tensorflow:Restoring parameters from ./models/ppo\\model-4750000.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/ppo\\model-4750000.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "Step: 4760000. Mean Reward: 0.6269222887423311. Std of Reward: 0.263868472253713.\n",
      "Step: 4770000. Mean Reward: 0.6354545439272726. Std of Reward: 0.1747553715235771.\n",
      "Step: 4780000. Mean Reward: 0.5607708316874997. Std of Reward: 0.5071980795649748.\n",
      "Step: 4790000. Mean Reward: 0.52142404878481. Std of Reward: 0.5468130983625192.\n",
      "Step: 4800000. Mean Reward: 0.5606646202638036. Std of Reward: 0.3540345194954728.\n",
      "Saved Model\n",
      "Step: 4810000. Mean Reward: 0.5890020561172838. Std of Reward: 0.4011562702179468.\n",
      "Step: 4820000. Mean Reward: 0.6383028442499998. Std of Reward: 0.1753536373284906.\n",
      "Step: 4830000. Mean Reward: 0.5193313358323353. Std of Reward: 0.5097785990492378.\n",
      "Step: 4840000. Mean Reward: 0.505429446570552. Std of Reward: 0.5560592852113955.\n",
      "Step: 4850000. Mean Reward: 0.5879245268427671. Std of Reward: 0.36753103383489494.\n",
      "Saved Model\n",
      "Step: 4860000. Mean Reward: 0.40098888698666657. Std of Reward: 0.7640101336915256.\n",
      "Step: 4870000. Mean Reward: 0.4211037513576157. Std of Reward: 0.7425584433500926.\n",
      "Step: 4880000. Mean Reward: 0.5519725725696201. Std of Reward: 0.46474683613409995.\n",
      "Step: 4890000. Mean Reward: 0.4500724624968942. Std of Reward: 0.6829807034390133.\n",
      "Step: 4900000. Mean Reward: 0.4496555543333331. Std of Reward: 0.6608406345554779.\n",
      "Saved Model\n",
      "Step: 4910000. Mean Reward: 0.517828281642424. Std of Reward: 0.4717379014511522.\n",
      "Step: 4920000. Mean Reward: 0.48172149003289455. Std of Reward: 0.6045032762074849.\n",
      "Step: 4930000. Mean Reward: 0.47772151784177186. Std of Reward: 0.6544049957297108.\n",
      "Step: 4940000. Mean Reward: 0.47648484733939367. Std of Reward: 0.5794769503712439.\n",
      "Step: 4950000. Mean Reward: 0.49373672952866227. Std of Reward: 0.5551861710075401.\n",
      "Saved Model\n",
      "Step: 4960000. Mean Reward: 0.6074016548509314. Std of Reward: 0.3265939225326154.\n",
      "Step: 4970000. Mean Reward: 0.5515104151562499. Std of Reward: 0.469999918486193.\n",
      "Step: 4980000. Mean Reward: 0.48435416516874985. Std of Reward: 0.5615518857178627.\n",
      "Step: 4990000. Mean Reward: 0.5279454914276728. Std of Reward: 0.5183459283060209.\n",
      "Step: 5000000. Mean Reward: 0.46156314533540355. Std of Reward: 0.6007096010728176.\n",
      "Saved Model\n",
      "Step: 5010000. Mean Reward: 0.5285115289182388. Std of Reward: 0.5397418822887207.\n",
      "Step: 5020000. Mean Reward: 0.5466868671454544. Std of Reward: 0.4940997317953298.\n",
      "Step: 5030000. Mean Reward: 0.4463481936369424. Std of Reward: 0.6607391506010599.\n",
      "Step: 5040000. Mean Reward: 0.561974204720238. Std of Reward: 0.4645059685659179.\n",
      "Step: 5050000. Mean Reward: 0.5340432079444443. Std of Reward: 0.515691825675467.\n",
      "Saved Model\n",
      "Step: 5060000. Mean Reward: 0.5019529630858894. Std of Reward: 0.6063167146664163.\n",
      "Step: 5070000. Mean Reward: 0.46663461355128205. Std of Reward: 0.7148731647799564.\n",
      "Step: 5080000. Mean Reward: 0.3370748282517005. Std of Reward: 0.8398333701441186.\n",
      "Step: 5090000. Mean Reward: 0.4422604147812498. Std of Reward: 0.6732829068009343.\n",
      "Step: 5100000. Mean Reward: 0.519899596698795. Std of Reward: 0.5536017727513894.\n",
      "Saved Model\n",
      "Step: 5110000. Mean Reward: 0.43530398166037715. Std of Reward: 0.7040045640977637.\n",
      "Step: 5120000. Mean Reward: 0.4903246731818181. Std of Reward: 0.587800859938671.\n",
      "Step: 5130000. Mean Reward: 0.5375294097411762. Std of Reward: 0.5157284085307138.\n",
      "Step: 5140000. Mean Reward: 0.5417543838713448. Std of Reward: 0.5553585106119905.\n",
      "Step: 5150000. Mean Reward: 0.5580177492544377. Std of Reward: 0.49472868813780924.\n",
      "Saved Model\n",
      "Step: 5160000. Mean Reward: 0.5486398443793102. Std of Reward: 0.5073152940616565.\n",
      "Step: 5170000. Mean Reward: 0.589009801435294. Std of Reward: 0.4115796326868063.\n",
      "Step: 5180000. Mean Reward: 0.5611515129575756. Std of Reward: 0.46041982782303015.\n",
      "Step: 5190000. Mean Reward: 0.5623238070799998. Std of Reward: 0.4364701082080393.\n",
      "Step: 5200000. Mean Reward: 0.641288757238372. Std of Reward: 0.2582032730804002.\n",
      "Saved Model\n",
      "Step: 5210000. Mean Reward: 0.5883901494488636. Std of Reward: 0.3842894408589594.\n",
      "Step: 5220000. Mean Reward: 0.5967757916309522. Std of Reward: 0.3143873767601373.\n",
      "Step: 5230000. Mean Reward: 0.6255492405227272. Std of Reward: 0.3184869654931681.\n",
      "Step: 5240000. Mean Reward: 0.6171832339181286. Std of Reward: 0.2875721955354139.\n",
      "Step: 5250000. Mean Reward: 0.6344543626130952. Std of Reward: 0.2262304316569705.\n",
      "Saved Model\n",
      "Step: 5260000. Mean Reward: 0.6450678276104651. Std of Reward: 0.2175135771734787.\n",
      "Step: 5270000. Mean Reward: 0.5862887576162789. Std of Reward: 0.4113008007875937.\n",
      "Step: 5280000. Mean Reward: 0.5618353155178569. Std of Reward: 0.4389589671134774.\n",
      "Step: 5290000. Mean Reward: 0.6008529392764703. Std of Reward: 0.31807695454177604.\n",
      "Step: 5300000. Mean Reward: 0.585138886863095. Std of Reward: 0.34834882708038006.\n",
      "Saved Model\n",
      "Step: 5310000. Mean Reward: 0.6211302662011492. Std of Reward: 0.31662567744199244.\n",
      "Step: 5320000. Mean Reward: 0.5988264276627218. Std of Reward: 0.34606734911342607.\n",
      "Step: 5330000. Mean Reward: 0.5953333311085712. Std of Reward: 0.3598295011371102.\n",
      "Step: 5340000. Mean Reward: 0.5624224784534881. Std of Reward: 0.451489542703773.\n",
      "Step: 5350000. Mean Reward: 0.5328585835090908. Std of Reward: 0.5152631574827266.\n",
      "Saved Model\n",
      "Step: 5360000. Mean Reward: 0.4816078409941175. Std of Reward: 0.597204267813877.\n",
      "Step: 5370000. Mean Reward: 0.4452965207055213. Std of Reward: 0.651414953876726.\n",
      "Step: 5380000. Mean Reward: 0.37129166449999984. Std of Reward: 0.7620068288586542.\n",
      "Step: 5390000. Mean Reward: 0.37976041414999984. Std of Reward: 0.7650491502970004.\n",
      "Step: 5400000. Mean Reward: 0.31283755028481. Std of Reward: 0.8384278554345036.\n",
      "Saved Model\n",
      "Step: 5410000. Mean Reward: 0.4684444419575756. Std of Reward: 0.658152989443564.\n",
      "Step: 5420000. Mean Reward: 0.343069618177215. Std of Reward: 0.8222777082017181.\n",
      "Step: 5430000. Mean Reward: 0.22078947139473667. Std of Reward: 0.9260453021438187.\n",
      "Step: 5440000. Mean Reward: 0.1271743907549667. Std of Reward: 0.9934475903000314.\n",
      "Step: 5450000. Mean Reward: 0.11329032034193534. Std of Reward: 0.9793907641697543.\n",
      "Saved Model\n",
      "Step: 5460000. Mean Reward: 0.0997315417651005. Std of Reward: 0.979690652360994.\n",
      "Step: 5470000. Mean Reward: 0.13601075062580628. Std of Reward: 0.9750634105489472.\n",
      "Step: 5480000. Mean Reward: 0.1641390707218541. Std of Reward: 0.9310323422467061.\n",
      "Step: 5490000. Mean Reward: 0.4054968918136645. Std of Reward: 0.7092076525193283.\n",
      "Step: 5500000. Mean Reward: 0.3757317050975608. Std of Reward: 0.7340765168088574.\n",
      "Saved Model\n",
      "Step: 5510000. Mean Reward: 0.44899795271779136. Std of Reward: 0.6403504687003583.\n",
      "Step: 5520000. Mean Reward: 0.3607407387777776. Std of Reward: 0.7811803654540607.\n",
      "Step: 5530000. Mean Reward: 0.43808823303529404. Std of Reward: 0.6443710937066265.\n",
      "Step: 5540000. Mean Reward: 0.30143605665408785. Std of Reward: 0.8386480514978953.\n",
      "Step: 5550000. Mean Reward: 0.4626147681856287. Std of Reward: 0.6263617092453153.\n",
      "Saved Model\n",
      "Step: 5560000. Mean Reward: 0.3983954428260868. Std of Reward: 0.7356978366892459.\n",
      "Step: 5570000. Mean Reward: 0.26490131329605243. Std of Reward: 0.8530639265660765.\n",
      "Step: 5580000. Mean Reward: 0.4284567876604937. Std of Reward: 0.6646272700794079.\n",
      "Step: 5590000. Mean Reward: 0.40222879471005907. Std of Reward: 0.7021792385102614.\n",
      "Step: 5600000. Mean Reward: 0.3614699770807452. Std of Reward: 0.7854330555559327.\n",
      "Saved Model\n",
      "Step: 5610000. Mean Reward: 0.4445702284968552. Std of Reward: 0.658677183772548.\n",
      "Step: 5620000. Mean Reward: 0.37654320758641957. Std of Reward: 0.7215391247500567.\n",
      "Step: 5630000. Mean Reward: 0.44251572172327025. Std of Reward: 0.6432992207276451.\n",
      "Step: 5640000. Mean Reward: 0.5016060584303029. Std of Reward: 0.5726358184057128.\n",
      "Step: 5650000. Mean Reward: 0.5003372992619047. Std of Reward: 0.5329924368766781.\n",
      "Saved Model\n",
      "Step: 5660000. Mean Reward: 0.4155486520434781. Std of Reward: 0.6898472480899654.\n",
      "Step: 5670000. Mean Reward: 0.5065187355443787. Std of Reward: 0.5639221689166452.\n",
      "Step: 5680000. Mean Reward: 0.5398431352352939. Std of Reward: 0.46491151890922733.\n",
      "Step: 5690000. Mean Reward: 0.5272736009537571. Std of Reward: 0.5291401281815844.\n",
      "Step: 5700000. Mean Reward: 0.5363352805906432. Std of Reward: 0.5075855271537519.\n",
      "Saved Model\n",
      "Step: 5710000. Mean Reward: 0.5415069839580836. Std of Reward: 0.4916733896853541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5720000. Mean Reward: 0.6008678477514792. Std of Reward: 0.3412060004752839.\n",
      "Step: 5730000. Mean Reward: 0.5645759346923076. Std of Reward: 0.4249790198065954.\n",
      "Step: 5740000. Mean Reward: 0.5846686724638553. Std of Reward: 0.36418511410286764.\n",
      "Step: 5750000. Mean Reward: 0.45156378398765423. Std of Reward: 0.6395304261571326.\n",
      "Saved Model\n",
      "Step: 5760000. Mean Reward: 0.42598725899999984. Std of Reward: 0.7219770649693877.\n",
      "Step: 5770000. Mean Reward: 0.5231612878774191. Std of Reward: 0.5493234079167104.\n",
      "Step: 5780000. Mean Reward: 0.5223852276287424. Std of Reward: 0.5695817784974208.\n",
      "Step: 5790000. Mean Reward: 0.4419461677142855. Std of Reward: 0.6699004957454036.\n",
      "Step: 5800000. Mean Reward: 0.49540880311949675. Std of Reward: 0.6227245849561652.\n",
      "Saved Model\n",
      "Step: 5810000. Mean Reward: 0.5633835325240962. Std of Reward: 0.46437487389434134.\n",
      "Step: 5820000. Mean Reward: 0.489404760220238. Std of Reward: 0.581691944661919.\n",
      "Step: 5830000. Mean Reward: 0.4977510019939758. Std of Reward: 0.5461096088247303.\n",
      "Step: 5840000. Mean Reward: 0.6036904744047618. Std of Reward: 0.3580649208276048.\n",
      "Step: 5850000. Mean Reward: 0.5735271299302324. Std of Reward: 0.3973654291086912.\n",
      "Saved Model\n",
      "Step: 5860000. Mean Reward: 0.5937599187023809. Std of Reward: 0.31884899649927323.\n",
      "Step: 5870000. Mean Reward: 0.6448787863090908. Std of Reward: 0.09680911934311061.\n",
      "Step: 5880000. Mean Reward: 0.6248475593475608. Std of Reward: 0.22379251565028244.\n",
      "Step: 5890000. Mean Reward: 0.617175646766467. Std of Reward: 0.2567125263902249.\n",
      "Step: 5900000. Mean Reward: 0.5746825376249999. Std of Reward: 0.3961169807306209.\n",
      "Saved Model\n",
      "Step: 5910000. Mean Reward: 0.5782738076249998. Std of Reward: 0.3524279868286421.\n",
      "Step: 5920000. Mean Reward: 0.5552934993773583. Std of Reward: 0.46159105367786224.\n",
      "Step: 5930000. Mean Reward: 0.6031595074601226. Std of Reward: 0.27607820569894365.\n",
      "Step: 5940000. Mean Reward: 0.5800833316374998. Std of Reward: 0.377548598651484.\n",
      "Step: 5950000. Mean Reward: 0.5978484830909089. Std of Reward: 0.32144449961728827.\n",
      "Saved Model\n",
      "Step: 5960000. Mean Reward: 0.560686866951515. Std of Reward: 0.3400816393431776.\n",
      "Step: 5970000. Mean Reward: 0.5888229152437499. Std of Reward: 0.32346188372574475.\n",
      "Step: 5980000. Mean Reward: 0.5882530105481926. Std of Reward: 0.380397470971223.\n",
      "Step: 5990000. Mean Reward: 0.5958232918313251. Std of Reward: 0.34617694725405795.\n",
      "Step: 6000000. Mean Reward: 0.5982291651687499. Std of Reward: 0.2695035100248267.\n",
      "Saved Model\n",
      "Step: 6010000. Mean Reward: 0.5895783120602408. Std of Reward: 0.3752834575073876.\n",
      "Step: 6020000. Mean Reward: 0.5921097034240504. Std of Reward: 0.23442791917726405.\n",
      "Step: 6030000. Mean Reward: 0.5929791653499998. Std of Reward: 0.22853863845094516.\n",
      "Step: 6040000. Mean Reward: 0.6096502045370369. Std of Reward: 0.2221517197039156.\n",
      "Step: 6050000. Mean Reward: 0.5682924320858894. Std of Reward: 0.3194170703258902.\n",
      "Saved Model\n",
      "Step: 6060000. Mean Reward: 0.6148502985688621. Std of Reward: 0.21754213011151102.\n",
      "Step: 6070000. Mean Reward: 0.6115644161472391. Std of Reward: 0.2376824950187722.\n",
      "Step: 6080000. Mean Reward: 0.595917158674556. Std of Reward: 0.32966247515319974.\n",
      "Step: 6090000. Mean Reward: 0.5992757928035713. Std of Reward: 0.2969508649301642.\n",
      "Step: 6100000. Mean Reward: 0.6211616151212119. Std of Reward: 0.25761358995511613.\n",
      "Saved Model\n",
      "Step: 6110000. Mean Reward: 0.5844810369281436. Std of Reward: 0.33179549974904315.\n",
      "Step: 6120000. Mean Reward: 0.5469427997337276. Std of Reward: 0.4379566248772165.\n",
      "Step: 6130000. Mean Reward: 0.6249397581867469. Std of Reward: 0.21842328484097834.\n",
      "Step: 6140000. Mean Reward: 0.5872052835365852. Std of Reward: 0.2969575818645148.\n",
      "Step: 6150000. Mean Reward: 0.5878124990749998. Std of Reward: 0.2763893683186926.\n",
      "Saved Model\n",
      "Step: 6160000. Mean Reward: 0.5827607354907974. Std of Reward: 0.29634764809785247.\n",
      "Step: 6170000. Mean Reward: 0.5950609744999998. Std of Reward: 0.26220898755769817.\n",
      "Step: 6180000. Mean Reward: 0.6079835383641974. Std of Reward: 0.23812981853051646.\n",
      "Step: 6190000. Mean Reward: 0.5964607831882351. Std of Reward: 0.3229398859910134.\n",
      "Step: 6200000. Mean Reward: 0.5911177633173651. Std of Reward: 0.3486962907552222.\n",
      "Saved Model\n",
      "Step: 6210000. Mean Reward: 0.6146464631939392. Std of Reward: 0.2718782894300752.\n",
      "Step: 6220000. Mean Reward: 0.6167153985906431. Std of Reward: 0.2899838912201752.\n",
      "Step: 6230000. Mean Reward: 0.6266367255868261. Std of Reward: 0.21759271420994541.\n",
      "Step: 6240000. Mean Reward: 0.5740532534615382. Std of Reward: 0.38853023564617806.\n",
      "Step: 6250000. Mean Reward: 0.6088293641130951. Std of Reward: 0.29951567851841493.\n",
      "Saved Model\n",
      "Step: 6260000. Mean Reward: 0.5903048771280487. Std of Reward: 0.2988077741117042.\n",
      "Step: 6270000. Mean Reward: 0.5585542158433734. Std of Reward: 0.37647503491410167.\n",
      "Step: 6280000. Mean Reward: 0.5534722215595236. Std of Reward: 0.3946934472010401.\n",
      "Step: 6290000. Mean Reward: 0.5914285704285712. Std of Reward: 0.32453672138913564.\n",
      "Step: 6300000. Mean Reward: 0.6428460031286548. Std of Reward: 0.22647062464036286.\n",
      "Saved Model\n",
      "Step: 6310000. Mean Reward: 0.6480339311317364. Std of Reward: 0.17386733594118178.\n",
      "Step: 6320000. Mean Reward: 0.6171856274910177. Std of Reward: 0.2862259821075101.\n",
      "Step: 6330000. Mean Reward: 0.621431371452941. Std of Reward: 0.2587957472165609.\n",
      "Step: 6340000. Mean Reward: 0.6035601567573963. Std of Reward: 0.2915015275150369.\n",
      "Step: 6350000. Mean Reward: 0.6273353283353291. Std of Reward: 0.23044246108334518.\n",
      "Saved Model\n",
      "Step: 6360000. Mean Reward: 0.5795568391502889. Std of Reward: 0.3886953823821242.\n",
      "Step: 6370000. Mean Reward: 0.6238293641845237. Std of Reward: 0.2650488225315335.\n",
      "Step: 6380000. Mean Reward: 0.5579783029053254. Std of Reward: 0.4357069027586563.\n",
      "Step: 6390000. Mean Reward: 0.581508874710059. Std of Reward: 0.3858399412910474.\n",
      "Step: 6400000. Mean Reward: 0.6090505042545453. Std of Reward: 0.2927589775592771.\n",
      "Saved Model\n",
      "Step: 6410000. Mean Reward: 0.5499705872823528. Std of Reward: 0.4412056222495778.\n",
      "Step: 6420000. Mean Reward: 0.5526305210301202. Std of Reward: 0.463250810306417.\n",
      "Step: 6430000. Mean Reward: 0.5160355017810649. Std of Reward: 0.5416652216347283.\n",
      "Step: 6440000. Mean Reward: 0.5326606414397589. Std of Reward: 0.5290587034684369.\n",
      "Step: 6450000. Mean Reward: 0.5329190739595374. Std of Reward: 0.519056260285919.\n",
      "Saved Model\n",
      "Step: 6460000. Mean Reward: 0.40565624891249985. Std of Reward: 0.7061459630493591.\n",
      "Step: 6470000. Mean Reward: 0.46796413360759476. Std of Reward: 0.6212895095351378.\n",
      "Step: 6480000. Mean Reward: 0.4737163548136644. Std of Reward: 0.6508568233229882.\n",
      "Step: 6490000. Mean Reward: 0.4701656303664594. Std of Reward: 0.6040060421564469.\n",
      "Step: 6500000. Mean Reward: 0.5304999984117645. Std of Reward: 0.46820739535438355.\n",
      "Saved Model\n",
      "Step: 6510000. Mean Reward: 0.4865631457267079. Std of Reward: 0.6370428760818576.\n",
      "Step: 6520000. Mean Reward: 0.6020882341294116. Std of Reward: 0.3490805878083635.\n",
      "Step: 6530000. Mean Reward: 0.6243775089999998. Std of Reward: 0.2300835380443906.\n",
      "Step: 6540000. Mean Reward: 0.5103725479294117. Std of Reward: 0.5485647270685194.\n",
      "Step: 6550000. Mean Reward: 0.5595906424795319. Std of Reward: 0.4294895819839059.\n",
      "Saved Model\n",
      "Step: 6560000. Mean Reward: 0.5831536918622754. Std of Reward: 0.37724440480178273.\n",
      "Step: 6570000. Mean Reward: 0.5538516249878047. Std of Reward: 0.46565746591122387.\n",
      "Step: 6580000. Mean Reward: 0.5683436846335401. Std of Reward: 0.4266831443117629.\n",
      "Step: 6590000. Mean Reward: 0.5420276000445858. Std of Reward: 0.4363729468770197.\n",
      "Step: 6600000. Mean Reward: 0.5791880333910254. Std of Reward: 0.32895313766101697.\n",
      "Saved Model\n",
      "Step: 6610000. Mean Reward: 0.5141041660187498. Std of Reward: 0.5215132190167782.\n",
      "Step: 6620000. Mean Reward: 0.5657291659499999. Std of Reward: 0.3970580039569813.\n",
      "Step: 6630000. Mean Reward: 0.5678343307005986. Std of Reward: 0.39567247023409363.\n",
      "Step: 6640000. Mean Reward: 0.5015189868164556. Std of Reward: 0.5215602733290008.\n",
      "Step: 6650000. Mean Reward: 0.6045426821341461. Std of Reward: 0.2978884203278533.\n",
      "Saved Model\n",
      "Step: 6660000. Mean Reward: 0.5437934554049079. Std of Reward: 0.4588089662949811.\n",
      "Step: 6670000. Mean Reward: 0.48748958236874984. Std of Reward: 0.630703998953295.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6680000. Mean Reward: 0.544926159620253. Std of Reward: 0.5229315067345478.\n",
      "Step: 6690000. Mean Reward: 0.5577844305209578. Std of Reward: 0.4317884883826199.\n",
      "Step: 6700000. Mean Reward: 0.48835305620118324. Std of Reward: 0.523277518992998.\n",
      "Saved Model\n",
      "Step: 6710000. Mean Reward: 0.49233539029012335. Std of Reward: 0.4966960266937366.\n",
      "Step: 6720000. Mean Reward: 0.531142556798742. Std of Reward: 0.5196337046520345.\n",
      "Step: 6730000. Mean Reward: 0.48161290254193534. Std of Reward: 0.6081662139851329.\n",
      "Step: 6740000. Mean Reward: 0.5349363048662417. Std of Reward: 0.48087793771632653.\n",
      "Step: 6750000. Mean Reward: 0.4830312492874998. Std of Reward: 0.5782501402044434.\n",
      "Saved Model\n",
      "Step: 6760000. Mean Reward: 0.45676406868181796. Std of Reward: 0.5912798028307474.\n",
      "Step: 6770000. Mean Reward: 0.45249999926623363. Std of Reward: 0.6550748512257498.\n",
      "Step: 6780000. Mean Reward: 0.4906458326312498. Std of Reward: 0.5442937890743381.\n",
      "Step: 6790000. Mean Reward: 0.40632478559615365. Std of Reward: 0.7136216795052479.\n",
      "Step: 6800000. Mean Reward: 0.5382083326812499. Std of Reward: 0.4873225299543139.\n",
      "Saved Model\n",
      "Step: 6810000. Mean Reward: 0.45840811898076905. Std of Reward: 0.6439008263782917.\n",
      "Step: 6820000. Mean Reward: 0.5665295351898733. Std of Reward: 0.3795398873970786.\n",
      "Step: 6830000. Mean Reward: 0.6047817452916665. Std of Reward: 0.2876553847108426.\n",
      "Step: 6840000. Mean Reward: 0.5098136637391302. Std of Reward: 0.5267807549510943.\n",
      "Step: 6850000. Mean Reward: 0.5238817196580643. Std of Reward: 0.534035160012831.\n",
      "Saved Model\n",
      "Step: 6860000. Mean Reward: 0.5200729158874997. Std of Reward: 0.5298384627761584.\n",
      "Step: 6870000. Mean Reward: 0.5514257018795178. Std of Reward: 0.4403621054662669.\n",
      "Step: 6880000. Mean Reward: 0.5940376974047618. Std of Reward: 0.349558087233631.\n",
      "Step: 6890000. Mean Reward: 0.580321284325301. Std of Reward: 0.35327947774931034.\n",
      "Step: 6900000. Mean Reward: 0.6157317064451218. Std of Reward: 0.22970496225398868.\n",
      "Saved Model\n",
      "Step: 6910000. Mean Reward: 0.5438271596913579. Std of Reward: 0.4678565769781167.\n",
      "Step: 6920000. Mean Reward: 0.5715928259113922. Std of Reward: 0.4001912491651268.\n",
      "Step: 6930000. Mean Reward: 0.5144927527453415. Std of Reward: 0.4969064770867045.\n",
      "Step: 6940000. Mean Reward: 0.5151337438950615. Std of Reward: 0.49582702997764516.\n",
      "Step: 6950000. Mean Reward: 0.5269018394110428. Std of Reward: 0.4811299072175754.\n",
      "Saved Model\n",
      "Step: 6960000. Mean Reward: 0.51166666545679. Std of Reward: 0.500630227058642.\n",
      "Step: 6970000. Mean Reward: 0.5321341453109756. Std of Reward: 0.4990331285777739.\n",
      "Step: 6980000. Mean Reward: 0.5349196775481925. Std of Reward: 0.48503514994655655.\n",
      "Step: 6990000. Mean Reward: 0.5087376714023667. Std of Reward: 0.5004620982695115.\n",
      "Step: 7000000. Mean Reward: 0.5532329305301203. Std of Reward: 0.46028571732394263.\n",
      "Saved Model\n",
      "Step: 7010000. Mean Reward: 0.5313273442694609. Std of Reward: 0.4757591033734468.\n",
      "Step: 7020000. Mean Reward: 0.5761077833532933. Std of Reward: 0.3499991261024271.\n",
      "Step: 7030000. Mean Reward: 0.5746041657812497. Std of Reward: 0.33263037792179523.\n",
      "Step: 7040000. Mean Reward: 0.6010707060848482. Std of Reward: 0.2752083968641224.\n",
      "Step: 7050000. Mean Reward: 0.5374503959464284. Std of Reward: 0.4421062475768147.\n",
      "Saved Model\n",
      "Step: 7060000. Mean Reward: 0.601044175825301. Std of Reward: 0.3176923475577679.\n",
      "Step: 7070000. Mean Reward: 0.4946341455731706. Std of Reward: 0.5309132736369654.\n",
      "Step: 7080000. Mean Reward: 0.5455063281582276. Std of Reward: 0.40767568257954484.\n",
      "Step: 7090000. Mean Reward: 0.5578703693333332. Std of Reward: 0.3538547403101238.\n",
      "Step: 7100000. Mean Reward: 0.5341463405487803. Std of Reward: 0.41126327658297546.\n",
      "Saved Model\n",
      "Step: 7110000. Mean Reward: 0.5981936119221555. Std of Reward: 0.3055714852430592.\n",
      "Step: 7120000. Mean Reward: 0.5099999990898202. Std of Reward: 0.5153305756519819.\n",
      "Step: 7130000. Mean Reward: 0.5935569094817073. Std of Reward: 0.26503844405231225.\n",
      "Step: 7140000. Mean Reward: 0.6124171526198829. Std of Reward: 0.3210864821870849.\n",
      "Step: 7150000. Mean Reward: 0.5921960770470587. Std of Reward: 0.33279929907339145.\n",
      "Saved Model\n",
      "Step: 7160000. Mean Reward: 0.5860562002499999. Std of Reward: 0.35074348684806045.\n",
      "Step: 7170000. Mean Reward: 0.5802564092721891. Std of Reward: 0.3544749614649629.\n",
      "Step: 7180000. Mean Reward: 0.6052890161329478. Std of Reward: 0.32769226751491215.\n",
      "Step: 7190000. Mean Reward: 0.5511805543035713. Std of Reward: 0.3909909043504428.\n",
      "Step: 7200000. Mean Reward: 0.537337397262195. Std of Reward: 0.48423562107340784.\n",
      "Saved Model\n",
      "Step: 7210000. Mean Reward: 0.5711078419588234. Std of Reward: 0.3947615290851742.\n",
      "Step: 7220000. Mean Reward: 0.5853779055581394. Std of Reward: 0.35764866426416425.\n",
      "Step: 7230000. Mean Reward: 0.5761637069230768. Std of Reward: 0.43513140859147154.\n",
      "Step: 7240000. Mean Reward: 0.5790239399613258. Std of Reward: 0.4261665808790635.\n",
      "Step: 7250000. Mean Reward: 0.5610153247068964. Std of Reward: 0.4199500028245914.\n",
      "Saved Model\n",
      "Step: 7260000. Mean Reward: 0.5654469261229049. Std of Reward: 0.4258584845766238.\n",
      "Step: 7270000. Mean Reward: 0.5602367412556817. Std of Reward: 0.4526486210874803.\n",
      "Step: 7280000. Mean Reward: 0.5977619036571428. Std of Reward: 0.3707680451612868.\n",
      "Step: 7290000. Mean Reward: 0.5390134085862067. Std of Reward: 0.4916403010471603.\n",
      "Step: 7300000. Mean Reward: 0.5488389501685391. Std of Reward: 0.45590527474028025.\n",
      "Saved Model\n",
      "Step: 7310000. Mean Reward: 0.5275094685852271. Std of Reward: 0.48558124074271325.\n",
      "Step: 7320000. Mean Reward: 0.604224136775862. Std of Reward: 0.35274940220615575.\n",
      "Step: 7330000. Mean Reward: 0.5799905292556816. Std of Reward: 0.3864391023084201.\n",
      "Step: 7340000. Mean Reward: 0.6004093557368421. Std of Reward: 0.3697577417056905.\n",
      "Step: 7350000. Mean Reward: 0.605190474942857. Std of Reward: 0.34291615221530947.\n",
      "Saved Model\n",
      "Step: 7360000. Mean Reward: 0.5343869720344826. Std of Reward: 0.4921186711036743.\n",
      "Step: 7370000. Mean Reward: 0.5135119035595237. Std of Reward: 0.49379067822310646.\n",
      "Step: 7380000. Mean Reward: 0.6164201171420117. Std of Reward: 0.26190394587960975.\n",
      "Step: 7390000. Mean Reward: 0.584815890494186. Std of Reward: 0.3525067784668527.\n",
      "Step: 7400000. Mean Reward: 0.6093274841578946. Std of Reward: 0.29436480095921996.\n",
      "Saved Model\n",
      "Step: 7410000. Mean Reward: 0.612028570205714. Std of Reward: 0.322422225080007.\n",
      "Step: 7420000. Mean Reward: 0.6131130255459769. Std of Reward: 0.32530479762595493.\n",
      "Step: 7430000. Mean Reward: 0.5913391126763005. Std of Reward: 0.35046146559087765.\n",
      "Step: 7440000. Mean Reward: 0.5939199243631283. Std of Reward: 0.3965956314028235.\n",
      "Step: 7450000. Mean Reward: 0.6215708803505745. Std of Reward: 0.29196310668548764.\n",
      "Saved Model\n",
      "Step: 7460000. Mean Reward: 0.5759302316976743. Std of Reward: 0.3830324995109065.\n",
      "Step: 7470000. Mean Reward: 0.6150383130919539. Std of Reward: 0.29292808532040093.\n",
      "Step: 7480000. Mean Reward: 0.5808092473410402. Std of Reward: 0.3712089730189818.\n",
      "Step: 7490000. Mean Reward: 0.6269444433103446. Std of Reward: 0.2919923241764719.\n",
      "Step: 7500000. Mean Reward: 0.6679096033050845. Std of Reward: 0.16973399476011286.\n",
      "Saved Model\n",
      "Step: 7510000. Mean Reward: 0.5941193171647726. Std of Reward: 0.3678594119506432.\n",
      "Step: 7520000. Mean Reward: 0.5589631773313952. Std of Reward: 0.43831359254863944.\n",
      "Step: 7530000. Mean Reward: 0.6203137244647057. Std of Reward: 0.27067230444020923.\n",
      "Step: 7540000. Mean Reward: 0.6178725480411763. Std of Reward: 0.2973783050042853.\n",
      "Step: 7550000. Mean Reward: 0.6148285702971427. Std of Reward: 0.3485799799548865.\n",
      "Saved Model\n",
      "Step: 7560000. Mean Reward: 0.5977490413678159. Std of Reward: 0.3536057113473582.\n",
      "Step: 7570000. Mean Reward: 0.5848520700769229. Std of Reward: 0.3223837615835108.\n",
      "Step: 7580000. Mean Reward: 0.580828624135593. Std of Reward: 0.39716284798970203.\n",
      "Step: 7590000. Mean Reward: 0.5927238083657141. Std of Reward: 0.3452557964404734.\n",
      "Step: 7600000. Mean Reward: 0.556837119931818. Std of Reward: 0.4524121685499066.\n",
      "Saved Model\n",
      "Step: 7610000. Mean Reward: 0.542813724317647. Std of Reward: 0.5065947846046912.\n",
      "Step: 7620000. Mean Reward: 0.6270265138238634. Std of Reward: 0.3011677555640827.\n",
      "Step: 7630000. Mean Reward: 0.6147534504970412. Std of Reward: 0.2659022975784237.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 7640000. Mean Reward: 0.562535352442424. Std of Reward: 0.4538420564712056.\n",
      "Step: 7650000. Mean Reward: 0.5752268234911241. Std of Reward: 0.4102993943863346.\n",
      "Saved Model\n",
      "Step: 7660000. Mean Reward: 0.5246198822339179. Std of Reward: 0.49137558875499465.\n",
      "Step: 7670000. Mean Reward: 0.5810404612658958. Std of Reward: 0.41877663419116107.\n",
      "Step: 7680000. Mean Reward: 0.587321771219653. Std of Reward: 0.4023024069389819.\n",
      "Step: 7690000. Mean Reward: 0.5550478914367815. Std of Reward: 0.4769474572245882.\n",
      "Step: 7700000. Mean Reward: 0.4448412680535713. Std of Reward: 0.695402447994171.\n",
      "Saved Model\n",
      "Step: 7710000. Mean Reward: 0.4332638875297617. Std of Reward: 0.6569560225045852.\n",
      "Step: 7720000. Mean Reward: 0.5218352045898875. Std of Reward: 0.5623993345253278.\n",
      "Step: 7730000. Mean Reward: 0.5008234112916665. Std of Reward: 0.5498246472246191.\n",
      "Step: 7740000. Mean Reward: 0.482725488717647. Std of Reward: 0.5706173610922507.\n",
      "Step: 7750000. Mean Reward: 0.6059763297692305. Std of Reward: 0.29084748316387227.\n",
      "Saved Model\n",
      "Step: 7760000. Mean Reward: 0.594512669111111. Std of Reward: 0.3629654456244522.\n",
      "Step: 7770000. Mean Reward: 0.5665361435602408. Std of Reward: 0.36209559414494474.\n",
      "Step: 7780000. Mean Reward: 0.5171329352619046. Std of Reward: 0.5248230340844559.\n",
      "Step: 7790000. Mean Reward: 0.602238657195266. Std of Reward: 0.3107252787287025.\n",
      "Step: 7800000. Mean Reward: 0.6292528724999998. Std of Reward: 0.2704094098607154.\n",
      "Saved Model\n",
      "Step: 7810000. Mean Reward: 0.5828642704431135. Std of Reward: 0.3364574104464977.\n",
      "Step: 7820000. Mean Reward: 0.5742800777928991. Std of Reward: 0.35077798070525573.\n",
      "Step: 7830000. Mean Reward: 0.55977969245977. Std of Reward: 0.4166623364402073.\n",
      "Step: 7840000. Mean Reward: 0.5998740299534883. Std of Reward: 0.325806592470064.\n",
      "Step: 7850000. Mean Reward: 0.5830346810346819. Std of Reward: 0.3513823842825473.\n",
      "Saved Model\n",
      "Step: 7860000. Mean Reward: 0.5771001910751444. Std of Reward: 0.3734035444373026.\n",
      "Step: 7870000. Mean Reward: 0.5870294106235292. Std of Reward: 0.3554517733486833.\n",
      "Step: 7880000. Mean Reward: 0.5967538748313952. Std of Reward: 0.35364570739012163.\n",
      "Step: 7890000. Mean Reward: 0.5684307982456138. Std of Reward: 0.37634582879913664.\n",
      "Step: 7900000. Mean Reward: 0.5975803202891565. Std of Reward: 0.46119989141292494.\n",
      "Saved Model\n",
      "Step: 7910000. Mean Reward: 0.5632938847810649. Std of Reward: 0.3884100817540026.\n",
      "Step: 7920000. Mean Reward: 0.5354265861666665. Std of Reward: 0.5006509318808378.\n",
      "Step: 7930000. Mean Reward: 0.5917714276628571. Std of Reward: 0.3467837783794498.\n",
      "Step: 7940000. Mean Reward: 0.5746666658457141. Std of Reward: 0.39123017216858297.\n",
      "Step: 7950000. Mean Reward: 0.5834472040248446. Std of Reward: 0.3896076719230231.\n",
      "Saved Model\n",
      "Step: 7960000. Mean Reward: 0.5465963846144576. Std of Reward: 0.4970452301041244.\n",
      "Step: 7970000. Mean Reward: 0.5961046503837207. Std of Reward: 0.3174472334550925.\n",
      "Step: 7980000. Mean Reward: 0.5889820350179639. Std of Reward: 0.35073498761289607.\n",
      "Step: 7990000. Mean Reward: 0.6031167598983049. Std of Reward: 0.34976913766916923.\n",
      "Step: 8000000. Mean Reward: 0.5833431944082839. Std of Reward: 0.3285133516701366.\n",
      "Saved Model\n",
      "Step: 8010000. Mean Reward: 0.5636926139461076. Std of Reward: 0.4041841671210705.\n",
      "Step: 8020000. Mean Reward: 0.5682943462982454. Std of Reward: 0.37582095333875626.\n",
      "Step: 8030000. Mean Reward: 0.5749325617052021. Std of Reward: 0.39972017170769303.\n",
      "Step: 8040000. Mean Reward: 0.5708234119226189. Std of Reward: 0.3819020032644271.\n",
      "Step: 8050000. Mean Reward: 0.6174848476303029. Std of Reward: 0.19424754384576604.\n",
      "Saved Model\n",
      "Step: 8060000. Mean Reward: 0.5643394301280485. Std of Reward: 0.42217062395696603.\n",
      "Step: 8070000. Mean Reward: 0.5682222214606059. Std of Reward: 0.42797936208435516.\n",
      "Step: 8080000. Mean Reward: 0.6153801160760232. Std of Reward: 0.27633608641205304.\n",
      "Step: 8090000. Mean Reward: 0.5641916159401197. Std of Reward: 0.4369866288509065.\n",
      "Step: 8100000. Mean Reward: 0.5988757388106507. Std of Reward: 0.3228303673593026.\n",
      "Saved Model\n",
      "Step: 8110000. Mean Reward: 0.5646421258773004. Std of Reward: 0.3858676224824655.\n",
      "Step: 8120000. Mean Reward: 0.600038758854651. Std of Reward: 0.3278399548152839.\n",
      "Step: 8130000. Mean Reward: 0.5620019714201182. Std of Reward: 0.3739693520143084.\n",
      "Step: 8140000. Mean Reward: 0.5782843128882351. Std of Reward: 0.34840610113935533.\n",
      "Step: 8150000. Mean Reward: 0.6156568617294116. Std of Reward: 0.3291730724535822.\n",
      "Saved Model\n",
      "Step: 8160000. Mean Reward: 0.5935536386839079. Std of Reward: 0.35031881930960346.\n",
      "Step: 8170000. Mean Reward: 0.61476331256213. Std of Reward: 0.26699414665804017.\n",
      "Step: 8180000. Mean Reward: 0.6050793640535713. Std of Reward: 0.2692350770700304.\n",
      "Step: 8190000. Mean Reward: 0.596209522862857. Std of Reward: 0.3517870214541564.\n",
      "Step: 8200000. Mean Reward: 0.5883812249655171. Std of Reward: 0.3763942759279187.\n",
      "Saved Model\n",
      "Step: 8210000. Mean Reward: 0.6364367805689654. Std of Reward: 0.22441497258726129.\n",
      "Step: 8220000. Mean Reward: 0.5850385346473986. Std of Reward: 0.3445738179717548.\n",
      "Step: 8230000. Mean Reward: 0.5871047609142855. Std of Reward: 0.3736046613803493.\n",
      "Step: 8240000. Mean Reward: 0.6216863893076922. Std of Reward: 0.23261462897517732.\n",
      "Step: 8250000. Mean Reward: 0.6064186496011904. Std of Reward: 0.26085061549906097.\n",
      "Saved Model\n",
      "Step: 8260000. Mean Reward: 0.6201267046491226. Std of Reward: 0.26178282421220955.\n",
      "Step: 8270000. Mean Reward: 0.6182647047647057. Std of Reward: 0.26518253401498715.\n",
      "Step: 8280000. Mean Reward: 0.6368644055028246. Std of Reward: 0.2711637780931215.\n",
      "Step: 8290000. Mean Reward: 0.5823988425144507. Std of Reward: 0.42648005560570634.\n",
      "Step: 8300000. Mean Reward: 0.6284291175287354. Std of Reward: 0.2593115647592754.\n",
      "Saved Model\n",
      "Step: 8310000. Mean Reward: 0.5972771304651161. Std of Reward: 0.379943724671312.\n",
      "Step: 8320000. Mean Reward: 0.6033528252690057. Std of Reward: 0.30513309391009735.\n",
      "Step: 8330000. Mean Reward: 0.6194791652613635. Std of Reward: 0.33127941598281496.\n",
      "Step: 8340000. Mean Reward: 0.6183914714476743. Std of Reward: 0.27065864341009943.\n",
      "Step: 8350000. Mean Reward: 0.6133718677225433. Std of Reward: 0.30421619558886515.\n",
      "Saved Model\n",
      "Step: 8360000. Mean Reward: 0.6314509790764704. Std of Reward: 0.2255146312785638.\n",
      "Step: 8370000. Mean Reward: 0.587589284333333. Std of Reward: 0.4083388710814088.\n",
      "Step: 8380000. Mean Reward: 0.6125678283313951. Std of Reward: 0.2978236279482149.\n",
      "Step: 8390000. Mean Reward: 0.5590963846385539. Std of Reward: 0.4015222707791455.\n",
      "Step: 8400000. Mean Reward: 0.6624858745649715. Std of Reward: 0.18591143274793984.\n",
      "Saved Model\n",
      "Step: 8410000. Mean Reward: 0.6004365068988093. Std of Reward: 0.39780387689727853.\n",
      "Step: 8420000. Mean Reward: 0.6064689253954801. Std of Reward: 0.35548642457071516.\n",
      "Step: 8430000. Mean Reward: 0.5990532532485205. Std of Reward: 0.3485691960759504.\n",
      "Step: 8440000. Mean Reward: 0.5804120865714285. Std of Reward: 0.40493803826792496.\n",
      "Step: 8450000. Mean Reward: 0.5940229876034481. Std of Reward: 0.3452153390711764.\n",
      "Saved Model\n",
      "Step: 8460000. Mean Reward: 0.617759258161111. Std of Reward: 0.3457049660473621.\n",
      "Step: 8470000. Mean Reward: 0.6281344686363636. Std of Reward: 0.2918078254061531.\n",
      "Step: 8480000. Mean Reward: 0.5915819199435026. Std of Reward: 0.4034786462859932.\n",
      "Step: 8490000. Mean Reward: 0.5580871202159089. Std of Reward: 0.4345838991925524.\n",
      "Step: 8500000. Mean Reward: 0.6320599240280897. Std of Reward: 0.29060905334192644.\n",
      "Saved Model\n",
      "Step: 8510000. Mean Reward: 0.5838920442556816. Std of Reward: 0.3720146577956826.\n",
      "Step: 8520000. Mean Reward: 0.5676310849213483. Std of Reward: 0.41922029081294504.\n",
      "Step: 8530000. Mean Reward: 0.6252276856612021. Std of Reward: 0.3472698529588577.\n",
      "Step: 8540000. Mean Reward: 0.5988380940571427. Std of Reward: 0.42754468696694814.\n",
      "Step: 8550000. Mean Reward: 0.6306800755804597. Std of Reward: 0.2699966305012337.\n",
      "Saved Model\n",
      "Step: 8560000. Mean Reward: 0.5758996200909089. Std of Reward: 0.39186333945856855.\n",
      "Step: 8570000. Mean Reward: 0.6061538449175823. Std of Reward: 0.35964906164997784.\n",
      "Step: 8580000. Mean Reward: 0.5824115443966479. Std of Reward: 0.42370088890568813.\n",
      "Step: 8590000. Mean Reward: 0.5695714272914284. Std of Reward: 0.3931721910258269.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 8600000. Mean Reward: 0.571104867696629. Std of Reward: 0.48409016310110625.\n",
      "Saved Model\n",
      "Step: 8610000. Mean Reward: 0.5839325827921347. Std of Reward: 0.40957795847866746.\n",
      "Step: 8620000. Mean Reward: 0.5668659406576085. Std of Reward: 0.440221476258019.\n",
      "Step: 8630000. Mean Reward: 0.5472644912880433. Std of Reward: 0.5063331449702483.\n",
      "Step: 8640000. Mean Reward: 0.5549079177624309. Std of Reward: 0.4464741387529373.\n",
      "Step: 8650000. Mean Reward: 0.5876684868306009. Std of Reward: 0.4040619477871966.\n",
      "Saved Model\n",
      "Step: 8660000. Mean Reward: 0.5985792337158469. Std of Reward: 0.3933757235550762.\n",
      "Step: 8670000. Mean Reward: 0.5808110504919783. Std of Reward: 0.44186696629961614.\n",
      "Step: 8680000. Mean Reward: 0.5975791419106143. Std of Reward: 0.3665842200128676.\n",
      "Step: 8690000. Mean Reward: 0.5869809510742855. Std of Reward: 0.40081557283243585.\n",
      "Step: 8700000. Mean Reward: 0.58673041752459. Std of Reward: 0.41895188867556893.\n",
      "Saved Model\n",
      "Step: 8710000. Mean Reward: 0.6150356491604277. Std of Reward: 0.3806494417206375.\n",
      "Step: 8720000. Mean Reward: 0.6289839557433154. Std of Reward: 0.36612402119133597.\n",
      "Step: 8730000. Mean Reward: 0.5899094187934781. Std of Reward: 0.4491032640712862.\n",
      "Step: 8740000. Mean Reward: 0.5844708978412696. Std of Reward: 0.4668689241742715.\n",
      "Step: 8750000. Mean Reward: 0.5956318668406592. Std of Reward: 0.3810918066890481.\n",
      "Saved Model\n",
      "Step: 8760000. Mean Reward: 0.541587717668421. Std of Reward: 0.499344988603922.\n",
      "Step: 8770000. Mean Reward: 0.6158839763646408. Std of Reward: 0.35350496800429554.\n",
      "Step: 8780000. Mean Reward: 0.5096428554670328. Std of Reward: 0.5523083791455146.\n",
      "Step: 8790000. Mean Reward: 0.5279928301935483. Std of Reward: 0.5346141686846584.\n",
      "Step: 8800000. Mean Reward: 0.5956517676033518. Std of Reward: 0.3920681333991397.\n",
      "Saved Model\n",
      "Step: 8810000. Mean Reward: 0.5027150524946236. Std of Reward: 0.5601221998263651.\n",
      "Step: 8820000. Mean Reward: 0.5499731168548386. Std of Reward: 0.5070855615634738.\n",
      "Step: 8830000. Mean Reward: 0.6302857129828571. Std of Reward: 0.2637668177781084.\n",
      "Step: 8840000. Mean Reward: 0.5751992741413042. Std of Reward: 0.43365611882495264.\n",
      "Step: 8850000. Mean Reward: 0.5286520932349725. Std of Reward: 0.5480095391999993.\n",
      "Saved Model\n",
      "Step: 8860000. Mean Reward: 0.5622253244860335. Std of Reward: 0.5259317332301127.\n",
      "Step: 8870000. Mean Reward: 0.5684898696906077. Std of Reward: 0.4350114730645891.\n",
      "Step: 8880000. Mean Reward: 0.6117222206555555. Std of Reward: 0.3455948931294048.\n",
      "Step: 8890000. Mean Reward: 0.5512477214863386. Std of Reward: 0.5097571422624176.\n",
      "Step: 8900000. Mean Reward: 0.6154450745340907. Std of Reward: 0.34285676974265616.\n",
      "Saved Model\n",
      "Step: 8910000. Mean Reward: 0.623003765016949. Std of Reward: 0.3008592547839863.\n",
      "Step: 8920000. Mean Reward: 0.6004351838611109. Std of Reward: 0.39302080191685157.\n",
      "Step: 8930000. Mean Reward: 0.5824250923314604. Std of Reward: 0.46283988327475295.\n",
      "Step: 8940000. Mean Reward: 0.576958567316384. Std of Reward: 0.4004210661916477.\n",
      "Step: 8950000. Mean Reward: 0.5832872913646406. Std of Reward: 0.47766143761807756.\n",
      "Saved Model\n",
      "Step: 8960000. Mean Reward: 0.5660427792192512. Std of Reward: 0.48268254475416156.\n",
      "Step: 8970000. Mean Reward: 0.6032780832154694. Std of Reward: 0.36755930593227015.\n",
      "Step: 8980000. Mean Reward: 0.5586187830110496. Std of Reward: 0.44650634968686087.\n",
      "Step: 8990000. Mean Reward: 0.5306492233139533. Std of Reward: 0.5178298812037406.\n",
      "Step: 9000000. Mean Reward: 0.586111109819672. Std of Reward: 0.44780105421120603.\n",
      "Saved Model\n",
      "Step: 9010000. Mean Reward: 0.5399908407857142. Std of Reward: 0.5622666523032568.\n",
      "Step: 9020000. Mean Reward: 0.5855344186249999. Std of Reward: 0.4475979247592289.\n",
      "Step: 9030000. Mean Reward: 0.6211111096892654. Std of Reward: 0.35229637065907804.\n",
      "Step: 9040000. Mean Reward: 0.4933140641849708. Std of Reward: 0.5877974517418374.\n",
      "Step: 9050000. Mean Reward: 0.600346082251366. Std of Reward: 0.40752462777079196.\n",
      "Saved Model\n",
      "Step: 9060000. Mean Reward: 0.6256998146077346. Std of Reward: 0.3435660962697575.\n",
      "Step: 9070000. Mean Reward: 0.6507222209333332. Std of Reward: 0.22638627112012902.\n",
      "Step: 9080000. Mean Reward: 0.6044927521576086. Std of Reward: 0.4324985156607277.\n",
      "Step: 9090000. Mean Reward: 0.5802317275935829. Std of Reward: 0.4972915112439337.\n",
      "Step: 9100000. Mean Reward: 0.5745543657860962. Std of Reward: 0.4799156724806307.\n",
      "Saved Model\n",
      "Step: 9110000. Mean Reward: 0.5557169444189943. Std of Reward: 0.4779439117693535.\n",
      "Step: 9120000. Mean Reward: 0.5395355177923497. Std of Reward: 0.5264953953022758.\n",
      "Step: 9130000. Mean Reward: 0.6226779012696627. Std of Reward: 0.3440222186579514.\n",
      "Step: 9140000. Mean Reward: 0.5751944431944443. Std of Reward: 0.47846679246723206.\n",
      "Step: 9150000. Mean Reward: 0.6095943549947089. Std of Reward: 0.4041762950115176.\n",
      "Saved Model\n",
      "Step: 9160000. Mean Reward: 0.4638295867528088. Std of Reward: 0.6354790825449925.\n",
      "Step: 9170000. Mean Reward: 0.5940833318388887. Std of Reward: 0.42820113525405923.\n",
      "Step: 9180000. Mean Reward: 0.5548542790437156. Std of Reward: 0.5137266757081139.\n",
      "Step: 9190000. Mean Reward: 0.521657507741758. Std of Reward: 0.5781421945748869.\n",
      "Step: 9200000. Mean Reward: 0.5624999985337077. Std of Reward: 0.5426661550276676.\n",
      "Saved Model\n",
      "Step: 9210000. Mean Reward: 0.5562891327403314. Std of Reward: 0.4987555986424936.\n",
      "Step: 9220000. Mean Reward: 0.43595237968452355. Std of Reward: 0.7187898498350818.\n",
      "Step: 9230000. Mean Reward: 0.474849339418079. Std of Reward: 0.6319566333238913.\n",
      "Step: 9240000. Mean Reward: 0.45303809397714273. Std of Reward: 0.6722686952681484.\n",
      "Step: 9250000. Mean Reward: 0.2493089414756096. Std of Reward: 0.9184864218011807.\n",
      "Saved Model\n",
      "Step: 9260000. Mean Reward: 0.3175301189096384. Std of Reward: 0.8618983692176767.\n",
      "Step: 9270000. Mean Reward: 0.40602490264367797. Std of Reward: 0.7312494903889584.\n",
      "Step: 9280000. Mean Reward: 0.5287090542312137. Std of Reward: 0.6079335822914843.\n",
      "Step: 9290000. Mean Reward: 0.49004816809248547. Std of Reward: 0.6350770141507375.\n",
      "Step: 9300000. Mean Reward: 0.4190138051065087. Std of Reward: 0.7602120756902465.\n",
      "Saved Model\n",
      "Step: 9310000. Mean Reward: 0.5497527458241757. Std of Reward: 0.5319802690658829.\n",
      "Step: 9320000. Mean Reward: 0.5380696785254236. Std of Reward: 0.5216312654988593.\n",
      "Step: 9330000. Mean Reward: 0.46422787041242924. Std of Reward: 0.6722188059676678.\n",
      "Step: 9340000. Mean Reward: 0.5969537020444442. Std of Reward: 0.3959743347267402.\n",
      "Step: 9350000. Mean Reward: 0.4808244666914892. Std of Reward: 0.6059827924645484.\n",
      "Saved Model\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "unpack requires a bytes object of length 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-fea09393b070>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_progress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;31m# Decide and take an action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mnew_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_experiences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_horizon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UNITY\\PPOL\\python\\ppo\\trainer.py\u001b[0m in \u001b[0;36mtake_action\u001b[1;34m(self, info, env, brain_name, steps, normalize)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entropy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearn_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[0mnew_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_experiences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UNITY\\PPOL\\python\\unityagents\\environment.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action, memory, value)\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb\"STEP\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 436\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    437\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_loaded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mUnityEnvironmentException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No Unity environment is loaded.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UNITY\\PPOL\\python\\unityagents\\environment.py\u001b[0m in \u001b[0;36m_get_state\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_brains\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m             \u001b[0mstate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"brain_name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[0mn_agent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"agents\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UNITY\\PPOL\\python\\unityagents\\environment.py\u001b[0m in \u001b[0;36m_get_state_dict\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \"\"\"\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb\"RECEIVED\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[0mstate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UNITY\\PPOL\\python\\unityagents\\environment.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m             \u001b[0mmessage_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"I\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mmessage_length\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: unpack requires a bytes object of length 4"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if curriculum_file == \"None\":\n",
    "    curriculum_file = None\n",
    "\n",
    "\n",
    "def get_progress():\n",
    "    if curriculum_file is not None:\n",
    "        if env._curriculum.measure_type == \"progress\":\n",
    "            return steps / max_steps\n",
    "        elif env._curriculum.measure_type == \"reward\":\n",
    "            return last_reward\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "ppo_model = create_agent_model(env, lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps, \n",
    "                               normalize=normalize, num_layers=num_layers)\n",
    "\n",
    "is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "\n",
    "model_path = './models/{}'.format(run_path)\n",
    "summary_path = './summaries/{}'.format(run_path)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps, last_reward = sess.run([ppo_model.global_step, ppo_model.last_reward])    \n",
    "    summary_writer = tf.summary.FileWriter(summary_path)\n",
    "    info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "    trainer = Trainer(ppo_model, sess, info, is_continuous, use_observations, use_states, train_model)\n",
    "    if train_model:\n",
    "        trainer.write_text(summary_writer, 'Hyperparameters', hyperparameter_dict, steps)\n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "        # Decide and take an action\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps, normalize)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        if len(trainer.training_buffer['actions']) > buffer_size and train_model:\n",
    "            # Perform gradient descent with experience buffer\n",
    "            trainer.update_model(batch_size, num_epoch)\n",
    "        if steps % summary_freq == 0 and steps != 0 and train_model:\n",
    "            # Write training statistics to tensorboard.\n",
    "            trainer.write_summary(summary_writer, steps, env._curriculum.lesson_number)\n",
    "        if steps % save_freq == 0 and steps != 0 and train_model:\n",
    "            # Save Tensorflow model\n",
    "            save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)\n",
    "        if len(trainer.stats['cumulative_reward']) > 0:\n",
    "            mean_reward = np.mean(trainer.stats['cumulative_reward'])\n",
    "            sess.run(ppo_model.update_reward, feed_dict={ppo_model.new_reward: mean_reward})\n",
    "            last_reward = sess.run(ppo_model.last_reward)\n",
    "    # Final save Tensorflow model\n",
    "    if steps != 0 and train_model:\n",
    "        save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "env.close()\n",
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/ppo\\model-9350000.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/ppo\\model-9350000.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 4 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 4 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 4 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
